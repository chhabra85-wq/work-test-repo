{
	"name": "validation_engine_unit_tests",
	"properties": {
		"folder": {
			"name": "tests/test_notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "29a8e679-81d6-4479-b265-d5ac4ebd08ed"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpool",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pytest\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, LongType\r\n",
					"from unittest.mock import Mock, MagicMock, ANY\r\n",
					"from typing import List, Dict, Any\r\n",
					"\r\n",
					"class TestValidationEngineUnit(SparkSessionBaseClass):    \r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        super().setUp()\r\n",
					"        \r\n",
					"    def create_mock_test(self) -> DataFrame:\r\n",
					"        schema = StructType([\r\n",
					"            StructField(\"dataChange\", StringType(), True),\r\n",
					"            StructField(\"index\", LongType(), True),\r\n",
					"            StructField(\"externalMatchId\", StringType(), True),\r\n",
					"            StructField(\"balances_balance\", StringType(), True),\r\n",
					"            StructField(\"balances_date\", StringType(), True),\r\n",
					"            StructField(\"startAt\", StringType(), True),\r\n",
					"            StructField(\"bankId\", StringType(), True)\r\n",
					"        ])\r\n",
					"\r\n",
					"        return self.spark_session.createDataFrame([\r\n",
					"                (\"MERGE\", 1, None, \"1\", \"03_28_23\", \"03_28_23\", \"VACATION\"), \r\n",
					"                ( None, 2, \"joe_id\", \"1\", \"03_28_23\", \"03_28_23\", \"VACATION\"),\r\n",
					"                (\"MERGE\", None, \"tim_id\", \"1\", \"03_28_23\", \"03_28_23\", \"VACATION\"),\r\n",
					"                (\"MERGE\", 4, \"carl_id\", \"1\", \"03_28_23\", \"03_28_23\", None),\r\n",
					"                (\"MERGE\", 5, \"alberto_id\", \"1\", \"03_28_23\", \"03_28_23\", \"VACATION\"),\r\n",
					"                (\"DELETE\", 6, \"bert_id\", \"1\", \"03_28_23\", \"03_28_23\", \"VACATION\"),\r\n",
					"                (None, None, None, \"1\", \"03_28_23\", \"03_28_23\", None),        \r\n",
					"            ],\r\n",
					"            schema=schema\r\n",
					"        )    \r\n",
					"\r\n",
					"    def test_validation_df_schema_behavior(self):\r\n",
					"        \"\"\"\r\n",
					"            Asserts validation engine's validate_df_schema() throws errors when validation fails\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        schemaMissingColumn = { \"columnNotInSchema\": \"StringType()\" }\r\n",
					"        schemaWrongTypeColumn = { \"externalMatchId\": \"LongType()\", \"index\": \"StringType()\" }  \r\n",
					"        validationSchema = { \"externalMatchId\": \"StringType()\", \"index\": \"LongType()\", \"dataChange\": \"StringType()\" }    \r\n",
					"        \r\n",
					"        mock_params = { \r\n",
					"            \"data_type\": \"default_type\",\r\n",
					"            \"run_id\": str(UUID(int=0)),\r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
					"        }\r\n",
					"\r\n",
					"        test_df = self.create_mock_test()\r\n",
					"        telemetry_mock_missing_column = Mock()\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        validation_engine_missing_column = ValidationEngine(telemetry_mock_missing_column, mock_params, datalake_client_mock)\r\n",
					"\r\n",
					"        telemetry_mock_wrong_type = Mock()\r\n",
					"        validation_engine_wrong_type = ValidationEngine(telemetry_mock_wrong_type, mock_params, datalake_client_mock)\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        with pytest.raises(Exception) as e:\r\n",
					"            validation_engine_missing_column.validate_df_schema(test_df, schemaMissingColumn)\r\n",
					"        assert e.type == TypeError\r\n",
					"        assert \"Expected columns: {'columnNotInSchema'}\" in str(e.value)\r\n",
					"        assert \"Actual columns:\" in str(e.value)\r\n",
					"        columns = ['startAt', 'dataChange', 'externalMatchId', 'balances_balance', 'index', 'bankId', 'balances_date']\r\n",
					"        for column in columns:\r\n",
					"            assert column in str(e.value)\r\n",
					"        with pytest.raises(Exception) as e:\r\n",
					"            validation_engine_wrong_type.validate_df_schema(test_df, schemaWrongTypeColumn)\r\n",
					"        assert e.type == TypeError\r\n",
					"        assert \"Expected type: LongType().\" in str(e.value)\r\n",
					"        assert \"Actual type: StringType().\" in str(e.value)\r\n",
					"\r\n",
					"    def test_validation_df_column_behavior(self):\r\n",
					"        \"\"\"\r\n",
					"            Asserts validation engine's validate_df_columns() filters invalid rows from dataframe\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_valid_id = \"alberto_id\"\r\n",
					"        expected_valid_index = 5\r\n",
					"        expected_valid_rows = 2\r\n",
					"\r\n",
					"        validationColumnRules = {\r\n",
					"            \"columnIsNotNull\": {\r\n",
					"                \"targetColumns\": [\"dataChange\", \"externalMatchId\", \"index\", \"bankId\"], \r\n",
					"                \"ruleExpression\": \"column_name IS NOT NULL\", \r\n",
					"                \"ruleExpressionType\": \"PySpark.DataFrame.Filter(str)\"\r\n",
					"            },\r\n",
					"            \"isMergeOrUpdate\": {\r\n",
					"                \"targetColumns\": [\"dataChange\"], \r\n",
					"                \"ruleExpression\": \"column_name == 'MERGE' OR column_name == 'DELETE'\", \r\n",
					"                \"ruleExpressionType\": \"PySpark.DataFrame.Filter(str)\"\r\n",
					"            },\r\n",
					"        }\r\n",
					"\r\n",
					"        data_type = \"default_data_type\"\r\n",
					"        api_type = \"default_api_type\"\r\n",
					"        mock_time = \"2021_01_01_00_00_00\"\r\n",
					"        mock_location = f\"errors/validations/{data_type}/columnIsNotNull/{mock_time}\"\r\n",
					"        expected_failed_validations_msg_data = f\"ValidationEngine error for {data_type} on {mock_time}: detected 5 failed columnIsNotNull on columns ['dataChange', 'externalMatchId', 'index', 'bankId']. Review parquet: {mock_location}.\"\r\n",
					"        mock_location_api = f\"errors/validations/{api_type}/columnIsNotNull/{mock_time}\"\r\n",
					"        expected_failed_validations_msg_api = f\"ValidationEngine error for {api_type} on {mock_time}: detected 5 failed columnIsNotNull on columns ['dataChange', 'externalMatchId', 'index', 'bankId']. Review parquet: {mock_location_api}.\"\r\n",
					"\r\n",
					"        test_df = self.create_mock_test()\r\n",
					"\r\n",
					"        mock_params_data = { \r\n",
					"            \"data_type\": data_type,\r\n",
					"            \"run_id\": str(UUID(int=0)),\r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
					"        }\r\n",
					"\r\n",
					"        mock_params_api = { \r\n",
					"            \"api_name\": api_type,\r\n",
					"            \"run_id\": str(UUID(int=0)),\r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
					"        }\r\n",
					"\r\n",
					"        telemetry_client_mock_data = Mock()\r\n",
					"        telemetry_client_mock_api = Mock()\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        validation_engine_data = ValidationEngine(telemetry_client_mock_data, mock_params_data, datalake_client_mock)\r\n",
					"        validation_engine_api = ValidationEngine(telemetry_client_mock_api, mock_params_api, datalake_client_mock)\r\n",
					"\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        valid_rows_df = validation_engine_data.validate_df_columns(test_df, validationColumnRules)\r\n",
					"        valid_rows_result_count = valid_rows_df.count()\r\n",
					"        valid_row = valid_rows_df.take(1)\r\n",
					"        valid_row_result_index = valid_row[0][\"index\"]\r\n",
					"        valid_row_result_id = valid_row[0][\"externalMatchId\"]\r\n",
					"\r\n",
					"        validation_engine_api.validate_df_columns(test_df, validationColumnRules)\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert valid_rows_df is not None, f\"Expected a dataframe, result was None.\"\r\n",
					"        assert valid_rows_result_count == expected_valid_rows, f\"Expected {expected_valid_rows} dataframe rows, result was {valid_rows_result_count}.\"\r\n",
					"        assert valid_row_result_index == expected_valid_index, f\"Expected {expected_valid_index} externalMatchId, result was {valid_row_result_index}.\"\r\n",
					"        assert valid_row_result_id == expected_valid_id, f\"Expected {expected_valid_id} externalMatchId, result was {valid_row_result_id}.\"\r\n",
					"        assert telemetry_client_mock_data.track_exception.call_count == 1, f\"Expected {1} track_exception() call, Result was {telemetry_client_mock_data.track_exception.call_count}\"\r\n",
					"        telemetry_client_mock_data.track_exception.assert_any_call(expected_failed_validations_msg_data)\r\n",
					"        assert telemetry_client_mock_api.track_exception.call_count == 1, f\"Expected {1} track_exception() call, Result was {telemetry_client_mock_api.track_exception.call_count}\"\r\n",
					"        telemetry_client_mock_api.track_exception.assert_any_call(expected_failed_validations_msg_api)\r\n",
					"        assert datalake_client_mock.write_parquet.call_count == 2\r\n",
					"\r\n",
					"    def test_existing_full_count_has_not_dropped_behavior(self):\r\n",
					"        \"\"\"\r\n",
					"            Asserts validation engine's existing_full_count_has_not_dropped() throws errors when validation fails\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        existing_count = 100\r\n",
					"        processed_count = 99\r\n",
					"        \r\n",
					"        mock_params = { \r\n",
					"            \"data_type\": \"default_type\",\r\n",
					"            \"run_id\": str(UUID(int=0)),\r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
					"        }\r\n",
					"\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        validation_engine = ValidationEngine(telemetry_client_mock, mock_params, datalake_client_mock)\r\n",
					"        validation_engine.existing_full_count_has_not_dropped(\r\n",
					"            deviation_percentage=0, \r\n",
					"            existing_full_df_count=99, \r\n",
					"            new_full_df_count=100\r\n",
					"        )\r\n",
					"\r\n",
					"        # ASSERT deviation is below threshold of 0\r\n",
					"        with pytest.raises(Exception) as e:\r\n",
					"            validation_engine.existing_full_count_has_not_dropped(\r\n",
					"                deviation_percentage=0, \r\n",
					"                existing_full_df_count=100, \r\n",
					"                new_full_df_count=90\r\n",
					"            )\r\n",
					"        assert e.type == ValueError, f\"Expected ValueError type. Result was {e.type}.\"\r\n",
					"        assert \"ValidationEngine error for default_type  on 2021_01_01_00_00_00:\" in str(e.value), e.value \r\n",
					"        assert \"Existing count: 100. Processed count: 90.\" in str(e.value), e.value\r\n",
					"\r\n",
					"        # ASSERT exception is logged but not thrown when existing count is 0; this represents initial run\r\n",
					"        validation_engine.existing_full_count_has_not_dropped(\r\n",
					"            deviation_percentage=0, \r\n",
					"            existing_full_df_count=0, \r\n",
					"            new_full_df_count=100, \r\n",
					"            data_type_code=\"Default\"\r\n",
					"        )\r\n",
					"        assert telemetry_client_mock.track_exception.call_count == 1, f\"Expected {1} track_exception() call, Result was {telemetry_client_mock.track_exception.call_count}\"\r\n",
					"        telemetry_client_mock.track_exception.assert_any_call(f\"Existing gold parquet for default_type Default has record count of 0. This should only happen during initial run; if not initial run this is a valid error.\")\r\n",
					"        "
				],
				"execution_count": 7
			}
		]
	}
}