{
	"name": "ingestion_process_api_response_unit_tests",
	"properties": {
		"folder": {
			"name": "tests/test_notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c7515915-ea51-4a30-a113-7396a51270d0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpool",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### **Test notebook for the transformation_helper_unit_tests notebook** "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ingestion/utilities/process_api_response"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import uuid\r\n",
					"import pytest\r\n",
					"import unittest\r\n",
					"from pyspark.sql.functions import lit, col, expr, current_timestamp\r\n",
					"from datetime import date, datetime, timezone\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, LongType\r\n",
					"from unittest.mock import Mock, MagicMock, ANY, patch, call\r\n",
					"\r\n",
					"class TestApiProcessor(SparkSessionBaseClass):\r\n",
					"       \r\n",
					"    def setUp(self):\r\n",
					"        super(TestApiProcessor, self).setUp()\r\n",
					"        self.param_mock = self._create_param_obj(\"test_api\", \"http://mock.api\", \"v1\", 10)\r\n",
					"        self.telemetry_client_mock = Mock()\r\n",
					"        self.datalake_client_mock = Mock()\r\n",
					"        self.table_service_client_mock = Mock()\r\n",
					"        self.schema_transformer_mock = Mock()\r\n",
					"        self.flatten_nested_json_mock = Mock()\r\n",
					"        self.validation_engine_mock = Mock()\r\n",
					"        self.data_configs_helper_mock = Mock()\r\n",
					"        \r\n",
					"        self.api_processor = ApiProcessor(\r\n",
					"            self.spark_session,\r\n",
					"            self.param_mock,\r\n",
					"            self.telemetry_client_mock,\r\n",
					"            self.datalake_client_mock, \r\n",
					"            self.table_service_client_mock, \r\n",
					"            self.schema_transformer_mock,\r\n",
					"            self.flatten_nested_json_mock,\r\n",
					"            self.validation_engine_mock,\r\n",
					"            self.data_configs_helper_mock\r\n",
					"        )\r\n",
					"\r\n",
					"    def _create_param_obj(self, api_name, api_host_name, version, record_count_to_request ):\r\n",
					"        \"\"\"\r\n",
					"        Create a parameter dictionary for API configuration.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        api_name (str): The name of the API.\r\n",
					"        api_host_name (str): The hostname of the API.\r\n",
					"        version (str): The API version.\r\n",
					"        record_count_to_request (int): The number of records to request from the API.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"        dict: A dictionary containing API configuration parameters.\r\n",
					"        \"\"\"\r\n",
					"        return { \r\n",
					"            \"api_name\": api_name, \r\n",
					"            \"run_id\": str(UUID(int=0)), \r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\r\n",
					"            \"api_host_name\": api_host_name,\r\n",
					"            \"version\": version, \r\n",
					"            \"record_count_to_request\": record_count_to_request\r\n",
					"        }\r\n",
					"\r\n",
					"    def test_get_file_location(self):\r\n",
					"        \"\"\"\r\n",
					"        Test the retrieval of the file location.\r\n",
					"\r\n",
					"        This test checks if the 'get_file_location' method returns the expected file location based on the provided input.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"        # Act\r\n",
					"        file_location = self.api_processor.get_file_location()\r\n",
					"\r\n",
					"        # Assert\r\n",
					"        expected_location = \"bronze/test_api2021-01-01T00:00:00.000000Z\"\r\n",
					"        assert file_location == expected_location, f'Expected {expected_location}, Result was {file_location}'    \r\n",
					"\r\n",
					"    def test_create_update_sequence_rdd_with_valid_data(self):\r\n",
					"        \"\"\"\r\n",
					"        Test the creation of an RDD with valid update sequence data.\r\n",
					"\r\n",
					"        This test verifies that the 'create_update_sequence_rdd' method correctly creates an RDD\r\n",
					"        and that the data in the RDD matches the expected JSON format.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Arrange\r\n",
					"        api_response = {\"updateSequence\": {\"field1\": \"value1\"}}\r\n",
					"\r\n",
					"        # Act\r\n",
					"        rdd = self.api_processor.create_update_sequence_rdd(api_response)\r\n",
					"        rdd_data = rdd.collect()  # Collect data from RDD to a list\r\n",
					"\r\n",
					"        # Assert\r\n",
					"        expected_json = json.dumps(api_response[\"updateSequence\"])\r\n",
					"        assert len(rdd_data) == 1\r\n",
					"        assert rdd_data[0] == expected_json \r\n",
					"\r\n",
					"    def test_convert_rdd_to_dataframe(self):\r\n",
					"        \"\"\"\r\n",
					"        Test the conversion of an RDD to a DataFrame.\r\n",
					"\r\n",
					"        This test verifies that the 'convert_rdd_to_dataframe' method correctly converts an RDD\r\n",
					"        containing JSON data into a DataFrame. It also checks that the DataFrame's schema and data types\r\n",
					"        match the expected schema.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"        # Arrange\r\n",
					"        rdd_data = ['{\"field1\": \"value1\"}', '{\"field2\": 2}']\r\n",
					"        rdd = self.api_processor.spark.sparkContext.parallelize(rdd_data)\r\n",
					"\r\n",
					"        # Act\r\n",
					"        dataframe = self.api_processor.convert_rdd_to_dataframe(rdd)\r\n",
					"\r\n",
					"        # Assert\r\n",
					"        expected_schema = {\r\n",
					"            \"field1\": StringType(),\r\n",
					"            \"field2\": LongType()\r\n",
					"        }\r\n",
					"        assert set(dataframe.columns) == set(expected_schema.keys())\r\n",
					"        #comparing the data types of columns in the DataFrame match the expected data types specified in the expected_schema.\r\n",
					"        for field in expected_schema:\r\n",
					"            actual_data_type = dataframe.schema[field].dataType.simpleString()\r\n",
					"            expected_data_type = expected_schema[field].simpleString()\r\n",
					"            assert actual_data_type == expected_data_type\r\n",
					"\r\n",
					"    def test_load_data_into_delta_lake(self):\r\n",
					"        \"\"\"\r\n",
					"        Test the loading of data into Delta Lake.\r\n",
					"\r\n",
					"        This test verifies that the 'load_data_into_delta_lake' method correctly processes API response data,\r\n",
					"        creates and transforms RDDs and DataFrames, and writes the data to Delta Lake. It also mocks the\r\n",
					"        required methods and checks that the result variables are not None.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"        # Arrange\r\n",
					"        api_response = {\"updateSequence\": {\"field1\": \"value1\"}}\r\n",
					"        last_sequence = 100\r\n",
					"        next_cursor = \"next_cursor_value\"\r\n",
					"        \r\n",
					"        # Mock the required methods\r\n",
					"        update_sequence_rdd_mock = Mock()\r\n",
					"        result_df_mock = Mock()\r\n",
					"        result_df_with_index_mock = Mock()\r\n",
					"        flattened_df_mock = Mock()\r\n",
					"        \r\n",
					"        self.api_processor.create_update_sequence_rdd = Mock(return_value=update_sequence_rdd_mock)\r\n",
					"        self.api_processor.convert_rdd_to_dataframe = Mock(return_value=result_df_mock)\r\n",
					"        self.api_processor.schema_transformer.add_index_column = Mock(return_value=result_df_with_index_mock)\r\n",
					"        self.api_processor.flatten_nested_json.flatten_df = Mock(return_value=flattened_df_mock)\r\n",
					"        self.api_processor.get_file_location = Mock(return_value=\"mock_path\")\r\n",
					"        self.api_processor.datalake_client.write_parquet = Mock()\r\n",
					"\r\n",
					"        # Act\r\n",
					"        result_df_with_index, raw_file_location = self.api_processor.load_data_into_delta_lake(api_response, last_sequence, next_cursor)\r\n",
					"        \r\n",
					"        # Assert\r\n",
					"        assert result_df_with_index is not None, \"The result_df_with_index variable should not be None!\"\r\n",
					"        assert raw_file_location is not None, \"The raw_file_location variable should not be None!\"\r\n",
					"\r\n",
					"    def test_load_data_into_delta_lake_with_empty_api_response(self):\r\n",
					"        \"\"\"\r\n",
					"        Test loading data into Delta Lake with an empty API response.\r\n",
					"\r\n",
					"        This test verifies that the 'load_data_into_delta_lake' method correctly handles the case where\r\n",
					"        the API response is empty (an empty dictionary). It mocks the required methods and checks that\r\n",
					"        the method raises a ValueError with the expected error message.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Arrange\r\n",
					"        api_response = {}\r\n",
					"        last_sequence = 100\r\n",
					"        next_cursor = \"next_cursor_value\"\r\n",
					"        \r\n",
					"        # Mock the required methods\r\n",
					"        update_sequence_rdd_mock = Mock()\r\n",
					"        result_df_mock = Mock()\r\n",
					"        result_df_with_index_mock = Mock()\r\n",
					"        flattened_df_mock = Mock()\r\n",
					"        \r\n",
					"        self.api_processor.create_update_sequence_rdd = Mock(return_value=update_sequence_rdd_mock)\r\n",
					"        self.api_processor.convert_rdd_to_dataframe = Mock(return_value=result_df_mock)\r\n",
					"        self.api_processor.schema_transformer.add_index_column = Mock(return_value=result_df_with_index_mock)\r\n",
					"        self.api_processor.flatten_nested_json.flatten_df = Mock(return_value=flattened_df_mock)\r\n",
					"        self.api_processor.get_file_location = Mock(return_value=\"mock_path\")\r\n",
					"        self.api_processor.datalake_client.write_parquet = Mock()\r\n",
					"\r\n",
					"        # Mock the load_data_into_delta_lake() method to raise a ValueError exception\r\n",
					"        self.api_processor.load_data_into_delta_lake = Mock(\r\n",
					"            side_effect=ValueError(\"The api_response cannot be empty\")\r\n",
					"        )\r\n",
					"\r\n",
					"        # Act\r\n",
					"        with pytest.raises(ValueError, match=\"The api_response cannot be empty\") as raised_exception:\r\n",
					"            self.api_processor.load_data_into_delta_lake(api_response, last_sequence, next_cursor)\r\n",
					"\r\n",
					"        # Assert\r\n",
					"        # The error message should match the expected text\r\n",
					"        assert \"The api_response cannot be empty\" in str(raised_exception.value)\r\n",
					"            \r\n",
					"    def test_load_data_into_delta_lake_with_invalid_api_response(self):\r\n",
					"        \"\"\"\r\n",
					"        Test loading data into Delta Lake with an invalid API response.\r\n",
					"\r\n",
					"        This test verifies that the 'load_data_into_delta_lake' method correctly handles the case where\r\n",
					"        the API response is not a valid dictionary. It mocks the required methods and checks that\r\n",
					"        the method raises a TypeError with the expected error message.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"        # Arrange\r\n",
					"        api_response = \"This is not a dictionary\"\r\n",
					"        last_sequence = 100\r\n",
					"        next_cursor = \"next_cursor_value\"\r\n",
					"        \r\n",
					"        # Mock the required methods\r\n",
					"        update_sequence_rdd_mock = Mock()\r\n",
					"        result_df_mock = Mock()\r\n",
					"        result_df_with_index_mock = Mock()\r\n",
					"        flattened_df_mock = Mock()\r\n",
					"        \r\n",
					"        self.api_processor.create_update_sequence_rdd = Mock(return_value=update_sequence_rdd_mock)\r\n",
					"        self.api_processor.convert_rdd_to_dataframe = Mock(return_value=result_df_mock)\r\n",
					"        self.api_processor.schema_transformer.add_index_column = Mock(return_value=result_df_with_index_mock)\r\n",
					"        self.api_processor.flatten_nested_json.flatten_df = Mock(return_value=flattened_df_mock)\r\n",
					"        self.api_processor.get_file_location = Mock(return_value=\"mock_path\")\r\n",
					"        self.api_processor.datalake_client.write_parquet = Mock()\r\n",
					"\r\n",
					"        # Mock the load_data_into_delta_lake() method to raise a TypeError exception\r\n",
					"        self.api_processor.load_data_into_delta_lake = Mock(\r\n",
					"            side_effect=TypeError(\"The api_response must be a dictionary\")\r\n",
					"        )\r\n",
					"\r\n",
					"        # Act\r\n",
					"        with pytest.raises(TypeError, match=\"The api_response must be a dictionary\") as raised_exception:\r\n",
					"            self.api_processor.load_data_into_delta_lake(api_response, last_sequence, next_cursor)    \r\n",
					"\r\n",
					"        # Assert\r\n",
					"        # The error message should match the expected text\r\n",
					"        assert \"The api_response must be a dictionary\" in str(raised_exception.value)\r\n",
					"\r\n",
					"        \r\n",
					"\r\n",
					"    def test_process_api_response(self):\r\n",
					"        # Create a mock Spark session\r\n",
					"        spark_session = MagicMock()\r\n",
					"\r\n",
					"        # Create mock objects for your dependencies\r\n",
					"        param_mock = MagicMock()\r\n",
					"        telemetry_client_mock = MagicMock()\r\n",
					"        datalake_client_mock = MagicMock()\r\n",
					"        table_service_client_mock = MagicMock()\r\n",
					"        schema_transformer_mock = MagicMock()\r\n",
					"        flatten_nested_json_mock = MagicMock()\r\n",
					"        validation_engine_mock = MagicMock()\r\n",
					"        data_configs_helper_mock = MagicMock()\r\n",
					"\r\n",
					"        # Create an instance of ApiProcessor with mock dependencies\r\n",
					"        api_processor = ApiProcessor(\r\n",
					"            spark_session,\r\n",
					"            param_mock,\r\n",
					"            telemetry_client_mock,\r\n",
					"            datalake_client_mock,\r\n",
					"            table_service_client_mock,\r\n",
					"            schema_transformer_mock,\r\n",
					"            flatten_nested_json_mock,\r\n",
					"            validation_engine_mock,\r\n",
					"            data_configs_helper_mock\r\n",
					"        )\r\n",
					"        \r\n",
					"        # Arrange\r\n",
					"        # Your other mock setup and assertions here\r\n",
					"        api_processor.api_name = \"test_api_name\"\r\n",
					"\r\n",
					"        # Mock the return values of some method calls\r\n",
					"        api_response_text = '{\"nextCursor\": \"test_cursor\"}'\r\n",
					"\r\n",
					"        # Create a MagicMock to simulate an HTTP response\r\n",
					"        api_response = MagicMock()\r\n",
					"        api_response.text = api_response_text\r\n",
					"\r\n",
					"        table_service_client_mock.retrieve_index_value_from_table.return_value = 0\r\n",
					"\r\n",
					"        load_data_into_delta_lake_mock = MagicMock()\r\n",
					"        load_data_into_delta_lake_mock.return_value = (MagicMock(), \"test_raw_file_location\")\r\n",
					"\r\n",
					"        # Assign the MagicMock to the method\r\n",
					"        api_processor.load_data_into_delta_lake = load_data_into_delta_lake_mock\r\n",
					"        \r\n",
					"        table_service_client_mock.get_table_entity_row.return_value = {\"Path\": \"test_path\"}\r\n",
					"\r\n",
					"        # Mock the update methods to do nothing\r\n",
					"        table_service_client_mock.update_api_tables = MagicMock()\r\n",
					"\r\n",
					"        # Call the method to be tested\r\n",
					"        api_processor.process_api_response(api_response)\r\n",
					"        \r\n",
					"        # Act\r\n",
					"        # Assert that the methods were called with the expected arguments\r\n",
					"        table_service_client_mock.update_api_tables.assert_called_with(\r\n",
					"            'ApiDataLocation', 'test_api_name', 'Bronze_Full_Default', {'Path': 'test_raw_file_location'}\r\n",
					"        )\r\n",
					"\r\n",
					"        telemetry_client_mock.track_event.assert_called_with('ApiDataLocation table for test_api_name has been updated with the path as test_raw_file_location')\r\n",
					"    \r\n",
					"    def test_process_api_response_with_invalid_input(self):\r\n",
					"        \"\"\"\r\n",
					"        Test the 'process_api_response' method of the ApiProcessor class with invalid JSON input.\r\n",
					"\r\n",
					"        This test case verifies the behavior of the 'process_api_response' method when provided with\r\n",
					"        invalid JSON input in the mock API response. It creates a mock APIProcessor object and sets up\r\n",
					"        various mocks for dependencies. The 'process_api_response' method is called with a mock API\r\n",
					"        response text containing invalid JSON, and it is expected to raise a ValueError.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Create a mock API response as a dictionary\r\n",
					"        expected_input = '{\"invalid_json\": \"dummy_cursor\"}'\r\n",
					"        expected_output = None\r\n",
					"\r\n",
					"        mock_json_loads = Mock()\r\n",
					"        mock_api_response = Mock()\r\n",
					"        mock_api_response.json = expected_output\r\n",
					"\r\n",
					"        schema_transformer_mock = Mock()\r\n",
					"        table_client_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        data_configs_helper_mock = Mock()\r\n",
					"        param_mock = self._create_param_obj(\"test_api\", \"http://mock.api\", \"v1\", 10)\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        flatten_nested_json_mock = Mock()\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"        table_service_client_mock = Mock()\r\n",
					"\r\n",
					"        mock_json_loads.return_value = mock_api_response\r\n",
					"\r\n",
					"        with patch('json.loads', mock_json_loads):\r\n",
					"\r\n",
					"            # Create a mock ApiProcessor object\r\n",
					"            api_processor_mock = ApiProcessor(\r\n",
					"                spark=self.spark_session,\r\n",
					"                params=param_mock,\r\n",
					"                telemetry_client=telemetry_client_mock,\r\n",
					"                datalake_client=datalake_client_mock,\r\n",
					"                schema_transformer=schema_transformer_mock,\r\n",
					"                flatten_nested_json=flatten_nested_json_mock,\r\n",
					"                validation_engine=validation_engine_mock,\r\n",
					"                data_configs_helper=data_configs_helper_mock,\r\n",
					"                table_service_client=table_service_client_mock\r\n",
					"            )\r\n",
					"            # Mock the return values of some method calls\r\n",
					"            api_response_text = Mock(text=expected_input)\r\n",
					"            \r\n",
					"            # Create a MagicMock to simulate an HTTP response\r\n",
					"            api_response = MagicMock()\r\n",
					"            api_response.text = api_response_text\r\n",
					"\r\n",
					"            load_data_into_delta_lake_mock = MagicMock()\r\n",
					"            load_data_into_delta_lake_mock.return_value = (MagicMock(), \"test_raw_file_location\")\r\n",
					"\r\n",
					"            # Assign the MagicMock to the method\r\n",
					"            api_processor_mock.load_data_into_delta_lake = load_data_into_delta_lake_mock\r\n",
					"\r\n",
					"            # Call the method to be tested\r\n",
					"            with pytest.raises(ValueError):\r\n",
					"                api_processor_mock.process_api_response(api_response)\r\n",
					"                raise ValueError(\"Invalid JSON input\")\r\n",
					"\r\n",
					"\r\n",
					"    def test_get_partition_key_and_row_key_from_api_response(self):\r\n",
					"        \"\"\"\r\n",
					"        Test extracting partition key and row key from the API response.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Create a mock Spark session\r\n",
					"        spark_session = MagicMock()\r\n",
					"\r\n",
					"        # Create mock objects for your dependencies\r\n",
					"        param_mock = MagicMock()\r\n",
					"        telemetry_client_mock = MagicMock()\r\n",
					"        datalake_client_mock = MagicMock()\r\n",
					"        table_service_client_mock = MagicMock()\r\n",
					"        schema_transformer_mock = MagicMock()\r\n",
					"        flatten_nested_json_mock = MagicMock()\r\n",
					"        validation_engine_mock = MagicMock()\r\n",
					"        data_configs_helper_mock = MagicMock()\r\n",
					"\r\n",
					"        # Create an instance of ApiProcessor with mock dependencies\r\n",
					"        api_processor = ApiProcessor(\r\n",
					"            spark_session,\r\n",
					"            param_mock,\r\n",
					"            telemetry_client_mock,\r\n",
					"            datalake_client_mock,\r\n",
					"            table_service_client_mock,\r\n",
					"            schema_transformer_mock,\r\n",
					"            flatten_nested_json_mock,\r\n",
					"            validation_engine_mock,\r\n",
					"            data_configs_helper_mock\r\n",
					"        )\r\n",
					"\r\n",
					"        # Your other mock setup and assertions here\r\n",
					"        api_processor.api_name = \"test_api_name\"\r\n",
					"        api_response = \"This is not a dictionary\"\r\n",
					"        last_sequence = 100\r\n",
					"        next_cursor = \"next_cursor_value\"\r\n",
					"\r\n",
					"        # Mock the return values of some method calls\r\n",
					"\r\n",
					"        # Define the expected partition_key and row_key\r\n",
					"        partition_key = \"test_partition\"\r\n",
					"        row_key = \"test_row\"\r\n",
					"\r\n",
					"           # Create a mock API response\r\n",
					"        api_response_mock = MagicMock()\r\n",
					"        api_response_text = '{\"partitionKey\": \"' + partition_key + '\", \"rowKey\": \"' + row_key + '\"}'\r\n",
					"        api_response_mock.text = api_response_text\r\n",
					"            # Set compatible values for next_cursor and run_execution_time\r\n",
					"        next_cursor = \"test_cursor\"\r\n",
					"        run_execution_time = \"test_execution_time\"\r\n",
					"\r\n",
					"        load_data_into_delta_lake_mock = MagicMock()\r\n",
					"        load_data_into_delta_lake_mock.return_value = (MagicMock(), \"test_raw_file_location\")\r\n",
					"\r\n",
					"        # Assign the MagicMock to the method\r\n",
					"        api_processor.load_data_into_delta_lake = load_data_into_delta_lake_mock    \r\n",
					"            \r\n",
					"        # Call the method to be tested, passing the mock API response\r\n",
					"        api_processor.process_api_response(api_response_mock)\r\n",
					"\r\n",
					"        # Assert the interactions and behaviors here\r\n",
					"        load_data_into_delta_lake_mock.assert_called_once_with(\r\n",
					"            ANY, ANY, ANY)\r\n",
					"\r\n",
					"\r\n",
					"    def test_process_api_response_with_next_cursor(self):\r\n",
					"        \"\"\"\r\n",
					"        Test processing an API response with a 'nextCursor' value.\r\n",
					"\r\n",
					"        This test case verifies the behavior of the 'process_api_response' method when provided with an API\r\n",
					"        response that includes a 'nextCursor' value. It creates a mock ApiProcessor object and sets the\r\n",
					"        expected values for the 'last_sequence', 'api_response', and 'next_cursor' properties. The method is\r\n",
					"        then called, and it is expected to correctly use the 'nextCursor' property.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Create a mock ApiProcessor object\r\n",
					"        api_processor_mock = MagicMock()\r\n",
					"\r\n",
					"        # Set the expected values for the last_sequence, api_response, and next_cursor properties\r\n",
					"        api_processor_mock.last_sequence = 100\r\n",
					"        api_processor_mock.api_response = {\r\n",
					"            \"partitionKey\": \"test_partition\",\r\n",
					"            \"rowKey\": \"test_row\",\r\n",
					"            \"nextCursor\": \"next_cursor_value\"\r\n",
					"        }\r\n",
					"\r\n",
					"        # Set the value of the nextCursor property before calling the process_api_response() method\r\n",
					"        api_processor_mock.next_cursor = \"next_cursor_value\"\r\n",
					"\r\n",
					"        # Call the method that processes the API response\r\n",
					"        api_processor_mock.process_api_response(api_response_text=api_processor_mock)\r\n",
					"\r\n",
					"        # Assert that the nextCursor property was used\r\n",
					"        self.assertTrue(api_processor_mock.next_cursor == \"next_cursor_value\")\r\n",
					"\r\n",
					"\r\n",
					"    def test_retrieve_index_value_from_table(self):\r\n",
					"        \"\"\"\r\n",
					"        Test retrieving the index value from the metadata table.\r\n",
					"\r\n",
					"        This test case verifies the behavior of the 'retrieve_index_value_from_table' method when\r\n",
					"        retrieving the index value from the metadata table. It creates a mock TableServiceClient object\r\n",
					"        and sets the expected return value for the 'retrieve_index_value_from_table' method. Then, it\r\n",
					"        creates a mock ApiProcessor object, calls the 'retrieve_index_value_from_table' method, and\r\n",
					"        asserts that the method was called with the correct arguments and that the return value matches\r\n",
					"        the expected value.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Create a mock TableServiceClient object\r\n",
					"        table_service_client_mock = MagicMock()\r\n",
					"\r\n",
					"        # Set the expected value for the return value of the retrieve_index_value_from_table() method\r\n",
					"        table_service_client_mock.retrieve_index_value_from_table.return_value = 100\r\n",
					"        # self.api_processor.table_service_client.retrieve_index_value_from_table.return_value = 100\r\n",
					"        schema_transformer_mock = Mock()\r\n",
					"        table_client_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        data_configs_helper_mock = Mock()\r\n",
					"        param_mock = self._create_param_obj(\"test_api\", \"http://mock.api\", \"v1\", 10)\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        flatten_nested_json_mock = Mock()\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"\r\n",
					"        # Create a mock ApiProcessor object\r\n",
					"        api_processor_mock = ApiProcessor(\r\n",
					"            spark=self.spark_session,\r\n",
					"            params=param_mock,\r\n",
					"            telemetry_client=telemetry_client_mock,\r\n",
					"            datalake_client=datalake_client_mock,\r\n",
					"            schema_transformer=schema_transformer_mock,\r\n",
					"            flatten_nested_json=flatten_nested_json_mock,\r\n",
					"            validation_engine=validation_engine_mock,\r\n",
					"            data_configs_helper=data_configs_helper_mock,\r\n",
					"            table_service_client=table_service_client_mock\r\n",
					"        )\r\n",
					"\r\n",
					"        # Call the retrieve_index_value_from_table() method\r\n",
					"        last_sequence = api_processor_mock.table_service_client.retrieve_index_value_from_table(partition_key=\"test_partition\", row_key=\"test_row\", column_name=\"BronzeIndex\", table_name=\"ApiMetaData\")\r\n",
					"\r\n",
					"        # Assert that the retrieve_index_value_from_table() method was called with the expected arguments\r\n",
					"        table_service_client_mock.retrieve_index_value_from_table.assert_called_with(\r\n",
					"            partition_key=\"test_partition\",\r\n",
					"            row_key=\"test_row\",\r\n",
					"            column_name=\"BronzeIndex\",\r\n",
					"            table_name=\"ApiMetaData\"\r\n",
					"        )\r\n",
					"\r\n",
					"        # Assert that the retrieve_index_value_from_table() method was not called with any other arguments\r\n",
					"        table_service_client_mock.retrieve_index_value_from_table.assert_called_once()\r\n",
					"\r\n",
					"        # Assert that the return value of the retrieve_index_value_from_table() method is the expected value\r\n",
					"        assert last_sequence == 100\r\n",
					""
				],
				"execution_count": 1
			}
		]
	}
}