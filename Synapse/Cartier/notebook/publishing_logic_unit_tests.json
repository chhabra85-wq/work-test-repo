{
	"name": "publishing_logic_unit_tests",
	"properties": {
		"folder": {
			"name": "tests/test_notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d74b7de5-3f5f-4649-9db3-c433dad3122e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpoolv34",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpoolv34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run publishing/logics/publishing_logic"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"from unittest.mock import Mock, MagicMock, ANY\r\n",
					"from datetime import datetime, timezone\r\n",
					"from uuid import UUID\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\r\n",
					"from pyspark.sql.functions import expr, lit\r\n",
					"\r\n",
					"class TestPublishingLogicUnit(SparkSessionBaseClass):\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        super().setUp()\r\n",
					"        \r\n",
					"    def _create_transformed_calculated_time(self):\r\n",
					"        calculated_time_schema = StructType([        \r\n",
					"            StructField(\"externalMatchId\", StringType(), nullable=True),\r\n",
					"            StructField(\"shiftId\", StringType(), nullable=True),\r\n",
					"            StructField(\"workDate\", StringType(), nullable=True),\r\n",
					"            StructField(\"endAt\", StringType(), nullable=True),\r\n",
					"            StructField(\"startAt\", StringType(), nullable=True),\r\n",
					"            StructField(\"shifts_activities_activityId\", StringType(), nullable=True),\r\n",
					"            StructField(\"shifts_activities_startAt\", StringType(), nullable=True),\r\n",
					"            StructField(\"shifts_activities_task\", StringType(), nullable=True),\r\n",
					"            StructField(\"dataChange\", StringType(), nullable=True),\r\n",
					"            StructField(\"index\", IntegerType(), nullable=True),\r\n",
					"        ])\r\n",
					"\r\n",
					"        calculated_time_abs_data = [\r\n",
					"            (\"id_1\", \"shift1\", \"2023-01-01\", \"2023-01-01\", \"2023-01-01\", \"activityId1\", \"2023-01-01T09:00:00\", \"task1\", \"MERGE\", 1),\r\n",
					"            (\"id_2\", \"shift2\", \"2023-01-02\", \"2023-01-02\", \"2023-01-02\", \"activityId2\", \"2023-01-02T09:00:00\", \"task2\", \"MERGE\", 2),\r\n",
					"            (\"id_3\", \"shift3\", \"2023-01-03\", \"2023-01-03\", \"2023-01-03\", \"activityId3\", \"2023-01-03T09:00:00\", \"task3\", \"MERGE\", 3),\r\n",
					"            (\"id_4\", \"shift4\", \"2023-01-04\", \"2023-01-04\", \"2023-01-04\", \"activityId4\", \"2023-01-04T09:00:00\", \"task4\", \"DELETE\", 4),\r\n",
					"            (\"id_5\", \"shift5\", \"2023-01-05\", \"2023-01-05\", \"2023-01-05\", \"activityId5\", \"2023-01-05T09:00:00\", \"task5\", \"DELETE\", 5)\r\n",
					"        ]\r\n",
					"\r\n",
					"        return self.spark_session.createDataFrame(calculated_time_abs_data, calculated_time_schema)\r\n",
					"\r\n",
					"    def _create_param_obj(self, data_type):\r\n",
					"        return { \r\n",
					"            \"data_type\": data_type, \r\n",
					"            \"run_id\": str(UUID(int=0)), \r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\r\n",
					"            \"time_earnings_code\": None\r\n",
					"        }\r\n",
					"\r\n",
					"    def test_invocations_full_and_delta_default(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the invocations of transformation logic for both full & delta files\r\n",
					"        Asserts expected invocation calls for non calculated-time datasets \r\n",
					"        \"\"\"     \r\n",
					"        # ARRANGE\r\n",
					"        data_type = \"shift\"\r\n",
					"        latest_data_location_mock = f\"silver/location/{data_type}\"        \r\n",
					"\r\n",
					"        spark_session_mock = Mock()\r\n",
					"        api_metadata_table_mock = Mock()\r\n",
					"        hrdl_jobs_table_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        params_mock = self._create_param_obj(data_type)\r\n",
					"\r\n",
					"        api_data_location_entity = ApiDataLocationEntity(\r\n",
					"            partition_key = data_type, \r\n",
					"            row_key = data_type, \r\n",
					"            dataset_location = latest_data_location_mock,\r\n",
					"            gold_index = 0,\r\n",
					"            is_processed = False)            \r\n",
					"        api_data_location_entity = api_data_location_entity.to_dict()\r\n",
					"\r\n",
					"        table_client_mock = Mock()\r\n",
					"        table_client_mock.get_table_entity_row = MagicMock(return_value = api_data_location_entity)\r\n",
					" \r\n",
					"        transformed_df = self._create_transformed_calculated_time()\r\n",
					"\r\n",
					"        spark_session_mock.createDataFrame = MagicMock(return_value = transformed_df)\r\n",
					"\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        datalake_client_mock.read_parquet = MagicMock(return_value = transformed_df) \r\n",
					"\r\n",
					"        data_configs_helper = Mock()\r\n",
					"        data_configs_helper.get_gold_validation_df_rules_init_schema = MagicMock(return_value = {\"externalMatchId\": \"StringType()\", \"index\": \"IntegerType()\", \"workDate\": \"StringType()\" })\r\n",
					"        data_configs_helper.update_df_column_names = MagicMock(return_value = transformed_df)\r\n",
					"\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"        validation_engine_mock.validate_df_columns = MagicMock(return_value = transformed_df)\r\n",
					"\r\n",
					"        ms_spark_utils_mock = Mock()\r\n",
					"        ms_spark_utils_mock.list_directory_contents = MagicMock(return_value = [\r\n",
					"            MockFileInfo(\"part-test-1.snappy.parquet\", \"root/file/system/part-test-1.snappy.parquet\"), \r\n",
					"            MockFileInfo(\"_SUCCESS\", \"root/file/system/_SUCCESS\")\r\n",
					"        ])\r\n",
					"\r\n",
					"        ms_spark_utils_mock.does_path_exist = MagicMock(return_value = False)\r\n",
					"\r\n",
					"        publishing_logic_engine = PublishingLogicEngine(\r\n",
					"            spark_session_mock, \r\n",
					"            data_configs_helper, \r\n",
					"            datalake_client_mock, \r\n",
					"            validation_engine_mock, \r\n",
					"            table_client_mock, \r\n",
					"            api_metadata_table_mock, \r\n",
					"            telemetry_client_mock,\r\n",
					"            ms_spark_utils_mock,\r\n",
					"            params_mock)\r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        publishing_logic_engine.execute()     \r\n",
					"\r\n",
					"        # ASSERT CALLED\r\n",
					"        assert telemetry_client_mock.track_event.call_count == 25, f\"Expected 25 track_event() calls, Result was {telemetry_client_mock.track_event.call_count}\"\r\n",
					"        assert table_client_mock.get_table_entity_row.call_count == 7, f\"Expected 7 get_table_entity_row() calls, Result was {table_client_mock.get_table_entity_row.call_count}\"\r\n",
					"        assert datalake_client_mock.read_parquet.call_count == 2, f\"Expected 2 read_parquet() calls, Result was {datalake_client_mock.read_parquet.call_count}\"\r\n",
					"        assert data_configs_helper.get_gold_validation_df_rules_init_schema.call_count == 2, f\"Expected 2 get_gold_validation_df_rules_init_schema() calls, Result was {data_configs_helper.get_gold_validation_df_rules_init_schema.call_count}\"  \r\n",
					"        assert validation_engine_mock.validate_df_columns.call_count == 1, f\"Expected 1 validate_df_columns() calls, Result was {validation_engine_mock.validate_df_columns.call_count}\"  \r\n",
					"        assert validation_engine_mock.validate_df_schema.call_count == 2, f\"Expected 2 validate_df_schema() calls, Result was {validation_engine_mock.validate_df_schema.call_count}\"\r\n",
					"        assert data_configs_helper.update_df_column_names.call_count == 2, f\"Expected 2 update_df_column_names() calls, Result was {data_configs_helper.update_df_column_names.call_count}\"\r\n",
					"        assert table_client_mock.update_table_entity_row.call_count == 4, f\"Expected 4 update_table_entity_row() calls, Result was {table_client_mock.update_table_entity_row.call_count}\"\r\n",
					"        assert spark_session_mock.createDataFrame.call_count == 2, f\"Expected 2 createDataFrame() calls, Result was {spark_session_mock.createDataFrame.call_count}\"\r\n",
					"        assert datalake_client_mock.write_parquet.call_count == 4, f\"Expected 4 write_parquet() calls, Result was {datalake_client_mock.write_parquet.call_count}\"\r\n",
					"        assert table_client_mock.create_entity.call_count == 1, f\"Expected 1 create_entity() calls, Result was {table_client_mock.create_entity.call_count}\"\r\n",
					"\r\n",
					"    def test_invocations_full_and_delta_exit_path(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the invocations of transformation logic for both full & delta files when files already exists\r\n",
					"        Asserts processing calls are not executed\r\n",
					"        \"\"\"     \r\n",
					"        # ARRANGE\r\n",
					"        data_type = \"shift\"\r\n",
					"        latest_data_location_mock = f\"silver/location/{data_type}\"        \r\n",
					"\r\n",
					"        spark_session_mock = Mock()\r\n",
					"        api_metadata_table_mock = Mock()\r\n",
					"        hrdl_jobs_table_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        params_mock = self._create_param_obj(data_type)\r\n",
					"\r\n",
					"        api_data_location_entity = ApiDataLocationEntity(\r\n",
					"            partition_key = data_type, \r\n",
					"            row_key = data_type, \r\n",
					"            dataset_location = latest_data_location_mock,\r\n",
					"            gold_index = 0,\r\n",
					"            is_processed = False)            \r\n",
					"        api_data_location_entity = api_data_location_entity.to_dict()\r\n",
					"\r\n",
					"        table_client_mock = Mock()\r\n",
					"        table_client_mock.get_table_entity_row = MagicMock(return_value = api_data_location_entity)\r\n",
					" \r\n",
					"        transformed_df = self._create_transformed_calculated_time()\r\n",
					"\r\n",
					"        spark_session_mock.createDataFrame = MagicMock(return_value = transformed_df)\r\n",
					"\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        datalake_client_mock.read_parquet = MagicMock(return_value = transformed_df) \r\n",
					"\r\n",
					"        data_configs_helper = Mock()\r\n",
					"        data_configs_helper.get_gold_validation_df_rules_init_schema = MagicMock(return_value = {\"externalMatchId\": \"StringType()\", \"index\": \"IntegerType()\", \"workDate\": \"StringType()\" })\r\n",
					"        data_configs_helper.update_df_column_names = MagicMock(return_value = transformed_df)\r\n",
					"\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"        validation_engine_mock.validate_df_columns = MagicMock(return_value = transformed_df)\r\n",
					"\r\n",
					"        ms_spark_utils_mock = Mock()\r\n",
					"        ms_spark_utils_mock.list_directory_contents = MagicMock(return_value = [\r\n",
					"            MockFileInfo(\"part-test-1.snappy.parquet\", \"root/file/system/part-test-1.snappy.parquet\"), \r\n",
					"            MockFileInfo(\"_SUCCESS\", \"root/file/system/_SUCCESS\")\r\n",
					"        ])\r\n",
					"\r\n",
					"        ms_spark_utils_mock.does_path_exist = MagicMock(return_value = True)\r\n",
					"\r\n",
					"        publishing_logic_engine = PublishingLogicEngine(\r\n",
					"            spark_session_mock, \r\n",
					"            data_configs_helper, \r\n",
					"            datalake_client_mock, \r\n",
					"            validation_engine_mock, \r\n",
					"            table_client_mock, \r\n",
					"            api_metadata_table_mock, \r\n",
					"            telemetry_client_mock,\r\n",
					"            ms_spark_utils_mock,\r\n",
					"            params_mock)\r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        publishing_logic_engine.execute()     \r\n",
					"\r\n",
					"        # ASSERT CALLED\r\n",
					"        assert telemetry_client_mock.track_event.call_count == 13, f\"Expected 13 track_event() calls, Result was {telemetry_client_mock.track_event.call_count}\"\r\n",
					"        table_client_mock.get_table_entity_row.assert_not_called()\r\n",
					"        datalake_client_mock.read_parquet.assert_not_called()\r\n",
					"        data_configs_helper.get_gold_validation_df_rules_init_schema.assert_not_called()\r\n",
					"        validation_engine_mock.validate_df_columns.assert_not_called()\r\n",
					"        validation_engine_mock.validate_df_schema.assert_not_called()\r\n",
					"        data_configs_helper.update_df_column_names.assert_not_called()\r\n",
					"        table_client_mock.update_table_entity_row.assert_not_called()\r\n",
					"        spark_session_mock.createDataFrame.assert_not_called()\r\n",
					"        datalake_client_mock.write_parquet.assert_not_called()\r\n",
					"        table_client_mock.create_entity.assert_not_called()\r\n",
					"\r\n",
					"    def test_get_hrdl_entity_naming(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of get_hrdl_entity_naming\r\n",
					"        Asserts return values of method for given data set type\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        spark_session_mock = Mock()\r\n",
					"        api_metadata_table_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        params_mock = self._create_param_obj(\"calculated-time\")\r\n",
					"        table_client_mock = Mock()\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        data_configs_helper = Mock()\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"        ms_spark_utils_mock = Mock()\r\n",
					"\r\n",
					"        publishing_logic_engine = PublishingLogicEngine(\r\n",
					"            spark_session_mock, \r\n",
					"            data_configs_helper, \r\n",
					"            datalake_client_mock, \r\n",
					"            validation_engine_mock, \r\n",
					"            table_client_mock, \r\n",
					"            api_metadata_table_mock, \r\n",
					"            telemetry_client_mock,\r\n",
					"            ms_spark_utils_mock,\r\n",
					"            params_mock)\r\n",
					"        \r\n",
					"        expected_shift = \"TR01ShiftRecords\"\r\n",
					"        expected_calc_att = \"TR01TimeRecords\"\r\n",
					"        expected_calc_abs = \"TR01TimeOffRecords\"\r\n",
					"        expected_calc_absSecured = \"TR01TimeOffRecordsS\"\r\n",
					"\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        shift_result = publishing_logic_engine.get_hrdl_entity_naming(\"shift\", None)\r\n",
					"        calculated_time_att_result = publishing_logic_engine.get_hrdl_entity_naming(\"calculated-time\", \"ATT\")\r\n",
					"        calculated_time_abs_result = publishing_logic_engine.get_hrdl_entity_naming(\"calculated-time\", \"ABS\")\r\n",
					"        calculated_time_absSecured_result = publishing_logic_engine.get_hrdl_entity_naming(\"calculated-time\", \"ABS-secured\")\r\n",
					"\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert shift_result == expected_shift, f\"Expected {expected_shift}. Result was {shift_result}\"\r\n",
					"        assert calculated_time_att_result == expected_calc_att, f\"Expected {expected_calc_att}. Result was {calculated_time_att_result}\"\r\n",
					"        assert calculated_time_abs_result == expected_calc_abs, f\"Expected {expected_calc_abs}. Result was {calculated_time_abs_result}\"\r\n",
					"        assert calculated_time_absSecured_result == expected_calc_absSecured, f\"Expected {expected_calc_absSecured}. Result was {calculated_time_absSecured_result}\"\r\n",
					"\r\n",
					"    def test_get_codes(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of get_codes\r\n",
					"        Asserts [\"ATT\", \"ABS\", \"ABS-secured\"] is returned for Calculated-time else [\"Default\"]\r\n",
					"        \"\"\"             \r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        spark_session_mock = Mock()\r\n",
					"        api_metadata_table_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        params_mock = self._create_param_obj(\"calculated-time\")\r\n",
					"        table_client_mock = Mock()\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        data_configs_helper = Mock()\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"        ms_spark_utils_mock = Mock()\r\n",
					"\r\n",
					"        publishing_logic_engine = PublishingLogicEngine(\r\n",
					"            spark_session_mock, \r\n",
					"            data_configs_helper, \r\n",
					"            datalake_client_mock, \r\n",
					"            validation_engine_mock, \r\n",
					"            table_client_mock, \r\n",
					"            api_metadata_table_mock, \r\n",
					"            telemetry_client_mock,\r\n",
					"            ms_spark_utils_mock,\r\n",
					"            params_mock)  \r\n",
					"\r\n",
					"        expected_calc_time_code = [\"ATT\", \"ABS\", \"ABS-secured\"]\r\n",
					"        expected_other_code = [\"Default\"]                      \r\n",
					"\r\n",
					"        # ACT\r\n",
					"        calc_time_code_result = publishing_logic_engine.get_codes(\"calculated-time\") \r\n",
					"        other_code_result = publishing_logic_engine.get_codes(\"other\")\r\n",
					"        \r\n",
					"        assert calc_time_code_result == expected_calc_time_code, f\"Expected {expected_calc_time_code}. Result was {calc_time_code_result}\"\r\n",
					"        assert other_code_result == expected_other_code, f\"Expected {expected_other_code}. Result was {other_code_result}\"\r\n",
					"\r\n",
					"    def test_data_paths(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of get_codes\r\n",
					"        Asserts [\"ATT\", \"ABS\", \"ABS-secured\"] is returned for Calculated-time else [\"Default\"]\r\n",
					"        \"\"\"             \r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        spark_session_mock = Mock()\r\n",
					"        api_metadata_table_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        params_mock = self._create_param_obj(\"calculated-time\")\r\n",
					"        table_client_mock = Mock()\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        data_configs_helper = Mock()\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"        ms_spark_utils_mock = Mock()\r\n",
					"\r\n",
					"        publishing_logic_engine = PublishingLogicEngine(\r\n",
					"            spark_session_mock, \r\n",
					"            data_configs_helper, \r\n",
					"            datalake_client_mock, \r\n",
					"            validation_engine_mock, \r\n",
					"            table_client_mock, \r\n",
					"            api_metadata_table_mock, \r\n",
					"            telemetry_client_mock,\r\n",
					"            ms_spark_utils_mock,\r\n",
					"            params_mock)  \r\n",
					"\r\n",
					"        expected_full_path = \"gold/TR01/TR01TimeRecords/Full/202306221753/D_TR01TimeRecords_2023_06_22_17_53_51\"\r\n",
					"        expected_full_handshake_path = \"gold/TR01/TR01TimeRecords/Full/Handshake/TR01TimeRecordsHandshake\"\r\n",
					"        expected_delta_path = \"gold/TR01/TR01TimeRecords/Delta/202306221753/D_TR01TimeRecords_2023_06_22_17_53_51\"\r\n",
					"        expected_delta_handshake_path = \"gold/TR01/TR01TimeRecords/Delta/Handshake/TR01TimeRecordsHandshake\"\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        full_hrdl_data_path_result = publishing_logic_engine.get_hrdl_data_path(\"Full\", \"ATT\", \"2023_06_22_17_53_51\") \r\n",
					"        full_hrdl_handhsake_path_result = publishing_logic_engine.get_hrdl_handshake_path(\"Full\", \"ATT\")\r\n",
					"        delta_hrdl_data_path_result = publishing_logic_engine.get_hrdl_data_path(\"Delta\", \"ATT\", \"2023_06_22_17_53_51\") \r\n",
					"        delta_hrdl_handhsake_path_result = publishing_logic_engine.get_hrdl_handshake_path(\"Delta\", \"ATT\")\r\n",
					"\r\n",
					"        assert full_hrdl_data_path_result == expected_full_path, f\"Expected {expected_full_path}. Result was {full_hrdl_data_path_result}\"\r\n",
					"        assert delta_hrdl_data_path_result == expected_delta_path, f\"Expected {expected_delta_path}. Result was {delta_hrdl_data_path_result}\"\r\n",
					"        assert full_hrdl_handhsake_path_result == expected_full_handshake_path, f\"Expected {expected_full_handshake_path}. Result was {full_hrdl_handhsake_path_result}\"\r\n",
					"        assert delta_hrdl_handhsake_path_result == expected_delta_handshake_path, f\"Expected {expected_delta_handshake_path}. Result was {delta_hrdl_handhsake_path_result}\"\r\n",
					"\r\n",
					"class MockFileInfo:\r\n",
					"    def __init__(self, name, path):\r\n",
					"        self.name = name\r\n",
					"        self.path = path"
				],
				"execution_count": null
			}
		]
	}
}