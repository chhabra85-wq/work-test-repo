{
	"name": "transformation_logic",
	"properties": {
		"folder": {
			"name": "transformation/logics"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "91270ad8-eaa8-428f-a9df-19ef1b88f26b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpool",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col, expr, count, lit\r\n",
					"from pyspark.sql.types import StringType\r\n",
					"from datetime import datetime\r\n",
					"from typing import Dict, Any\r\n",
					"import os\r\n",
					"import json\r\n",
					"\r\n",
					"class TransformationLogicEngine:\r\n",
					"    \"\"\"\r\n",
					"        Class responsible for the transformation of WFS API data.\r\n",
					"        Execute function produces a transformed parquet file for publish to consume.\r\n",
					"        Execute function gets ingestion data, validates & transforms data to fit publish needs.\r\n",
					"        Specific transformations are done for WFS API type 'calculated-time'.\r\n",
					"    \"\"\"\r\n",
					"    \r\n",
					"    DATA_CHANGE_COL = \"dataChange\"\r\n",
					"    GOLD_INDEX = \"GoldIndex\"\r\n",
					"    MERGE = \"MERGE\"\r\n",
					"    DELETE = \"DELETE\"\r\n",
					"    API_DATA_LOCATION = \"ApiDataLocation\"\r\n",
					"    \r\n",
					"    def __init__(\r\n",
					"        self, \r\n",
					"        spark_session: SparkSession, \r\n",
					"        data_configs_helper: DataConfigsHelper, \r\n",
					"        datalake_client: DataLakeClient, \r\n",
					"        transformation_helper: TransformationHelper, \r\n",
					"        table_client: AzureTableClient,  \r\n",
					"        telemetry_client: TelemetryClient,\r\n",
					"        ms_spark_utils,\r\n",
					"        params: Dict[str, Any]):\r\n",
					"        \"\"\"Constructor setting the class dependencies\"\"\"        \r\n",
					"        self.spark_session = spark_session; \r\n",
					"        self.data_configs_helper = data_configs_helper\r\n",
					"        self.datalake_client = datalake_client\r\n",
					"        self.transformation_helper = transformation_helper    \r\n",
					"        self.table_client = table_client\r\n",
					"        self.telemetry_client = telemetry_client\r\n",
					"        self.ms_spark_utils = ms_spark_utils\r\n",
					"        self.data_type = params.get(\"data_type\")\r\n",
					"        self.run_id = params.get(\"run_id\")\r\n",
					"        self.run_execution_time = \"{:%Y_%m_%d_%H_%M_%S}\".format(datetime.strptime(params.get(\"run_execution_time\"),\"%Y-%m-%dT%H:%M:%S.%fZ\"))\r\n",
					"    \r\n",
					"    def execute(self):\r\n",
					"        \"\"\"\r\n",
					"        Executes the transformation's logic; gets raw data, validates, transforms & pushes to Delta parquet & Full Delta table.\r\n",
					"        \"\"\"\r\n",
					"        try:\r\n",
					"            self.telemetry_client.track_event(f\"Starting {self.data_type} transformation logic for run id {self.run_id} on {self.run_execution_time}.\")\r\n",
					"            ingestion_api_data_location_entity = ApiDataLocationEntity.from_dict(self.table_client.get_table_entity_row(TransformationLogicEngine.API_DATA_LOCATION, self.data_type, \"Bronze_Full_Default\"))\r\n",
					"            \r\n",
					"            if (ingestion_api_data_location_entity.is_processed == False):\r\n",
					"                self.telemetry_client.track_event(f\"Ingestion data for {self.data_type} '{ingestion_api_data_location_entity.dataset_location}' needs to be processed on {self.run_execution_time}.\")\r\n",
					"\r\n",
					"                root_temp_location = f\"temp/silver/{self.data_type}\"\r\n",
					"\r\n",
					"                ingested_df = self.datalake_client.read_parquet(ingestion_api_data_location_entity.dataset_location)\r\n",
					"                self.telemetry_client.track_event(f\"Pulled ingestion data for {self.data_type} from location: {ingestion_api_data_location_entity.dataset_location}.\")\r\n",
					"                self.telemetry_client.track_event(f\"Ingestion data count for {self.data_type} is {ingested_df.count()}.\")\r\n",
					"\r\n",
					"                # Temporary filtering until WFS push change to remove hased ids\r\n",
					"                self.transformation_helper.log_invalid_hashed_ids_rows(ingested_df)\r\n",
					"                ingested_df = ingested_df.filter(~col(\"externalMatchId\").contains(\"-\"))                \r\n",
					"                self.telemetry_client.track_event(f\"Ingestion data count after filtering hashed externalMatchId for {self.data_type} is {ingested_df.count()}.\")\r\n",
					"                \r\n",
					"                validated_ingested_df = self.transformation_helper.perform_ingestion_validations(ingested_df)\r\n",
					"                self.telemetry_client.track_event(f\"Ingestion data count after validations for {self.data_type} is {validated_ingested_df.count()}.\")\r\n",
					"\r\n",
					"                ingested_deletes_df, ingested_merges_df = self.transformation_helper.perform_ingestion_transformations(validated_ingested_df)\r\n",
					"                self.telemetry_client.track_event(f\"Ingestion deletes data count to process for {self.data_type} is {ingested_deletes_df.count()}.\")\r\n",
					"                self.telemetry_client.track_event(f\"Ingestion merges data count to process for {self.data_type} is {ingested_merges_df.count()}.\")        \r\n",
					"\r\n",
					"                if ingested_deletes_df.count() > 0 or ingested_merges_df.count() > 0:\r\n",
					"                    api_data_location_entity = ApiDataLocationEntity.from_dict(self.table_client.get_table_entity_row(TransformationLogicEngine.API_DATA_LOCATION, self.data_type, \"Silver_Temp_Parquet\"))\r\n",
					"                    existing_location_path = api_data_location_entity.dataset_location\r\n",
					"\r\n",
					"                    self.telemetry_client.track_event(f\"Performing merge & delete actions on {existing_location_path} for {self.data_type}.\")\r\n",
					"                    processed_full_df = self.transformation_helper.process_full_data(\r\n",
					"                        self.run_execution_time, \r\n",
					"                        existing_location_path, \r\n",
					"                        ingested_merges_df, \r\n",
					"                        ingested_deletes_df, \r\n",
					"                        root_temp_location\r\n",
					"                    )\r\n",
					"\r\n",
					"                    # Create an empty final union history DF\r\n",
					"                    final_history_df = self.spark_session.createDataFrame([], processed_full_df.schema) \r\n",
					"\r\n",
					"                    transforms = {}\r\n",
					"\r\n",
					"                    if self.data_type == 'calculated-time':\r\n",
					"                        self.telemetry_client.track_event(f\"Detected 'calculated-time' data type. Performing {self.data_type} specific transformations on dataset to generate ATT & ABS.\")\r\n",
					"                        transforms = self.transformation_helper.perform_calculated_time_transformations(processed_full_df)\r\n",
					"                    else:\r\n",
					"                        self.telemetry_client.track_event(f\"Detected 'DEFAULT' data type. Skipping any specific api type transformations for {self.data_type}.\")\r\n",
					"                        transforms = { processed_full_df: (None, 'DEFAULT') }\r\n",
					"                    \r\n",
					"                    for df, (transform_expr, df_type) in transforms.items(): \r\n",
					"                        \r\n",
					"                        data_entity_type = f\"{self.data_type}_{df_type}\"\r\n",
					"                        self.telemetry_client.track_event(f\"Starting process to write Delta & Full files for {data_entity_type}.\")\r\n",
					"\r\n",
					"                        new_folder = f\"{self.data_type}_{self.run_execution_time}\"\r\n",
					"                        delta_latest_location_path = f\"silver/wfs_delta/{self.run_execution_time}/{new_folder}/{df_type}\"                        \r\n",
					"                        full_latest_location_path = f\"silver/wfs_full/{self.run_execution_time}/{new_folder}/{df_type}\"\r\n",
					"\r\n",
					"                        filtered_time_off_df = self.transformation_helper.filter_time_off(df, transform_expr)                        \r\n",
					"                        \r\n",
					"                        silver_api_location_row_key = self.transformation_helper.get_transformation_row_key(self.data_type, full_latest_location_path, \"Full\")                                \r\n",
					"                        previous_gold_index = self.table_client.retrieve_index_value_from_table(self.data_type, silver_api_location_row_key, TransformationLogicEngine.GOLD_INDEX, TransformationLogicEngine.API_DATA_LOCATION)                        \r\n",
					"                        indexed_df, updated_gold_index = self.transformation_helper.add_index_to_df(filtered_time_off_df, previous_gold_index)\r\n",
					"                        self.telemetry_client.track_event(f\"Gold index for {data_entity_type}: (Previous, {previous_gold_index}), (Updated, {updated_gold_index}).\")\r\n",
					"                        \r\n",
					"                        self.datalake_client.write_generic_format(indexed_df, f\"{root_temp_location}/indexed_df/{self.data_type}_{df_type}/{self.run_execution_time}\", \"parquet\", \"overwrite\")\r\n",
					"                        temp_indexed_df = self.datalake_client.read_parquet(f\"{root_temp_location}/indexed_df/{self.data_type}_{df_type}/{self.run_execution_time}\")\r\n",
					"                        finalized_indexed_df = self.data_configs_helper.update_df_column_names(temp_indexed_df, self.data_configs_helper.get_silver_finalized_schema())\r\n",
					"\r\n",
					"                        finalized_df = finalized_indexed_df.select(\"*\").drop(TransformationLogicEngine.DATA_CHANGE_COL)                    \r\n",
					"                        self.telemetry_client.track_event(f\"Finalized data set count for {data_entity_type} is {finalized_indexed_df.count()}.\")\r\n",
					"        \r\n",
					"                        self.telemetry_client.track_event(f\"Starting writing of Delta & Full output files for {data_entity_type}.\")\r\n",
					"                        delta_df = self.transformation_helper.write_full_or_delta(\r\n",
					"                            self.data_type, \r\n",
					"                            finalized_df, \r\n",
					"                            delta_latest_location_path, \r\n",
					"                            previous_gold_index, \r\n",
					"                            updated_gold_index, \r\n",
					"                            \"Delta\", \r\n",
					"                            ingested_deletes_df\r\n",
					"                        )\r\n",
					"                        self.telemetry_client.track_event(f\"Finalized Delta data set count for {data_entity_type} is {delta_df.count()} at {delta_latest_location_path}.\")\r\n",
					"                        full_df = self.transformation_helper.write_full_or_delta(\r\n",
					"                            self.data_type, \r\n",
					"                            finalized_df, \r\n",
					"                            full_latest_location_path, \r\n",
					"                            previous_gold_index, \r\n",
					"                            updated_gold_index, \r\n",
					"                            \"Full\", \r\n",
					"                            ingested_deletes_df\r\n",
					"                        )\r\n",
					"                        self.telemetry_client.track_event(f\"Finalized Full data set count for {data_entity_type} is {full_df.count()} at {full_latest_location_path}.\")                    \r\n",
					"\r\n",
					"                        self.telemetry_client.track_metric(f\"Transformed_Delta_{data_entity_type}\", delta_df.count()),\r\n",
					"                        self.telemetry_client.track_metric(f\"Transformed_Full_{data_entity_type}\", full_df.count()),\r\n",
					"                        self.telemetry_client.track_event(f\"Completed writing of Delta & Full files for {data_entity_type}.\")\r\n",
					"\r\n",
					"                        # Collect rows for history\r\n",
					"                        finalized_indexed_df = finalized_indexed_df.select(\"*\").drop(\"Unit\", \"Value\")\r\n",
					"\r\n",
					"                        # Performing a union operation here to merge data from separate datasets (ATT, ABS), \r\n",
					"                        # thereby upholding a unified historical record within a single file.\r\n",
					"                        final_history_df = final_history_df.unionByName(finalized_indexed_df, allowMissingColumns=True)\r\n",
					"\r\n",
					"                    api_data_location_entity.dataset_location = f\"silver/{self.data_type}/temp_merges_delete/{self.run_execution_time}\" \r\n",
					"                    self.datalake_client.write_generic_format(final_history_df, api_data_location_entity.dataset_location, \"parquet\", \"overwrite\")\r\n",
					"                    self.table_client.update_table_entity_row(TransformationLogicEngine.API_DATA_LOCATION, entity=api_data_location_entity.to_dict())\r\n",
					"                    self.telemetry_client.track_event(f\"Updated {TransformationLogicEngine.API_DATA_LOCATION} with latest Silver_Temp_Parquet location at {api_data_location_entity.dataset_location} for {self.data_type}.\")                        \r\n",
					"                \r\n",
					"                    self.ms_spark_utils.delete_directory(root_temp_location, recurse = True)\r\n",
					"                    self.telemetry_client.track_event(f\"Performed clean up on the {root_temp_location} location for {self.data_type}.\")\r\n",
					"                else:    \r\n",
					"                    self.telemetry_client.track_event(f\"No merges or deletes in ingestion data {ingestion_api_data_location_entity.dataset_location} for {self.data_type} to process on {self.run_execution_time}.\")\r\n",
					"                    \r\n",
					"                self.transformation_helper.update_ingestion_api_data_location_entity_is_processed(ingestion_api_data_location_entity)  \r\n",
					"                self.telemetry_client.track_event(f\"Updated {TransformationLogicEngine.API_DATA_LOCATION} with Bronze_Full_Default IsProcessed field true for {self.data_type}.\")\r\n",
					"\r\n",
					"            else:\r\n",
					"                self.telemetry_client.track_event(f\"Ingestion data for {self.data_type} '{ingestion_api_data_location_entity.dataset_location}' has already been processed. No need to re-process data on {self.run_execution_time}.\")\r\n",
					"            \r\n",
					"            self.telemetry_client.track_event(f\"Completed {self.data_type} transformation logic for run id {self.run_id} on {self.run_execution_time}.\")\r\n",
					"        except Exception as e:\r\n",
					"            self.telemetry_client.track_exception(e)\r\n",
					"            raise e"
				],
				"execution_count": 1
			}
		]
	}
}