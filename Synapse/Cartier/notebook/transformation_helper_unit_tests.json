{
	"name": "transformation_helper_unit_tests",
	"properties": {
		"folder": {
			"name": "tests/test_notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3d9de01d-cd9a-4152-89a8-d9d07186ad38"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpool",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### **Test notebook for the transformation_helper_unit_tests notebook** "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run transformation/utilities/transformation_helper"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import uuid\r\n",
					"import pytest\r\n",
					"from pyspark.sql.functions import lit, col, expr, current_timestamp\r\n",
					"from datetime import date, datetime, timezone\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"from unittest.mock import Mock, MagicMock, ANY, patch\r\n",
					"\r\n",
					"data_config_calc_helper = DataConfigsHelper(DataConfigs, \"calculated_time_config\")\r\n",
					"\r\n",
					"class TestTransformationHelperUnit(SparkSessionBaseClass):\r\n",
					"    PAY_CODE_MOCK = {'ADDITIONAL_PAID_TIME': 'START_DTTM', 'ADOPTION_UNPAID': 'HOURS', 'ADOPTIVE_LEAVE_PAID': 'HOURS', 'ANNUAL_VACATION_CARRY_FWD': 'DAYS', 'ANNUAL_VACATION_PAID': 'DAYS', 'ANTICIPATED_LEAVE': 'DAYS', 'BANK_CLEAR': 'DAYS', 'BEL_EDUCATIONAL_LEGAL_DAYS': 'DAYS', 'BEL_EDUCATIONAL_LEGAL_DAYS_ADJ': 'DAYS', 'BEL_EDUCATIONAL_MSFT_DAYS': 'DAYS', 'BEL_EDUCATIONAL_MSFT_DAYS_ADJ': 'DAYS', 'BEL_EUROPEAN_HOLIDAYS_DAYS': 'DAYS', 'BEL_EUROPEAN_HOLIDAYS_DAYS_ADJ': 'DAYS', 'BEL_LEGAL_VACATION_ADJ': 'DAYS', 'BEL_RECUPERATION_DAYS': 'HOURS', 'BEL_SENIORITY_DAYS_ADJ': 'DAYS', 'BEL_TIME_CREDIT': 'HOURS', 'BEL_URGENT_FAMILY_UNPAID': 'HOURS', 'BEL_WORK_TIME_COMP_ADJ': 'DAYS', 'BEL_YOUTH_HOLIDAYS_DAYS': 'DAYS', 'BEL_YOUTH_HOLIDAYS_DAYS_ADJ': 'DAYS', 'BEREAVEMENT': 'HOURS', 'BEREAVEMENT_CLOSE_RLTV': 'HOURS', 'BEREAVEMENT_IMT_RLTV': 'HOURS', 'BIRTHDAY': 'HOURS', 'BONUS_DISC': 'AMOUNT', 'BONUS_NONDISC': 'AMOUNT', 'BRA_BLOOD_DONATION': 'HOURS', 'BRA_BRVMNT_FAMILY': 'HOURS', 'BRA_BRVMNT_LEAVE': 'HOURS', 'BRA_CAR_OVR_DAYS_FROM_PRE_YEAR_LEGACY': 'DAYS', 'BRA_ELECTORIAL': 'HOURS', 'BRA_JURY_DUTY': 'HOURS', 'BRA_LOA_15_DAYS_LESS': 'HOURS', 'BRA_LOA_15_DAYS_MORE': 'HOURS', 'BRA_LV_CARE_GIVER_28_DAYS': 'HOURS', 'BRA_LV_CHILD_MED_APPOINTMENT': 'HOURS', 'BRA_LV_MED_APPOINTMENT': 'HOURS', 'BRA_MARRIAGE': 'HOURS', 'BRA_MATERNITY_LEAVE': 'HOURS', 'BRA_MILITARY_SERVICE': 'HOURS', 'BRA_PATERNITY_LEAVE': 'HOURS', 'BRA_SICK_AND_MENT_HEALTH_OFF': 'HOURS', 'BRA_SINDICATE': 'HOURS', 'BRA_SINDICATE2': 'HOURS', 'BRA_STUDENT': 'HOURS', 'BRA_UNPAID_LEAVE': 'HOURS', 'BRA_VOLUNTEER_DAYS': 'HOURS', 'BREAK_OVERRIDE_PAID': 'START_DTTM', 'BREAK_OVERRIDE_UNPAID': 'START_DTTM', 'BUSINESS_TRAVEL_PAID': 'DAYS', 'CALL_BACK': 'HOURS', 'CALL_BACK_CORRECTION': 'START_DTTM', 'CALL_BACK_ELA_100': 'HOURS', 'CALL_BACK_ELA_75': 'HOURS', 'CALL_BACK_IO': 'START_DTTM', 'CALL_BACK_IO_100': 'START_DTTM', 'CALL_BACK_IO_75': 'START_DTTM', 'CALL_TIME_IO': 'START_DTTM', 'CAREGIVER_LEAVE_PAID': 'DAYS', 'CAREGIVER_LEAVE_PAID_HOURS': 'HOURS', 'CLOCK': 'START_DTTM', 'CLOCK_ADJ': 'START_DTTM', 'CLOCK_CLEAR': 'HOURS', 'CLOCK_PUNCH_CORRECTION': 'START_DTTM', 'CLOCK_RPT': 'START_DTTM', 'COLLECTIVE_LEAVE': 'DAYS', 'COLLECTIVE_LEAVE_CARRYOVER': 'DAYS', 'COLLECTIVE_LEAVE_ENTITLEMENT': 'DAYS', 'COLLECTIVE_VACATION': 'DAYS', 'COMPANION_LV_CANCER': 'HOURS', 'COMPANION_LV_PARENT': 'HOURS', 'COMPASSIONATE_LEAVE': 'DAYS', 'COMPASSIONATE_LEAVE_5_DAYS': 'DAYS', 'COMPENSATORY_DAYS_OFF': 'DAYS', 'COUNTRY_SABBATICAL_LEAVE_PAID': 'HOURS', 'COUNTRY_SABBATICAL_LEAVE_UNPAID': 'HOURS', 'DAY_OF_REST': 'START_DTTM', 'DISABILITY_75': 'HOURS', 'DISABILITY_75_NO_PFML': 'HOURS', 'DISABILITY_LEAVE_PAID': 'HOURS', 'DT_20': 'HOURS', 'DT_20_ADJ': 'HOURS', 'EDUCATIONAL_LEAVE_PAID_DAYS': 'DAYS', 'EDUCATIONAL_LEAVE_UNPAID_DAYS': 'DAYS', 'EMPLOYEE_VOLUNTEER_TIME_OFF': 'HOURS', 'EU_MEDICAL_LEAVE': 'HOURS', 'FLOATING_HOLIDAY': 'HOURS', 'FLOATING_HOLIDAY_ADJ': 'HOURS', 'FLSA_ADJ': 'AMOUNT', 'FORCE_LEAVE_MAJEURE_PAID': 'DAYS', 'GARDEN_JOB_SEARCH_PAID': 'HOURS', 'HHTO_CARRYOVER_PRIOR_YEAR': 'HOURS', 'HOLIDAY': 'HOURS', 'HOLIDAY_05': 'HOURS', 'HOLIDAY_WORKED': 'HOURS', 'HOLISTIC_HEALTH_TIME_OFF': 'HOURS', 'HOLISTIC_HEALTH_TIME_OFF_ADJ': 'HOURS', 'HOLISTIC_HEALTH_TIME_OFF_CARRYOVER_ADJ': 'HOURS', 'HOLISTIC_HEALTH_TIME_OFF_DAYS': 'DAYS', 'HOLISTIC_HEALTH_TIME_OFF_DAYS_ADJ': 'DAYS', 'ILLNESS_PARTNER_CHILD': 'HOURS', 'JURY': 'HOURS', 'JURY_DAYS': 'DAYS', 'JURY_DUTY_PAID': 'DAYS', 'LEGACY_CARRYOVER_DAYS': 'DAYS', 'LEGACY_CARRYOVER_DAYS_ADJ': 'DAYS', 'LEGAL_VACATION': 'DAYS', 'LOA': 'HOURS', 'LOA_INTERMITTENT': 'HOURS', 'LOA_PROTECTED': 'HOURS', 'LOA_UNPROTECTED': 'HOURS', 'MATERNITY_LEAVE_PAID': 'HOURS', 'MATERNITY_LEAVE_UNPAID': 'HOURS', 'MEAL_BREAK': 'HOURS', 'MEAL_BREAK_ADJ': 'START_DTTM', 'MEAL_BREAK_IO': 'START_DTTM', 'MEAL_BREAK_OVERRIDE': 'HOURS', 'MEAL_BREAK_PAID': 'START_DTTM', 'MEAL_BREAK_UNPAID': 'START_DTTM', 'MEAL_PENALTY': 'HOURS', 'MEAL_PENALTY_ADJ': 'HOURS', 'MEAL_PREMIUM': 'HOURS', 'MEAL_PREMIUM_ADJ': 'HOURS', 'MEAL_PUNCH_CORRECTION': 'START_DTTM', 'MEDICAL_LEAVE_UNPAID': 'DAYS', 'MILITARY_ABSENCE': 'DAYS', 'MILITARY_ABSENCE_SCHEDULE': 'HOURS', 'MILITARY_LEAVE_PAID': 'HOURS', 'MISSED_BREAK': 'START_DTTM', 'MSFT_BRA_11_HR_PREM': 'HOURS', 'MSFT_BRA_UNAPPROVED_PREMIUMS': 'HOURS', 'MSFT_IRE_FMLY_CG_LEAVE_ADJ': 'DAYS', 'MSFT_IRE_REPLACEMENT_FOR_PUBLIC_HOLIDAY_ADJ': 'DAYS', 'NETH_ANNIVERSARY_DAYS_ADJ': 'DAYS', 'NETH_EXTRALEGAL_ACCRUAL_ADJ': 'DAYS', 'NETH_EXTRALEGAL_CARRYOVER_ADJ': 'DAYS', 'NETH_EXTRALEGAL_DAYS': 'DAYS', 'NETH_MEDICAL_APPT': 'HOURS', 'NETH_RELOCATION_DAYS': 'HOURS', 'NETH_WEDDING_ANNIVERSARY_PAID': 'HOURS', 'NIGHT_SHIFT_30': 'HOURS', 'NO_TIME_DAY': 'HOURS', 'ON_CALL': 'HOURS', 'ON_CALL_CORRECTION': 'START_DTTM', 'ON_CALL_DAYS': 'DAYS', 'ON_CALL_IO': 'START_DTTM', 'OTHER_PAID_TIME_OFF': 'HOURS', 'OT_05': 'HOURS', 'OT_10': 'HOURS', 'OT_15': 'HOURS', 'OT_15_ADJ': 'HOURS', 'OT_15_SCHED': 'HOURS', 'OT_1_75': 'HOURS', 'OVERTIME_100': 'HOURS', 'OVERTIME_75': 'HOURS', 'PAID_ABSENCE_COLLECTIVE': 'DAYS', 'PAID_BREAK': 'START_DTTM', 'PAID_EMERGENCY_LEAVE': 'HOURS', 'PAID_HOLIDAYS': 'HOURS', 'PARENTAL_LEAVE': 'HOURS', 'PARENTAL_LEAVE_75': 'HOURS', 'PARENTAL_LEAVE_UNPAID': 'HOURS', 'PARENTAL_LOA_NO_PFML': 'HOURS', 'PARENTS_LEAVE_PAID': 'HOURS', 'PARENTS_LEAVE_UNPAID': 'HOURS', 'PATERNITY_LEAVE_PAID': 'HOURS', 'PERSONAL_LEAVE_PAID_DAYS': 'DAYS', 'PERSONAL_LEAVE_UNPAID_DAYS': 'DAYS', 'PERSONAL_LEAVE_UNPAID_HOURS': 'HOURS', 'PLEASE_SELECT': None, 'PREM_OT': 'HOURS', 'REAL_OT': 'HOURS', 'REG': 'HOURS', 'REG_ADJ': 'HOURS', 'REG_OT': 'HOURS', 'REPLACEMENT_FOR_PUBLIC_HOLIDAYS': 'DAYS', 'REPLACE_ME': None, 'REPORTING_OT': 'HOURS', 'REPORTING_TIME': 'HOURS', 'REST_BREAK_PREMIUM': 'HOURS', 'REST_BREAK_PUNCH_CORRECTION': 'START_DTTM', 'SCHEDULE': 'START_DTTM', 'SCHEDULED_ON_CALL_HOURS': 'HOURS', 'SCHEDULED_ON_CALL_HOURS_IO': 'START_DTTM', 'SCHEDULE_AS': 'START_DTTM', 'SCHEDULE_AS_ON_CALL': 'START_DTTM', 'SCHEDULE_BREAK_PAID': 'START_DTTM', 'SCHEDULE_BREAK_UNPAID': 'START_DTTM', 'SCHEDULE_HOURS': 'HOURS', 'SCHEDULE_MEAL_BREAK': 'HOURS', 'SHIFT_PREM_125': 'HOURS', 'SHIFT_PREM_33': 'HOURS', 'SICK': 'HOURS', 'SICK_ADJ': 'HOURS', 'SICK_CARRYOVER_PRIOR_YEAR': 'HOURS', 'SICK_DAYS_ADJ': 'DAYS', 'SICK_LEAVE_AFTER_15': 'DAYS', 'SICK_LEAVE_FIRST_15': 'DAYS', 'SICK_LEAVE_LONG_TERM_SCHEDULE': 'HOURS', 'SICK_LEAVE_PAID': 'DAYS', 'SICK_LEAVE_UNPAID': 'DAYS', 'SPECIAL_OLYMPICS_DAY': 'DAYS', 'STD_100': 'HOURS', 'STUDENT': 'HOURS', 'SUNDAY_05': 'HOURS', 'TEMP_LOCATION': 'HOURS', 'THIRTEENTH_MONTH_ADVANCE': 'DAYS', 'TIME_OFF': 'HOURS', 'TIME_OFF_UNPAID': 'HOURS', 'TIME_OFF_UNPAID_DAYS': 'DAYS', 'TOIL': 'HOURS', 'TOIL_ACCRUAL': 'HOURS', 'TOIL_ADJ': 'HOURS', 'TOIL_DAYS': 'DAYS', 'TOIL_DAYS_ADJ': 'DAYS', 'TOIL_DAYS_PAYOUT': 'DAYS', 'TOIL_PAYOUT': 'HOURS', 'TOIL_TRANSFER': 'HOURS', 'TRAINING': 'HOURS', 'TRAINING_DAY_INTERNAL_PAID': 'DAYS', 'UNJUSTIFIED_ABSENCE': 'DAYS', 'UNJUSTIFIED_ABSENCE_ADJ': 'DAYS', 'UNPAID': 'HOURS', 'UNPAID_BREAK': 'START_DTTM', 'UNPAID_CAREGIVER': 'HOURS', 'UNPAID_CAREGIVER_PFML': 'HOURS', 'UNPAID_DAYS': 'DAYS', 'UNPAID_EMERGENCY_LEAVE': 'HOURS', 'UNPAID_I9_VERIFICATION_LEAVE': 'HOURS', 'UNPAID_JOB_SEARCH': 'HOURS', 'UNPAID_LEAVE': 'HOURS', 'UNPAID_LTD_MEDICAL_LEAVE': 'HOURS', 'UNPAID_MILITARY_LEAVE': 'HOURS', 'UNPAID_MIL_EXGNCY_PFML': 'HOURS', 'VACATION': 'HOURS', 'VACATION_ACCRUAL_ADJ': 'DAYS', 'VACATION_ADJ': 'HOURS', 'VACATION_CARRYOVER_ADJ': 'HOURS', 'VACATION_CARRYOVER_PRIOR_YEAR': 'HOURS', 'VACATION_DAYS': 'DAYS', 'VACATION_DAYS_ADJ': 'DAYS', 'VACATION_DAYS_CARRYOVER': 'DAYS', 'VACATION_DAYS_CARRYOVER_ADJ': 'DAYS', 'VACATION_DAYS_CARRYOVER_PAYOUT': 'DAYS', 'VACATION_DAYS_PAYOUT': 'DAYS', 'VACATION_ENTITLEMENT': 'DAYS', 'VACATION_ENTITLEMENT_ADJ': 'DAYS', 'VACATION_ENTITLEMENT_PAYOUT': 'DAYS', 'VACATION_PAYOUT': 'HOURS', 'VACATION_PURCHASE_ADJ': 'DAYS', 'VACATION_PURCHASE_ADJ_1': 'DAYS', 'VACATION_PURCHASE_ADJ_2': 'DAYS', 'VACATION_PURCHASE_ADJ_3': 'DAYS', 'VACATION_PURCHASE_ADJ_4': 'DAYS', 'VACATION_SENIORITY_DAYS': 'DAYS', 'VISA_PAID': 'HOURS', 'VISA_UNPAID': 'HOURS', 'WEDDING': 'HOURS', 'WEDDING_PAID': 'HOURS', 'WKEND_PREMIUM': 'HOURS', 'WORKED_EL': 'HOURS', 'WORKED_IO': 'START_DTTM', 'WORK_TIME_COMPENSATION': 'DAYS'}\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        super().setUp()\r\n",
					"        self.data_configs_helper_calc = data_config_calc_helper\r\n",
					"        self.param_mock = self._create_param_obj(\"default\")\r\n",
					"\r\n",
					"        self.transformation_helper = TransformationHelper(\r\n",
					"            Mock(), \r\n",
					"            Mock(), \r\n",
					"            Mock(), \r\n",
					"            Mock(),\r\n",
					"            Mock(),\r\n",
					"            Mock(),\r\n",
					"            self.spark_session,\r\n",
					"            self.param_mock\r\n",
					"        )\r\n",
					"\r\n",
					"    def _create_param_obj(self, data_type):\r\n",
					"        return { \r\n",
					"            \"data_type\": data_type, \r\n",
					"            \"run_id\": str(uuid.UUID(int=0)), \r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\r\n",
					"            \"time_earnings_code\": None\r\n",
					"        }\r\n",
					"\r\n",
					"    def test_add_column_expression(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of add_column_expression\r\n",
					"        Expects a new column to be added with the set value\r\n",
					"        \"\"\"              \r\n",
					"        \r\n",
					"        # ARRANGE\r\n",
					"        df = self.spark_session.createDataFrame([(1, \"a\"), (2, \"b\"), (3, \"c\")], [\"id\", \"name\"])\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        updated_df = self.transformation_helper.add_column_expression(df, \"pk\", lit(0))\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert updated_df.schema.fieldNames() == [\"id\", \"name\", \"pk\"]\r\n",
					"        assert updated_df.select(\"pk\").distinct().collect() == [Row(pk=0)]\r\n",
					"\r\n",
					"    def test_join_on_pks(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of join_on_pks\r\n",
					"        Expects a two Dataframes to be joined on two keys\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        df1 = self.spark_session.createDataFrame([(1, 1), (2, 2), (3, 4)], [\"id\", \"bronze_index\"])\r\n",
					"        df2 = self.spark_session.createDataFrame([(1, 3), (2, 2), (4, 3)], [\"max_id\", \"max_bronze_index\"])\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        joined_df = self.transformation_helper.join_on_pks(df1, [\"id\", \"bronze_index\"], df2, [\"max_id\", \"max_bronze_index\"])\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert joined_df.schema.fieldNames() == [\"id\", \"bronze_index\", \"max_id\", \"max_bronze_index\"]\r\n",
					"        assert joined_df.count() == 1\r\n",
					"        assert joined_df.collect() == [Row(id=2, bronze_index=2, max_id=2, max_bronze_index=2)]\r\n",
					"\r\n",
					"    def test_add_column(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of add_column\r\n",
					"        Expects a Dataframe to have a new column with given value\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Create test data\r\n",
					"        df = self.spark_session.createDataFrame([(1, \"a\"), (2, \"b\"), (3, \"c\")], [\"id\", \"name\"])\r\n",
					"\r\n",
					"        # Add new column\r\n",
					"        new_df = self.transformation_helper.add_column(df, \"new_one\", lit(\"x\"))\r\n",
					"        new_df = self.transformation_helper.add_column(new_df, \"new_two\", True)\r\n",
					"        new_df = self.transformation_helper.add_column(new_df, \"new_three\", 3)\r\n",
					"\r\n",
					"        # Check that the new column was added correctly\r\n",
					"        assert new_df.schema.fieldNames() == [\"id\", \"name\", \"new_one\", \"new_two\", \"new_three\"]\r\n",
					"        assert new_df.select(\"new_one\").distinct().collect() == [Row(new_col=\"x\")]\r\n",
					"        assert new_df.select(\"new_two\").distinct().collect() == [Row(new_col=True)]\r\n",
					"        assert new_df.select(\"new_three\").distinct().collect() == [Row(new_col=3)]\r\n",
					"\r\n",
					"    def test_drop_column(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of drop_column\r\n",
					"        Expects a Dataframe to no longer have dropped column\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        df = self.spark_session.createDataFrame([(1, \"a\", \"drop\"), (2, \"b\", \"drop\"), (3, \"c\", \"drop\")], [\"id\", \"name\", \"col_to_drop\"])\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        new_df = self.transformation_helper.drop_column(df, \"col_to_drop\")\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert new_df.schema.fieldNames() == [\"id\", \"name\"]\r\n",
					"\r\n",
					"    def test_process_full_merge_and_delete_look_up(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of process_full_merge_and_delete_look_up\r\n",
					"        Expects a Dataframe to perform deletes & merges\r\n",
					"        Deletes will be handled by setting isDeleted to True\r\n",
					"        Merges will be handled by updating existing records to isDeleted is True & insert new record with isDeleted is False\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE                     \r\n",
					"        existing_full_df = self.create_process_merge_delete_existing_df()\r\n",
					"\r\n",
					"        delete_data = [\r\n",
					"            (\"id1\", \"2023-05-01\", \"recordId_1\"),\r\n",
					"            (\"id2\", \"2023-05-02\", \"recordId_2\"),\r\n",
					"            (\"id_ben\", \"2023-05-03\", \"recordId_ben\"),\r\n",
					"            (\"id_not_there_1\", \"2023-05-10\", \"recordId_not_there_1\"),\r\n",
					"            (\"id_not_there_2\", \"2023-05-10\", \"recordId_not_there_2\"),                        \r\n",
					"        ]\r\n",
					"        deletes_to_process_df = self.spark_session.createDataFrame(delete_data, [\"externalMatchId\", \"workDate\", \"recordId\"])\r\n",
					"        \r\n",
					"        merge_data = [\r\n",
					"            (\"id0\", \"2023-05-00\", \"5000\", \"6000\", \"new_column\", \"recordId_0\"),\r\n",
					"            (\"id3\", \"2023-05-03\", \"5000\", \"6000\", \"new_column\", \"recordId_3\"),\r\n",
					"            (\"id4\", \"2023-05-03\", \"5000\", \"6000\", \"new_column\", \"recordId_4\"),\r\n",
					"            (\"id5\", \"2023-05-03\", \"5000\", \"6000\", \"new_column\", \"recordId_5\"),\r\n",
					"            (\"id6\", \"2023-05-04\", \"5000\", \"6000\", \"new_column\", \"recordId_6\"), \r\n",
					"            (\"id_amy\", \"2023-05-03\", \"1000\", \"2000\", \"new_column\", \"recordId_amy\"),           \r\n",
					"        ]\r\n",
					"        merges_to_process_df = self.spark_session.createDataFrame(merge_data, [\"externalMatchId\", \"workDate\", \"value\", \"value_2\", \"new_value\", \"recordId\"])\r\n",
					"        merges_to_process_soft_deletes_df = merges_to_process_df.select(\"externalMatchId\", \"workDate\")\r\n",
					"        merges_to_process_inserts_df = merges_to_process_df.select(\"*\").withColumn(\"isDeleted\", lit(False))\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        result_df = self.transformation_helper.process_full_merge_and_delete_look_up(\r\n",
					"            existing_full_df, \r\n",
					"            deletes_to_process_df, \r\n",
					"            merges_to_process_soft_deletes_df, \r\n",
					"            merges_to_process_inserts_df)\r\n",
					"\r\n",
					"        result_id0_list = sorted(result_df.filter((result_df[\"externalMatchId\"] == \"id0\") & (result_df[\"workDate\"] == \"2023-05-00\")).collect(), key=lambda x: x.value)\r\n",
					"        result_id1_list = result_df.filter((result_df[\"externalMatchId\"] == \"id1\") & (result_df[\"workDate\"] == \"2023-05-01\")).collect()\r\n",
					"        result_id2_list = result_df.filter((result_df[\"externalMatchId\"] == \"id2\") & (result_df[\"workDate\"] == \"2023-05-02\")).collect()\r\n",
					"        result_id3_list = sorted(result_df.filter((result_df[\"externalMatchId\"] == \"id3\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect(), key=lambda x: x.value)\r\n",
					"        result_id4_list = sorted(result_df.filter((result_df[\"externalMatchId\"] == \"id4\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect(), key=lambda x: x.value)\r\n",
					"        result_id5_list = result_df.filter((result_df[\"externalMatchId\"] == \"id5\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id6_list = result_df.filter((result_df[\"externalMatchId\"] == \"id6\") & (result_df[\"workDate\"] == \"2023-05-04\")).collect()\r\n",
					"        result_id_ben_list = result_df.filter((result_df[\"externalMatchId\"] == \"id_ben\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_amy_list = result_df.filter((result_df[\"externalMatchId\"] == \"id_amy\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_not_there_1 = result_df.filter((result_df[\"externalMatchId\"] == \"id_not_there_1\") & (result_df[\"workDate\"] == \"2023-05-10\")).collect()\r\n",
					"        result_id_not_there_2 = result_df.filter((result_df[\"externalMatchId\"] == \"id_not_there_2\") & (result_df[\"workDate\"] == \"2023-05-10\")).collect()\r\n",
					"        leave_me_alone_list_false = result_df.filter((result_df[\"externalMatchId\"] == \"id_no_operations_to_be_performed\") & (result_df[\"isDeleted\"] == False)).collect()\r\n",
					"        leave_me_alone_list_true = result_df.filter((result_df[\"externalMatchId\"] == \"id_no_operations_to_be_performed\") & (result_df[\"isDeleted\"] == True)).collect()\r\n",
					"\r\n",
					"        # ASSERT Counts\r\n",
					"        assert deletes_to_process_df.count() == 5\r\n",
					"        assert existing_full_df.count() == 9\r\n",
					"        assert merges_to_process_df.count() == 6\r\n",
					"        assert merges_to_process_soft_deletes_df.count() == 6\r\n",
					"        assert merges_to_process_inserts_df.count() == 6\r\n",
					"        assert result_df.count() == 15\r\n",
					"        assert len(result_id0_list) == 2\r\n",
					"        assert len(result_id1_list) == 1\r\n",
					"        assert len(result_id2_list) == 1\r\n",
					"        assert len(result_id3_list) == 2\r\n",
					"        assert len(result_id4_list) == 2\r\n",
					"        assert len(result_id_ben_list) == 1   \r\n",
					"        assert len(result_id5_list) == 1    \r\n",
					"        assert len(result_id6_list) == 1   \r\n",
					"        assert len(result_id_amy_list) == 2\r\n",
					"        assert len(leave_me_alone_list_false) == 1\r\n",
					"        assert len(leave_me_alone_list_true) == 1\r\n",
					"\r\n",
					"        # ASSERT DELETE with NO PRIOR MERGE IS NOT ADDED\r\n",
					"        assert len(result_id_not_there_1) == 0    \r\n",
					"        assert len(result_id_not_there_2) == 0               \r\n",
					"\r\n",
					"        # ASSERT DELETES\r\n",
					"        assert result_id0_list[0].isDeleted == True\r\n",
					"        assert result_id0_list[0].value == \"1\"\r\n",
					"        assert result_id0_list[1].isDeleted == False\r\n",
					"        assert result_id0_list[1].value == \"5000\"\r\n",
					"        assert result_id1_list[0].isDeleted == True\r\n",
					"        assert result_id1_list[0].value == \"1\"\r\n",
					"        assert result_id1_list[0].recordId == \"recordId_1\"\r\n",
					"        assert result_id2_list[0].isDeleted == True     \r\n",
					"        assert result_id2_list[0].value == \"1\"       \r\n",
					"        assert result_id2_list[0].recordId == \"recordId_2\"\r\n",
					"        assert result_id3_list[0].isDeleted == True\r\n",
					"        assert result_id3_list[0].value == \"1\"\r\n",
					"        assert result_id4_list[0].isDeleted == True\r\n",
					"        assert result_id4_list[0].value == \"1\"\r\n",
					"        assert result_id_ben_list[0].isDeleted == True\r\n",
					"        assert result_id_ben_list[0].value == \"1\"\r\n",
					"        assert result_id_ben_list[0].value_2 == \"200\"\r\n",
					"        assert result_id_ben_list[0].value_3 == \"300\"\r\n",
					"\r\n",
					"        # ASSERT MERGES\r\n",
					"        assert result_id3_list[1].isDeleted == False\r\n",
					"        assert result_id3_list[1].value == \"5000\"   \r\n",
					"        assert result_id4_list[1].isDeleted == False\r\n",
					"        assert result_id4_list[1].value == \"5000\"\r\n",
					"        assert result_id5_list[0].isDeleted == False\r\n",
					"        assert result_id5_list[0].value == \"5000\"   \r\n",
					"        assert result_id6_list[0].isDeleted == False\r\n",
					"        assert result_id6_list[0].value == \"5000\"\r\n",
					"\r\n",
					"        #ASSERT records with no operations\r\n",
					"        assert leave_me_alone_list_false[0].externalMatchId == \"id_no_operations_to_be_performed\"\r\n",
					"        assert leave_me_alone_list_false[0].workDate == \"2001-01-01\"\r\n",
					"        assert leave_me_alone_list_false[0].isDeleted == False\r\n",
					"        assert leave_me_alone_list_false[0].value == \"1\"\r\n",
					"        assert leave_me_alone_list_false[0].recordId == \"record_id_no_operations_performed\"\r\n",
					"        assert leave_me_alone_list_true[0].isDeleted == True\r\n",
					"        assert leave_me_alone_list_true[0].value == \"2\"        \r\n",
					"\r\n",
					"    def test_nothing_to_delete_process_full_look_up(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of process_full_merge_and_delete_look_up with ONLY invalid DELETES\r\n",
					"        Expects a Dataframe to not perform any DELETES and keep original data\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE                     \r\n",
					"        existing_full_df = self.create_process_merge_delete_existing_df()\r\n",
					"\r\n",
					"        delete_data = [\r\n",
					"            (\"id_not_there_1\", \"2023-05-10\", \"recordId_not_there_1\"),\r\n",
					"            (\"id_not_there_2\", \"2023-05-10\", \"recordId_not_there_2\"),                        \r\n",
					"        ]\r\n",
					"        deletes_to_process_df = self.spark_session.createDataFrame(delete_data, [\"externalMatchId\", \"workDate\", \"recordId\"])\r\n",
					"        \r\n",
					"        merge_schema = StructType([\r\n",
					"            StructField(\"externalMatchId\", StringType(), True),\r\n",
					"            StructField(\"workDate\", StringType(), True),\r\n",
					"            StructField(\"value\", StringType(), True),\r\n",
					"            StructField(\"value_2\", StringType(), True),\r\n",
					"            StructField(\"new_value\", StringType(), True),\r\n",
					"            StructField(\"recordId\", StringType(), True)\r\n",
					"        ])\r\n",
					"        merges_to_process_df = self.spark_session.createDataFrame([], merge_schema)\r\n",
					"        merges_to_process_soft_deletes_df = merges_to_process_df.select(\"externalMatchId\", \"workDate\")\r\n",
					"        merges_to_process_inserts_df = merges_to_process_df.select(\"*\").withColumn(\"isDeleted\", lit(False))\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        result_df = self.transformation_helper.process_full_merge_and_delete_look_up(\r\n",
					"            existing_full_df, \r\n",
					"            deletes_to_process_df, \r\n",
					"            merges_to_process_soft_deletes_df, \r\n",
					"            merges_to_process_inserts_df)\r\n",
					"\r\n",
					"        result_id0_list = result_df.filter((result_df[\"externalMatchId\"] == \"id0\") & (result_df[\"workDate\"] == \"2023-05-00\")).collect()\r\n",
					"        result_id1_list = result_df.filter((result_df[\"externalMatchId\"] == \"id1\") & (result_df[\"workDate\"] == \"2023-05-01\")).collect()\r\n",
					"        result_id2_list = result_df.filter((result_df[\"externalMatchId\"] == \"id2\") & (result_df[\"workDate\"] == \"2023-05-02\")).collect()\r\n",
					"        result_id3_list = result_df.filter((result_df[\"externalMatchId\"] == \"id3\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id4_list = result_df.filter((result_df[\"externalMatchId\"] == \"id4\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_ben_list = result_df.filter((result_df[\"externalMatchId\"] == \"id_ben\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_amy_list = result_df.filter((result_df[\"externalMatchId\"] == \"id_amy\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_not_there_1 = result_df.filter((result_df[\"externalMatchId\"] == \"id_not_there_1\") & (result_df[\"workDate\"] == \"2023-05-10\")).collect()\r\n",
					"        result_id_not_there_2 = result_df.filter((result_df[\"externalMatchId\"] == \"id_not_there_2\") & (result_df[\"workDate\"] == \"2023-05-10\")).collect()\r\n",
					"        leave_me_alone_list_false = result_df.filter((result_df[\"externalMatchId\"] == \"id_no_operations_to_be_performed\") & (result_df[\"isDeleted\"] == False)).collect()\r\n",
					"        leave_me_alone_list_true = result_df.filter((result_df[\"externalMatchId\"] == \"id_no_operations_to_be_performed\") & (result_df[\"isDeleted\"] == True)).collect()\r\n",
					"\r\n",
					"        # ASSERT Counts\r\n",
					"        assert result_df.count() == 9\r\n",
					"        assert len(result_id0_list) == 1\r\n",
					"        assert len(result_id1_list) == 1\r\n",
					"        assert len(result_id2_list) == 1\r\n",
					"        assert len(result_id3_list) == 1\r\n",
					"        assert len(result_id4_list) == 1\r\n",
					"        assert len(result_id_ben_list) == 1    \r\n",
					"        assert len(result_id_amy_list) == 1\r\n",
					"        assert len(leave_me_alone_list_false) == 1\r\n",
					"        assert len(leave_me_alone_list_true) == 1\r\n",
					"\r\n",
					"        # ASSERT DELETE with NO PRIOR MERGE IS NOT ADDED / SKIPPED\r\n",
					"        assert len(result_id_not_there_1) == 0    \r\n",
					"        assert len(result_id_not_there_2) == 0               \r\n",
					"\r\n",
					"        # ASSERT Existing Records\r\n",
					"        assert result_id0_list[0].isDeleted == False\r\n",
					"        assert result_id0_list[0].value == \"1\"\r\n",
					"        assert result_id1_list[0].isDeleted == False\r\n",
					"        assert result_id1_list[0].value == \"1\"\r\n",
					"        assert result_id1_list[0].recordId == \"recordId_1\"\r\n",
					"        assert result_id2_list[0].isDeleted == False    \r\n",
					"        assert result_id2_list[0].value == \"1\"       \r\n",
					"        assert result_id2_list[0].recordId == \"recordId_2\"\r\n",
					"        assert result_id3_list[0].isDeleted == False\r\n",
					"        assert result_id3_list[0].value == \"1\"\r\n",
					"        assert result_id4_list[0].isDeleted == False\r\n",
					"        assert result_id4_list[0].value == \"1\"\r\n",
					"        assert result_id_ben_list[0].isDeleted == False\r\n",
					"        assert result_id_ben_list[0].value == \"1\"\r\n",
					"        assert result_id_ben_list[0].value_2 == \"200\"\r\n",
					"        assert result_id_ben_list[0].value_3 == \"300\"\r\n",
					"        assert leave_me_alone_list_false[0].externalMatchId == \"id_no_operations_to_be_performed\"\r\n",
					"        assert leave_me_alone_list_false[0].workDate == \"2001-01-01\"\r\n",
					"        assert leave_me_alone_list_false[0].isDeleted == False\r\n",
					"        assert leave_me_alone_list_false[0].value == \"1\"\r\n",
					"        assert leave_me_alone_list_false[0].recordId == \"record_id_no_operations_performed\"\r\n",
					"        assert leave_me_alone_list_true[0].isDeleted == True\r\n",
					"        assert leave_me_alone_list_true[0].value == \"2\"\r\n",
					"\r\n",
					"    def test_only_deletes_process_full_look_up(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of process_full_merge_and_delete_look_up with ONLY  DELETES\r\n",
					"        Expects a Dataframe to perform DELETES on records with existing matching RecorIds\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE                     \r\n",
					"        existing_full_df = self.create_process_merge_delete_existing_df()\r\n",
					"\r\n",
					"        delete_data = [\r\n",
					"            (\"id_not_there_1\", \"2023-05-10\", \"recordId_not_there_1\"),\r\n",
					"            (\"id_not_there_2\", \"2023-05-10\", \"recordId_not_there_2\"),  \r\n",
					"            (\"id0\", \"2023-05-00\", \"recordId_0\"),                        \r\n",
					"            (\"id1\", \"2023-05-01\", \"recordId_1\"), \r\n",
					"            (\"id2\", \"2023-05-02\", \"recordId_2\"),                       \r\n",
					"        ]\r\n",
					"        deletes_to_process_df = self.spark_session.createDataFrame(delete_data, [\"externalMatchId\", \"workDate\", \"recordId\"])\r\n",
					"        \r\n",
					"        merge_schema = StructType([\r\n",
					"            StructField(\"externalMatchId\", StringType(), True),\r\n",
					"            StructField(\"workDate\", StringType(), True),\r\n",
					"            StructField(\"value\", StringType(), True),\r\n",
					"            StructField(\"value_2\", StringType(), True),\r\n",
					"            StructField(\"new_value\", StringType(), True),\r\n",
					"            StructField(\"recordId\", StringType(), True)\r\n",
					"        ])\r\n",
					"        merges_to_process_df = self.spark_session.createDataFrame([], merge_schema)\r\n",
					"        merges_to_process_soft_deletes_df = merges_to_process_df.select(\"externalMatchId\", \"workDate\")\r\n",
					"        merges_to_process_inserts_df = merges_to_process_df.select(\"*\").withColumn(\"isDeleted\", lit(False))\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        result_df = self.transformation_helper.process_full_merge_and_delete_look_up(\r\n",
					"            existing_full_df, \r\n",
					"            deletes_to_process_df, \r\n",
					"            merges_to_process_soft_deletes_df, \r\n",
					"            merges_to_process_inserts_df)\r\n",
					"\r\n",
					"        result_id0_list = result_df.filter((result_df[\"externalMatchId\"] == \"id0\") & (result_df[\"workDate\"] == \"2023-05-00\")).collect()\r\n",
					"        result_id1_list = result_df.filter((result_df[\"externalMatchId\"] == \"id1\") & (result_df[\"workDate\"] == \"2023-05-01\")).collect()\r\n",
					"        result_id2_list = result_df.filter((result_df[\"externalMatchId\"] == \"id2\") & (result_df[\"workDate\"] == \"2023-05-02\")).collect()\r\n",
					"        result_id3_list = result_df.filter((result_df[\"externalMatchId\"] == \"id3\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id4_list = result_df.filter((result_df[\"externalMatchId\"] == \"id4\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_ben_list = result_df.filter((result_df[\"externalMatchId\"] == \"id_ben\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_amy_list = result_df.filter((result_df[\"externalMatchId\"] == \"id_amy\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id_not_there_1 = result_df.filter((result_df[\"externalMatchId\"] == \"id_not_there_1\") & (result_df[\"workDate\"] == \"2023-05-10\")).collect()\r\n",
					"        result_id_not_there_2 = result_df.filter((result_df[\"externalMatchId\"] == \"id_not_there_2\") & (result_df[\"workDate\"] == \"2023-05-10\")).collect()\r\n",
					"        leave_me_alone_list_false = result_df.filter((result_df[\"externalMatchId\"] == \"id_no_operations_to_be_performed\") & (result_df[\"isDeleted\"] == False)).collect()\r\n",
					"        leave_me_alone_list_true = result_df.filter((result_df[\"externalMatchId\"] == \"id_no_operations_to_be_performed\") & (result_df[\"isDeleted\"] == True)).collect()\r\n",
					"\r\n",
					"        # ASSERT Counts\r\n",
					"        assert result_df.count() == 9\r\n",
					"        assert len(result_id0_list) == 1\r\n",
					"        assert len(result_id1_list) == 1\r\n",
					"        assert len(result_id2_list) == 1\r\n",
					"        assert len(result_id3_list) == 1\r\n",
					"        assert len(result_id4_list) == 1\r\n",
					"        assert len(result_id_ben_list) == 1    \r\n",
					"        assert len(result_id_amy_list) == 1\r\n",
					"        assert len(leave_me_alone_list_false) == 1\r\n",
					"        assert len(leave_me_alone_list_true) == 1\r\n",
					"\r\n",
					"        # ASSERT DELETE with NO PRIOR MERGE IS NOT ADDED / SKIPPED\r\n",
					"        assert len(result_id_not_there_1) == 0    \r\n",
					"        assert len(result_id_not_there_2) == 0               \r\n",
					"\r\n",
					"        # ASSERT Existing Records\r\n",
					"        assert result_id0_list[0].isDeleted == True\r\n",
					"        assert result_id0_list[0].value == \"1\"\r\n",
					"        assert result_id1_list[0].isDeleted == True\r\n",
					"        assert result_id1_list[0].value == \"1\"\r\n",
					"        assert result_id1_list[0].recordId == \"recordId_1\"\r\n",
					"        assert result_id2_list[0].isDeleted == True    \r\n",
					"        assert result_id2_list[0].value == \"1\"       \r\n",
					"        assert result_id2_list[0].recordId == \"recordId_2\"\r\n",
					"        assert result_id3_list[0].isDeleted == False\r\n",
					"        assert result_id3_list[0].value == \"1\"\r\n",
					"        assert result_id4_list[0].isDeleted == False\r\n",
					"        assert result_id4_list[0].value == \"1\"\r\n",
					"        assert result_id_ben_list[0].isDeleted == False\r\n",
					"        assert result_id_ben_list[0].value == \"1\"\r\n",
					"        assert result_id_ben_list[0].value_2 == \"200\"\r\n",
					"        assert result_id_ben_list[0].value_3 == \"300\"\r\n",
					"        assert leave_me_alone_list_false[0].externalMatchId == \"id_no_operations_to_be_performed\"\r\n",
					"        assert leave_me_alone_list_false[0].workDate == \"2001-01-01\"\r\n",
					"        assert leave_me_alone_list_false[0].isDeleted == False\r\n",
					"        assert leave_me_alone_list_false[0].value == \"1\"\r\n",
					"        assert leave_me_alone_list_false[0].recordId == \"record_id_no_operations_performed\"\r\n",
					"        assert leave_me_alone_list_true[0].isDeleted == True\r\n",
					"        assert leave_me_alone_list_true[0].value == \"2\"               \r\n",
					"\r\n",
					"    def test_filter_time_off(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of filter_time_off\r\n",
					"        Expects column status to b added if expression is met\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expect_approved = \"APPROVED\"\r\n",
					"        expected_count = 12\r\n",
					"        expected_calc_count = 14\r\n",
					"        expected_no_exp_schema = ['externalMatchId', 'workDate', 'bronze_index', 'startDate', 'dataChange', 'value']\r\n",
					"        expected_calc_schema = [\"externalMatchId\", \"workDate\", \"timeRecords_payCode\", \"timeRecords_additionalFields_DAYS_OFF\", \"timeRecords_hours\", \"timeRecords_grossPay\", \"timeRecords_additionalFields_SP_EARNINGS_CODE\", \"status\"]\r\n",
					"\r\n",
					"        mock_df = self.get_mock_wfs_df()\r\n",
					"        mock_abs = self._get_mock_wfs_df_calc_time(\"ABS\")\r\n",
					"        mock_att = self._get_mock_wfs_df_calc_time(\"ATT\")\r\n",
					"\r\n",
					"        time_off_filter_abs_exp = self.data_configs_helper_calc.get_time_off_data()[\"transformExprABS\"]\r\n",
					"        time_off_filter_att_exp = self.data_configs_helper_calc.get_time_off_data()[\"transformExprATT\"]\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        result_no_exp_df = self.transformation_helper.filter_time_off(mock_df, None)        \r\n",
					"        result_abs_df = self.transformation_helper.filter_time_off(mock_abs, time_off_filter_abs_exp)        \r\n",
					"        result_att_df = self.transformation_helper.filter_time_off(mock_att, time_off_filter_att_exp)\r\n",
					"\r\n",
					"        no_ex_result_schema = result_no_exp_df.schema.fieldNames()\r\n",
					"        no_ex_result_count = result_no_exp_df.count()\r\n",
					"\r\n",
					"        abs_result_schema = result_abs_df.schema.fieldNames()\r\n",
					"        abs_result_count = result_abs_df.count()\r\n",
					"        abs_id0_list = result_abs_df.filter((result_abs_df[\"externalMatchId\"] == \"1\") & (result_abs_df[\"workDate\"] == \"2001-01-01\")).collect()\r\n",
					"\r\n",
					"        att_result_schema = result_att_df.schema.fieldNames()\r\n",
					"        att_result_count = result_att_df.count()\r\n",
					"        att_id0_list = result_att_df.filter((result_att_df[\"externalMatchId\"] == \"1\") & (result_att_df[\"workDate\"] == \"2001-01-01\")).collect()\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert no_ex_result_count == expected_count, f'Expected {expected_count}, Result was {no_ex_result_count}'\r\n",
					"        assert no_ex_result_schema == expected_no_exp_schema, f'Expected {expected_no_exp_schema}, Result was {no_ex_result_schema}'\r\n",
					"        \r\n",
					"        assert abs_result_count == expected_calc_count , f'Expected {expected_calc_count }, Result was {abs_result_count}'\r\n",
					"        assert abs_result_schema == expected_calc_schema, f'Expected {expected_calc_schema}, Result was {abs_result_schema}'\r\n",
					"        assert abs_id0_list[0].status == expect_approved, f'Expected {expect_approved}, Result was {abs_id0_list[0].status}'\r\n",
					"        \r\n",
					"        assert att_result_count == expected_calc_count , f'Expected {expected_calc_count }, Result was {att_result_count}'\r\n",
					"        assert att_result_schema == expected_calc_schema, f'Expected {expected_calc_schema}, Result was {att_result_schema}'        \r\n",
					"        assert att_id0_list[0].status == expect_approved, f'Expected {expect_approved}, Result was {att_id0_list[0].status}'\r\n",
					"\r\n",
					"    def test_get_max_index_by_id(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of get_max_bronze_index_by_id\r\n",
					"        Takes datafrae & performs a group by followed by a max to return the latest bronze_index\r\n",
					"        Asserts df output has expected counts & values\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        existing_full_df = self.get_mock_wfs_df()\r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        result_df = self.transformation_helper.get_max_index_by_id(existing_full_df)        \r\n",
					"\r\n",
					"        result_id0_list = result_df.filter((result_df[\"externalMatchId\"] == \"id0\") & (result_df[\"workDate\"] == \"2023-05-00\")).collect()\r\n",
					"        result_id1_list = result_df.filter((result_df[\"externalMatchId\"] == \"id1\") & (result_df[\"workDate\"] == \"2023-05-01\")).collect()\r\n",
					"        result_id2_list = result_df.filter((result_df[\"externalMatchId\"] == \"id2\") & (result_df[\"workDate\"] == \"2023-05-02\")).collect()\r\n",
					"        result_id3_list = result_df.filter((result_df[\"externalMatchId\"] == \"id3\") & (result_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_id3_diff_date_list = result_df.filter((result_df[\"externalMatchId\"] == \"id3\") & (result_df[\"workDate\"] == \"2023-05-13\")).collect()\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert result_df.count() == 5, f'Expected 5, Result was {result_df.count()}'\r\n",
					"\r\n",
					"        assert len(result_id0_list) == 1, f'Expected 1, Result was {len(result_id0_list)}'          \r\n",
					"        assert result_id0_list[0].dataChange == \"MERGE\", f'Expected MERGE, Result was {result_id0_list[0].dataChange}'  \r\n",
					"        assert result_id0_list[0].value == \"4\", f'Expected 4, Result was {result_id0_list[0].value}'\r\n",
					"\r\n",
					"        assert len(result_id1_list) == 1, f'Expected 1, Result was {len(result_id1_list)}'\r\n",
					"        assert result_id1_list[0].dataChange == \"DELETE\", f'Expected DELETE, Result was {result_id1_list[0].dataChange}'  \r\n",
					"        assert result_id1_list[0].value == None, f'Expected NONE, Result was {result_id1_list[0].value}'\r\n",
					"\r\n",
					"        assert len(result_id2_list) == 1, f'Expected 1, Result was {len(result_id2_list)}'\r\n",
					"        assert result_id2_list[0].dataChange == \"DELETE\", f'Expected DELETE, Result was {result_id2_list[0].dataChange}'  \r\n",
					"        assert result_id2_list[0].value == None, f'Expected NONE, Result was {result_id2_list[0].value}'\r\n",
					"\r\n",
					"        assert len(result_id3_list) == 1, f'Expected 1, Result was {len(result_id3_list)}'\r\n",
					"        assert result_id3_list[0].dataChange == \"MERGE\", f'Expected MERGE, Result was {result_id3_list[0].dataChange}'\r\n",
					"        assert result_id3_list[0].value == \"1\", f'Expected MERGE, Result was {result_id3_list[0].value}'\r\n",
					"\r\n",
					"        assert len(result_id3_diff_date_list) == 1, f'Expected 1, Result was {len(result_id3_diff_date_list)}' \r\n",
					"        assert result_id3_diff_date_list[0].dataChange == \"MERGE\", f'Expected 1, Result was {result_id3_diff_date_list[0].dataChange}' \r\n",
					"        assert result_id3_diff_date_list[0].value == \"10\", f'Expected 1, Result was {result_id3_diff_date_list[0].value}' \r\n",
					"\r\n",
					"    def test_update_dataframe_for_units_values(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior and output of the update_dataframe_for_units_values().\r\n",
					"        \"\"\" \r\n",
					"        # ARRANGE\r\n",
					"        calc_time_df = self._get_mock_wfs_df_calc_time(\"ATT\")\r\n",
					"\r\n",
					"        table_service_client_mock = Mock()\r\n",
					"        table_service_client_mock.fetch_all_rowkey_values = MagicMock(return_value = TestTransformationHelperUnit.PAY_CODE_MOCK)\r\n",
					"\r\n",
					"        self.transformation_helper = TransformationHelper(\r\n",
					"            Mock(), \r\n",
					"            Mock(), \r\n",
					"            Mock(), \r\n",
					"            Mock(),\r\n",
					"            Mock(),\r\n",
					"            self.data_configs_helper_calc,\r\n",
					"            self.spark_session,\r\n",
					"            self.param_mock\r\n",
					"        )\r\n",
					"\r\n",
					"        result_df = self.transformation_helper.update_dataframe_for_units_values(\r\n",
					"            self.spark_session, \r\n",
					"            calc_time_df, \r\n",
					"            table_service_client_mock)\r\n",
					"\r\n",
					"        no_hours_add_fields_days_off_days_result = result_df.filter((result_df[\"externalMatchId\"] == \"1\")).collect()[0]\r\n",
					"        add_fields_days_off_days_result = result_df.filter((result_df[\"externalMatchId\"] == \"2\")).collect()[0]\r\n",
					"        timeRecords_hours_hours_result = result_df.filter((result_df[\"externalMatchId\"] == \"3\")).collect()[0]\r\n",
					"        no_hours_timeRecords_hours_hours_result = result_df.filter((result_df[\"externalMatchId\"] == \"4\")).collect()[0]\r\n",
					"        timeRecords_grossPay_ammount_result = result_df.filter((result_df[\"externalMatchId\"] == \"5\")).collect()[0]\r\n",
					"        timeRecords_hours_start_dttm_result = result_df.filter((result_df[\"externalMatchId\"] == \"6\")).collect()[0]\r\n",
					"        no_hours_timeRecords_hours_start_dttm_result = result_df.filter((result_df[\"externalMatchId\"] == \"7\")).collect()[0]\r\n",
					"        no_code_in_look_up = result_df.filter((result_df[\"externalMatchId\"] == \"8\")).collect()[0]\r\n",
					"        none_for_all = result_df.filter((result_df[\"externalMatchId\"] == \"9\")).collect()[0]\r\n",
					"        second_no_code_in_look_up = result_df.filter((result_df[\"externalMatchId\"] == \"10\")).collect()[0]\r\n",
					"        hash_result = result_df.filter((result_df[\"externalMatchId\"] == \"hash-id-example\")).collect()\r\n",
					"        null_day_no_hours_add_fields_days_off_days_result = result_df.filter((result_df[\"externalMatchId\"] == \"11\")).collect()[0]\r\n",
					"        maternity_leave_result = result_df.filter((result_df[\"externalMatchId\"] == \"12\")).collect()[0]\r\n",
					"\r\n",
					"        missing_cols_schema = StructType([\r\n",
					"            StructField(\"externalMatchId\", StringType(), False)\r\n",
					"        ])\r\n",
					"\r\n",
					"        missing_data = [\r\n",
					"            (\"one\",),\r\n",
					"            (\"two\",),\r\n",
					"            (\"hash-id-example\",),\r\n",
					"            (\"hash-id-example\",)\r\n",
					"        ]\r\n",
					"\r\n",
					"        missing_col_df = self.spark_session.createDataFrame(missing_data, missing_cols_schema)\r\n",
					"\r\n",
					"        missing_col_df_result = self.transformation_helper.update_dataframe_for_units_values(\r\n",
					"            self.spark_session, \r\n",
					"            missing_col_df, \r\n",
					"            table_service_client_mock)        \r\n",
					"        \r\n",
					"        missing_col_df_result_one = missing_col_df_result.filter((missing_col_df_result[\"externalMatchId\"] == \"one\")).collect()[0]\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert result_df.count() == 14\r\n",
					"        assert result_df.schema.fieldNames() == [\"externalMatchId\", \"workDate\", \"timeRecords_payCode\", \"timeRecords_additionalFields_DAYS_OFF\", \"timeRecords_hours\", \"timeRecords_grossPay\", \"timeRecords_additionalFields_SP_EARNINGS_CODE\", \"Unit\", \"Value\"]\r\n",
					"        assert len(hash_result) == 2\r\n",
					"        assert missing_col_df_result.count() == 4\r\n",
					"\r\n",
					"        # ASSERT - Null values for missing columns\r\n",
					"        missing_col_df_result_one.externalMatchId == \"one\"\r\n",
					"        missing_col_df_result_one.Value == None\r\n",
					"        missing_col_df_result_one.timeRecords_grossPay == None\r\n",
					"        missing_col_df_result_one.timeRecords_additionalFields_DAYS_OFF == None\r\n",
					"        missing_col_df_result_one.timeRecords_hours == None\r\n",
					"        missing_col_df_result_one.timeRecords_payCode == None\r\n",
					"\r\n",
					"        # ASSERT - if \"timeRecords_additionalFields_DAYS_OFF\" in row and rowkey_value == \"DAYS\"\r\n",
					"        assert no_hours_add_fields_days_off_days_result.externalMatchId == \"1\"\r\n",
					"        assert no_hours_add_fields_days_off_days_result.timeRecords_hours == \"0\"\r\n",
					"        assert no_hours_add_fields_days_off_days_result.Value == \"1\"\r\n",
					"        assert no_hours_add_fields_days_off_days_result.Unit == \"DAYS\"\r\n",
					"\r\n",
					"        # ASSERT - NULL DAY no hours & if \"timeRecords_additionalFields_DAYS_OFF\" in row and rowkey_value == \"DAYS\"        \r\n",
					"        assert null_day_no_hours_add_fields_days_off_days_result.externalMatchId == \"11\"\r\n",
					"        assert null_day_no_hours_add_fields_days_off_days_result.timeRecords_hours == \"0\"        \r\n",
					"        assert null_day_no_hours_add_fields_days_off_days_result.Value == \"0\"\r\n",
					"        assert null_day_no_hours_add_fields_days_off_days_result.Unit == \"DAYS\"\r\n",
					"\r\n",
					"        # ASSERT - no hours & if \"timeRecords_additionalFields_DAYS_OFF\" in row and rowkey_value == \"DAYS\"\r\n",
					"        assert add_fields_days_off_days_result.externalMatchId == \"2\"\r\n",
					"        assert add_fields_days_off_days_result.timeRecords_hours == \"3\"        \r\n",
					"        assert add_fields_days_off_days_result.Value == \"2\"\r\n",
					"        assert add_fields_days_off_days_result.Unit == \"DAYS\"\r\n",
					"        \r\n",
					"        # ASSERT - elif \"timeRecords_hours\" in row and rowkey_value == \"HOURS\"\r\n",
					"        assert timeRecords_hours_hours_result.externalMatchId == \"3\"\r\n",
					"        assert timeRecords_hours_hours_result.timeRecords_hours == \"4\"        \r\n",
					"        assert timeRecords_hours_hours_result.Value == \"4\"\r\n",
					"        assert timeRecords_hours_hours_result.Unit == \"HOURS\"   \r\n",
					"\r\n",
					"        # ASSERT - no hours elif \"timeRecords_hours\" in row and rowkey_value == \"HOURS\"\r\n",
					"        assert no_hours_timeRecords_hours_hours_result.externalMatchId == \"4\"\r\n",
					"        assert no_hours_timeRecords_hours_hours_result.timeRecords_hours == \"0\"       \r\n",
					"        assert no_hours_timeRecords_hours_hours_result.Value == \"0\"\r\n",
					"        assert no_hours_timeRecords_hours_hours_result.Unit == \"HOURS\" \r\n",
					"\r\n",
					"        # ASSERT - elif \"timeRecords_grossPay\" in row and rowkey_value == \"AMOUNT\"\r\n",
					"        assert timeRecords_grossPay_ammount_result.externalMatchId== \"5\"\r\n",
					"        assert timeRecords_grossPay_ammount_result.timeRecords_hours == \"0\"       \r\n",
					"        assert timeRecords_grossPay_ammount_result.timeRecords_additionalFields_DAYS_OFF == \"0\"  \r\n",
					"        assert timeRecords_grossPay_ammount_result.Value == \"5\"\r\n",
					"        assert timeRecords_grossPay_ammount_result.Unit == \"AMOUNT\"    \r\n",
					"\r\n",
					"        # ASSERT - elif \"timeRecords_hours\" in row and rowkey_value == \"START_DTTM\"\r\n",
					"        assert timeRecords_hours_start_dttm_result.externalMatchId == \"6\"\r\n",
					"        assert timeRecords_hours_start_dttm_result.timeRecords_hours == \"7\"\r\n",
					"        assert timeRecords_hours_start_dttm_result.Value == \"7\"\r\n",
					"        assert timeRecords_hours_start_dttm_result.Unit == \"START_DTTM\"\r\n",
					"        \r\n",
					"        # ASSERT - no hours & elif \"timeRecords_hours\" in row and rowkey_value == \"START_DTTM\"\r\n",
					"        assert no_hours_timeRecords_hours_start_dttm_result.externalMatchId == \"7\"\r\n",
					"        assert no_hours_timeRecords_hours_start_dttm_result.timeRecords_hours == \"0\"   \r\n",
					"        assert no_hours_timeRecords_hours_start_dttm_result.Value == \"0\"\r\n",
					"        assert no_hours_timeRecords_hours_start_dttm_result.Unit == \"START_DTTM\"\r\n",
					"\r\n",
					"        # ASSERT - no mapping paycodes\r\n",
					"        assert no_code_in_look_up.externalMatchId == \"8\"\r\n",
					"        assert no_code_in_look_up.Value == None\r\n",
					"        assert no_code_in_look_up.Unit == None\r\n",
					"        assert second_no_code_in_look_up.externalMatchId == \"10\"\r\n",
					"        assert second_no_code_in_look_up.Value == None\r\n",
					"        assert second_no_code_in_look_up.Unit == None\r\n",
					"        assert none_for_all.externalMatchId == \"9\"\r\n",
					"        assert none_for_all.Value == None\r\n",
					"        assert none_for_all.Unit == None\r\n",
					"\r\n",
					"        #ASSERT - scientific notations\r\n",
					"        assert maternity_leave_result.externalMatchId == \"12\"\r\n",
					"        assert maternity_leave_result.timeRecords_hours == \"0.00004\"\r\n",
					"\r\n",
					"    def test_initial_run_process_full_data(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of process_full_data during initial run\r\n",
					"        Expects to process only merges with isDeleted set to False\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE initial run\r\n",
					"        expected_count = 5\r\n",
					"        expected_schema = [\"externalMatchId\", \"workDate\", \"dataChange\", \"value_1\", \"value_2\", \"recordId\", \"isDeleted\"]\r\n",
					"\r\n",
					"        initial_data = [\r\n",
					"            (\"id0\", \"2023-05-00\", \"MERGE\", \"6000\", \"new_column\", \"recordId_0\"),\r\n",
					"            (\"id3\", \"2023-05-03\", \"MERGE\", \"6000\", \"new_column\", \"recordId_3\"),\r\n",
					"            (\"id4\", \"2023-05-03\", \"MERGE\", \"6000\", \"new_column\", \"recordId_4\"),\r\n",
					"            (\"id5\", \"2023-05-03\", \"MERGE\", \"6000\", \"new_column\", \"recordId_5\"),\r\n",
					"            (\"id6\", \"2023-05-04\", \"MERGE\", \"6000\", \"new_column\", \"recordId_6\"), \r\n",
					"        ]\r\n",
					"        initial_run_df = self.spark_session.createDataFrame(initial_data, [\"externalMatchId\", \"workDate\", \"dataChange\", \"value_1\", \"value_2\", \"recordId\"])\r\n",
					"\r\n",
					"        # ACT initial run\r\n",
					"        init_run_full_df_result = self.transformation_helper.process_full_data(\r\n",
					"            \"run_execution_time\",\r\n",
					"            None,   \r\n",
					"            initial_run_df,\r\n",
					"            None,\r\n",
					"            \"root_temp_location\")\r\n",
					"\r\n",
					"        result_count = init_run_full_df_result.count()\r\n",
					"        result_schema = init_run_full_df_result.schema.fieldNames()\r\n",
					"        data_change_deletes_count_result = init_run_full_df_result.filter(col(\"dataChange\") == \"DELETE\").count()\r\n",
					"        is_deleted_count_result = init_run_full_df_result.filter(col(\"isDeleted\") == True).count()\r\n",
					"        is_not_deleted_count_result = init_run_full_df_result.filter(col(\"isDeleted\") == False).count()\r\n",
					"\r\n",
					"        id_zero_result = init_run_full_df_result.filter((init_run_full_df_result[\"externalMatchId\"] == \"id0\") & (init_run_full_df_result[\"workDate\"] == \"2023-05-00\")).collect()\r\n",
					"\r\n",
					"        # ASSERT DataFrame properties\r\n",
					"        assert result_count == expected_count, f\"Expected {expected_count}. Result was {result_count}\"\r\n",
					"        assert data_change_deletes_count_result == 0, f\"Expected {0}. Result was {data_change_deletes_count_result}\"\r\n",
					"        assert is_deleted_count_result == 0, f\"Expected 0. Result was {is_deleted_count_result}\" \r\n",
					"        assert is_not_deleted_count_result == 5, f\"Expected 5. Result was {is_not_deleted_count_result}\"\r\n",
					"        assert result_schema == expected_schema, f\"Expected {expected_schema}. Result was {result_schema}\"\r\n",
					"        \r\n",
					"        # ASSERT DataFrame record id 0\r\n",
					"        assert len(id_zero_result) == 1\r\n",
					"        assert id_zero_result[0].externalMatchId == \"id0\"\r\n",
					"        assert id_zero_result[0].workDate == \"2023-05-00\"\r\n",
					"        assert id_zero_result[0].dataChange == \"MERGE\"\r\n",
					"        assert id_zero_result[0].isDeleted == False\r\n",
					"        assert id_zero_result[0].value_1 == \"6000\"\r\n",
					"        assert id_zero_result[0].value_2 == \"new_column\"\r\n",
					"        assert id_zero_result[0].recordId == \"recordId_0\"\r\n",
					"\r\n",
					"    def test_subsequent_run_process_full_data(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of process_full_data during subsequent run\r\n",
					"        Expects to process DELETE operations by setting existing record isDeleted to True; skips records with non-matching recordId\r\n",
					"        Expects to process Merge operations by setting existing record isDeleted to True & inserting new record isDeleted to False\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE subsequent run\r\n",
					"        expected_schema = [\"dataChange\", \"externalMatchId\", \"isDeleted\", \"recordId\", \"value_1\", \"value_2\", \"workDate\"]\r\n",
					"\r\n",
					"        existing_data = [\r\n",
					"            (\"id0\", \"2023-05-00\", \"MERGE\", \"6000\", \"new_column\", \"recordId_0\", False),\r\n",
					"            (\"id3\", \"2023-05-03\", \"MERGE\", \"6000\", \"new_column\", \"recordId_3\", False),\r\n",
					"            (\"id4\", \"2023-05-03\", \"MERGE\", \"6000\", \"new_column\", \"recordId_4\", False),\r\n",
					"            (\"id5\", \"2023-05-03\", \"MERGE\", \"6000\", \"new_column\", \"recordId_5\", False),\r\n",
					"            (\"id6\", \"2023-05-04\", \"MERGE\", \"6000\", \"new_column\", \"recordId_6\", False), \r\n",
					"        ]\r\n",
					"        existing_df = self.spark_session.createDataFrame(existing_data, [\"externalMatchId\", \"workDate\", \"dataChange\", \"value_1\", \"value_2\", \"recordId\", \"isDeleted\"])\r\n",
					"\r\n",
					"        merges_data = [\r\n",
					"            (\"id0\", \"2023-05-00\", \"MERGE\", \"10000\", \"updated_col\", \"recordId_0\"),\r\n",
					"            (\"id3\", \"2023-05-03\", \"MERGE\", \"10000\", \"updated_col\", \"recordId_3\"),\r\n",
					"            (\"id4\", \"2023-05-03\", \"MERGE\", \"10000\", \"updated_col\", \"recordId_4\"),\r\n",
					"            (\"id10\", \"2023-05-03\", \"MERGE\", \"10000\", \"new_column\", \"recordId_10\"),\r\n",
					"            (\"id11\", \"2023-05-04\", \"MERGE\", \"10000\", \"new_column\", \"recordId_11\"),            \r\n",
					"        ] \r\n",
					"        merges_df = self.spark_session.createDataFrame(merges_data, [\"externalMatchId\", \"workDate\", \"dataChange\", \"value_1\", \"value_2\", \"recordId\"])\r\n",
					"\r\n",
					"        deletes_data = [\r\n",
					"            (\"id5\", \"2023-05-03\", \"DELETE\", \"recordId_5\"),\r\n",
					"            (\"id6\", \"2023-05-04\", \"DELETE\", \"recordId_6\"), \r\n",
					"            (\"id12\", \"2023-05-00\", \"DELETE\", \"recordId_Not_Existing_1\"),\r\n",
					"            (\"id13\", \"2023-05-04\", \"DELETE\", \"recordId_Not_Existing_1\"),                         \r\n",
					"        ]                \r\n",
					"        deletes_df = self.spark_session.createDataFrame(deletes_data, [\"externalMatchId\", \"workDate\", \"dataChange\", \"recordId\"])\r\n",
					"       \r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        datalake_client_mock.read_parquet.side_effect = [existing_df, merges_df, deletes_df]      \r\n",
					"        mock_schema_transformer = Mock()\r\n",
					"        table_client_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        data_configs_helper_mock = Mock()\r\n",
					"        validation_engine_mock = Mock()\r\n",
					"        param_mock = self._create_param_obj(\"default\")\r\n",
					"\r\n",
					"        transformation_helper = TransformationHelper(\r\n",
					"            mock_schema_transformer, \r\n",
					"            datalake_client_mock, \r\n",
					"            table_client_mock, \r\n",
					"            telemetry_client_mock,\r\n",
					"            validation_engine_mock, \r\n",
					"            data_configs_helper_mock,\r\n",
					"            self.spark_session,\r\n",
					"            param_mock\r\n",
					"        )  \r\n",
					"\r\n",
					"        # ACT subsequent run\r\n",
					"        subsequent_run_full_df_result = transformation_helper.process_full_data(\r\n",
					"            \"run_execution_time\",\r\n",
					"            \"existing_location_path\",   \r\n",
					"            merges_df,\r\n",
					"            deletes_df,\r\n",
					"            \"root_temp_location\")\r\n",
					"        \r\n",
					"        id_zero_result = sorted(subsequent_run_full_df_result.filter((subsequent_run_full_df_result[\"externalMatchId\"] == \"id0\") & (subsequent_run_full_df_result[\"workDate\"] == \"2023-05-00\")).collect(), key=lambda x: x.value_1)\r\n",
					"        id_five_result = subsequent_run_full_df_result.filter((subsequent_run_full_df_result[\"externalMatchId\"] == \"id5\") & (subsequent_run_full_df_result[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        id_ten_result = subsequent_run_full_df_result.filter((subsequent_run_full_df_result[\"externalMatchId\"] == \"id10\") & (subsequent_run_full_df_result[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"\r\n",
					"        # ASSERT subsequent run DataFrame propertoes\r\n",
					"        assert subsequent_run_full_df_result.count() == 10, f\"{subsequent_run_full_df_result.count()}\"\r\n",
					"        assert subsequent_run_full_df_result.schema.fieldNames() == expected_schema \r\n",
					"        assert subsequent_run_full_df_result.filter(col(\"dataChange\") == \"DELETE\").count() == 0\r\n",
					"        assert subsequent_run_full_df_result.filter(col(\"dataChange\") == \"MERGE\").count() == 10\r\n",
					"        assert subsequent_run_full_df_result.filter(col(\"isDeleted\") == True).count() == 5  \r\n",
					"        assert subsequent_run_full_df_result.filter(col(\"isDeleted\") == False).count() == 5     \r\n",
					"        \r\n",
					"        # ASSERT subsequent run df records\r\n",
					"        assert len(id_zero_result) == 2\r\n",
					"        assert id_zero_result[1].externalMatchId == \"id0\"\r\n",
					"        assert id_zero_result[1].workDate == \"2023-05-00\"\r\n",
					"        assert id_zero_result[1].dataChange == \"MERGE\"\r\n",
					"        assert id_zero_result[1].isDeleted == True\r\n",
					"        assert id_zero_result[1].value_1 == \"6000\"\r\n",
					"        assert id_zero_result[1].value_2 == \"new_column\"\r\n",
					"        assert id_zero_result[1].recordId == \"recordId_0\"\r\n",
					"        assert id_zero_result[0].externalMatchId == \"id0\"\r\n",
					"        assert id_zero_result[0].workDate == \"2023-05-00\"\r\n",
					"        assert id_zero_result[0].dataChange == \"MERGE\"\r\n",
					"        assert id_zero_result[0].isDeleted == False\r\n",
					"        assert id_zero_result[0].value_1 == \"10000\"\r\n",
					"        assert id_zero_result[0].value_2 == \"updated_col\"\r\n",
					"        assert id_zero_result[0].recordId == \"recordId_0\"\r\n",
					"        assert len(id_five_result) == 1\r\n",
					"        assert id_five_result[0].externalMatchId == \"id5\"\r\n",
					"        assert id_five_result[0].workDate == \"2023-05-03\"\r\n",
					"        assert id_five_result[0].dataChange == \"MERGE\"\r\n",
					"        assert id_five_result[0].isDeleted == True\r\n",
					"        assert id_five_result[0].value_1 == \"6000\"\r\n",
					"        assert id_five_result[0].value_2 == \"new_column\"\r\n",
					"        assert id_five_result[0].recordId == \"recordId_5\"\r\n",
					"        assert len(id_ten_result) == 1\r\n",
					"        assert id_ten_result[0].externalMatchId == \"id10\"\r\n",
					"        assert id_ten_result[0].workDate == \"2023-05-03\"\r\n",
					"        assert id_ten_result[0].dataChange == \"MERGE\"\r\n",
					"        assert id_ten_result[0].isDeleted == False\r\n",
					"        assert id_ten_result[0].value_1 == \"10000\"\r\n",
					"        assert id_ten_result[0].value_2 == \"new_column\"\r\n",
					"        assert id_ten_result[0].recordId == \"recordId_10\"\r\n",
					"\r\n",
					"    def test_output_file_full_or_delta_output_for_delta(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of _output_file_full_or_delta() when Delta output type is passed.\r\n",
					"        Expects the output df to ONLY include to deletes processed & the latest merges via the gold_index col. \r\n",
					"        Expects row key to be of type Delta.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_row_key = \"Silver_Delta_Default\"\r\n",
					"        expected_schema = [\"col1\", \"col2\", \"col3\", \"externalMatchId\", \"gold_index\", \"isDeleted\", \"recordId\", \"workDate\"]\r\n",
					"\r\n",
					"        processed_full_schema = [\"externalMatchId\", \"workDate\", \"isDeleted\", \"recordId\", \"gold_index\", \"col1\", \"col2\", \"col3\"]\r\n",
					"        processed_full_data = [\r\n",
					"            (\"id1\", \"2023-05-00\", True, \"recordId_1\", \"1\", 1, 2, 3),\r\n",
					"            (\"id2\", \"2023-05-01\", True, \"recordId_2\", \"2\", 1, 2, 3),\r\n",
					"            (\"id3\", \"2023-05-02\", True, \"recordId_3\", \"3\", 1, 2, 3),\r\n",
					"            (\"id4\", \"2023-05-03\", False, \"recordId_4\", \"4\", 1, 2, 3),\r\n",
					"            (\"id5\", \"2023-05-04\", False, \"recordId_5\", \"5\", 1, 2, 3),\r\n",
					"            (\"id6\", \"2023-05-05\", False, \"recordId_6\", \"6\", 1, 2, 3),\r\n",
					"            (\"id7\", \"2023-05-05\", False, \"recordId_7\", \"7\", 1, 2, 3),\r\n",
					"        ]\r\n",
					"        processed_full_mock = self.spark_session.createDataFrame(processed_full_data, processed_full_schema)        \r\n",
					"        mock_deletes_df = processed_full_mock.filter(col(\"gold_index\").isin(1, 2 )).withColumn(\"isDeleted\", lit(True))\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        output_df_result, row_key_result = self.transformation_helper._output_file_full_or_delta(\r\n",
					"            \"shift\", mock_deletes_df, processed_full_mock, \"df_latest_location_path\", 5, \"Delta\"\r\n",
					"        )\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert row_key_result == expected_row_key, f\"Expected: {expected_row_key}. Result was: {row_key_result}.\"\r\n",
					"        assert output_df_result.schema.fieldNames() == expected_schema, f\"Expected: {expected_schema}. Result was: {output_df_result.schema.fieldNames()}.\"\r\n",
					"        assert output_df_result.count() == 4, f\"Expected: 4. Result was: {output_df_result.count()}.\"\r\n",
					"\r\n",
					"    def test_output_file_full_or_delta_output_for_full(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of _output_file_full_or_delta() when Full output type is passed.\r\n",
					"        Expects the output df to be same as the original df and the row key to be of type Full\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_row_key = \"Silver_Full_Default\"\r\n",
					"        mock_full_df = self.create_process_merge_delete_existing_df()\r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        output_df_result, row_key_result = self.transformation_helper._output_file_full_or_delta(\r\n",
					"            \"shift\", None, mock_full_df, \"df_latest_location_path\", 4, \"Full\"\r\n",
					"        )\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert row_key_result == expected_row_key, f\"Expected: {expected_row_key}. Result was: {row_key_result}.\"\r\n",
					"        assert output_df_result.count() == 9, f\"Expected: 9. Result was: {output_df_result.count()}.\"\r\n",
					"\r\n",
					"    def test_output_file_full_or_delta_exception(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of _output_file_full_or_delta() when an invalid output type is passed.\r\n",
					"        Expects a exception to be raised stating output type is not supported.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_error_msg = \"transformation_helper.write_full_or_delta() expects only Full or Delta values. Receieved: InvalidType.\"\r\n",
					"        \r\n",
					"        data_type = \"\"\r\n",
					"        deletes_df = MagicMock()\r\n",
					"        full_df = MagicMock()\r\n",
					"        location_path = \"\"\r\n",
					"        previous_gold_index = 0\r\n",
					"        output_type = \"InvalidType\"\r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        with pytest.raises(ValueError) as exc_info:\r\n",
					"            self.transformation_helper._output_file_full_or_delta(\r\n",
					"                data_type, deletes_df, full_df, location_path, previous_gold_index, output_type\r\n",
					"            )\r\n",
					"        \r\n",
					"        # ASSERT\r\n",
					"        assert str(exc_info.value) == expected_error_msg, f\"Expected: {expected_error_msg}. Result was {exc_info.value}.\" \r\n",
					"\r\n",
					"    def test_filter_deletes_merges_dfs(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of filter_deletes_merges_dfs.\r\n",
					"        Expects to return a tupple of DataFrames, one of Deletes and one of Merges.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_deletes_count = 4\r\n",
					"        expected_merges_count = 8\r\n",
					"        mock_df = self.get_mock_wfs_df()\r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        result_deletes_df, result_merges_df = self.transformation_helper._filter_deletes_merges_dfs(mock_df)\r\n",
					"        result_deletes_count = result_deletes_df.count()\r\n",
					"        result_merges_count = result_merges_df.count()\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert result_deletes_count == 4, f\"Expected {expected_deletes_count}. Result was {result_deletes_count}\"\r\n",
					"        assert result_merges_count == 8, f\"Expected {expected_merges_count}. Result was {result_merges_count}\"\r\n",
					"\r\n",
					"    def test_perform_ingestion_transformations(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of perform_ingestion_transformations.\r\n",
					"        Expects to return a tupple of DataFrames, one of Deletes and one of Merges.\r\n",
					"        Expectes for both deletes to have been grouped by max index, added gold index column & properly filtered.\r\n",
					"        \"\"\"        \r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_deletes_count = 2\r\n",
					"        expected_merges_count = 3\r\n",
					"        expected_schema = ['externalMatchId', 'workDate', 'bronze_index', 'startDate', 'dataChange', 'value', 'gold_index']\r\n",
					"\r\n",
					"        mock_df = self.get_mock_wfs_df()\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        result_deletes_df, result_merges_df = self.transformation_helper.perform_ingestion_transformations(mock_df)\r\n",
					"\r\n",
					"        result_deletes_count = result_deletes_df.count()\r\n",
					"        result_deletes_schema = result_deletes_df.schema.fieldNames()\r\n",
					"        result_deletes_one = result_deletes_df.filter((result_deletes_df[\"externalMatchId\"] == \"id1\") & (result_deletes_df[\"workDate\"] == \"2023-05-01\")).collect()\r\n",
					"        result_deletes_two = result_deletes_df.filter((result_deletes_df[\"externalMatchId\"] == \"id2\") & (result_deletes_df[\"workDate\"] == \"2023-05-02\")).collect()\r\n",
					"\r\n",
					"        result_merges_count = result_merges_df.count()\r\n",
					"        result_merges_schema = result_merges_df.schema.fieldNames()\r\n",
					"        result_merges_zero = result_merges_df.filter((result_merges_df[\"externalMatchId\"] == \"id0\") & (result_merges_df[\"workDate\"] == \"2023-05-00\")).collect()\r\n",
					"        result_merges_three = result_merges_df.filter((result_merges_df[\"externalMatchId\"] == \"id3\") & (result_merges_df[\"workDate\"] == \"2023-05-03\")).collect()\r\n",
					"        result_merges_three_diff_date = result_merges_df.filter((result_merges_df[\"externalMatchId\"] == \"id3\") & (result_merges_df[\"workDate\"] == \"2023-05-13\")).collect()\r\n",
					"\r\n",
					"        # ASSERT\r\n",
					"        assert expected_schema == result_deletes_schema, f\"Expected {expected_schema}. Result was {result_deletes_schema}\"\r\n",
					"        assert expected_schema == result_merges_schema, f\"Expected {expected_schema}. Result was {result_deletes_schema}\"\r\n",
					"        assert expected_deletes_count == result_deletes_count, f\"Expected {expected_deletes_count}. Result was {result_deletes_count}\"\r\n",
					"        assert expected_merges_count == result_merges_count, f\"Expected {expected_merges_count}. Result was {result_merges_count}\"\r\n",
					"\r\n",
					"        assert result_merges_zero[0].dataChange == \"MERGE\"\r\n",
					"        assert result_merges_zero[0].value == \"4\"\r\n",
					"        assert result_merges_three[0].dataChange == \"MERGE\"\r\n",
					"        assert result_merges_three[0].value == \"1\"\r\n",
					"        assert result_merges_three_diff_date[0].dataChange == \"MERGE\"\r\n",
					"        assert result_merges_three_diff_date[0].value == \"10\"\r\n",
					"\r\n",
					"        assert result_deletes_one[0].dataChange == \"DELETE\"\r\n",
					"        assert result_deletes_one[0].value == None\r\n",
					"        assert result_deletes_two[0].dataChange == \"DELETE\"\r\n",
					"        assert result_deletes_two[0].value == None\r\n",
					"\r\n",
					"    def test_perform_calculated_time_transformations(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of perform_calculated_time_transformations.\r\n",
					"        Expects to return a dictionary where the key is the calc time along with the expression & earnings code.\r\n",
					"        Expectes for only two DataFrames, one of type ATT and the other of type ABS.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_abs = \"ABS\"\r\n",
					"        expected_att = \"ATT\"\r\n",
					"        expected_count = 14\r\n",
					"        expected_schema = [\"externalMatchId\", \"workDate\", \"timeRecords_payCode\", \"timeRecords_additionalFields_DAYS_OFF\", \"timeRecords_hours\", \"timeRecords_grossPay\", \"timeRecords_additionalFields_SP_EARNINGS_CODE\", \"Unit\", \"Value\"]\r\n",
					"        expected_exp_abs = \"CASE WHEN timeRecords_additionalFields_SP_EARNINGS_CODE = 'ABS' THEN 1 ELSE 0 END = 1\"\r\n",
					"        expected_exp_att = \"CASE WHEN timeRecords_additionalFields_SP_EARNINGS_CODE = 'ATT' THEN 1 ELSE 0 END = 1\"\r\n",
					"\r\n",
					"        calc_time_att_df = self._get_mock_wfs_df_calc_time(\"ATT\")\r\n",
					"        calc_time_abs_df = self._get_mock_wfs_df_calc_time(\"ABS\")\r\n",
					"        calc_time_df = calc_time_att_df.unionByName(calc_time_abs_df)\r\n",
					"\r\n",
					"        table_service_client_mock = Mock()\r\n",
					"        table_service_client_mock.fetch_all_rowkey_values = MagicMock(return_value = TestTransformationHelperUnit.PAY_CODE_MOCK)\r\n",
					"\r\n",
					"        self.transformation_helper = TransformationHelper(\r\n",
					"            Mock(), \r\n",
					"            Mock(), \r\n",
					"            table_service_client_mock, \r\n",
					"            Mock(),\r\n",
					"            Mock(),\r\n",
					"            self.data_configs_helper_calc,\r\n",
					"            self.spark_session,\r\n",
					"            self.param_mock\r\n",
					"        )\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        calc_transformations_result = self.transformation_helper.perform_calculated_time_transformations(calc_time_df)\r\n",
					"\r\n",
					"        for result_df, (result_expr, result_type) in calc_transformations_result.items():\r\n",
					"            result_count =  result_df.count()\r\n",
					"            result_schema = result_df.schema.fieldNames()                        \r\n",
					"            result_code = result_df.select(\"timeRecords_additionalFields_SP_EARNINGS_CODE\").distinct().collect()[0].timeRecords_additionalFields_SP_EARNINGS_CODE\r\n",
					"\r\n",
					"            if result_type == expected_att:\r\n",
					"                assert result_type == expected_att, f\"Expected {expected_att}. Result was {result_type}\"\r\n",
					"                assert result_expr == expected_exp_att, f\"Expected {expected_exp_att}. Result was {result_expr}\"\r\n",
					"                assert result_schema == expected_schema, f\"Expected {expected_schema}. Result was {result_schema}\"\r\n",
					"                assert result_count == expected_count, f\"Expected {expected_count}. Result was {result_count}\"\r\n",
					"                assert result_code == expected_att, f\"Expected {expected_att}. Result was {result_code}\"\r\n",
					"            else:\r\n",
					"                assert result_type == expected_abs, f\"Expected {expected_abs}. Result was {result_type}\"\r\n",
					"                assert result_expr == expected_exp_abs, f\"Expected {expected_exp_abs}. Result was {result_expr}\"\r\n",
					"                assert result_df.schema.fieldNames() == expected_schema, f\"Expected {expected_schema}. Result was {result_schema}\"\r\n",
					"                assert result_count == expected_count, f\"Expected {expected_count}. Result was {result_count}\"\r\n",
					"                assert result_code == expected_abs, f\"Expected {expected_abs}. Result was {result_code}\"\r\n",
					"\r\n",
					"    def create_process_merge_delete_existing_df(self):\r\n",
					"        existing_data = [\r\n",
					"            (\"id0\", \"2023-05-00\", \"1\", False, \"2\", \"3\", \"recordId_0\", \"1\"),\r\n",
					"            (\"id1\", \"2023-05-01\", \"1\", False, \"2\", \"3\", \"recordId_1\", \"2\"),\r\n",
					"            (\"id2\", \"2023-05-02\", \"1\", False, \"2\", \"3\", \"recordId_2\", \"3\"),\r\n",
					"            (\"id3\", \"2023-05-03\", \"1\", False, \"2\", \"3\", \"recordId_3\", \"4\"),\r\n",
					"            (\"id4\", \"2023-05-03\", \"1\", False, \"2\", \"3\", \"recordId_4\", \"5\"),\r\n",
					"            (\"id_ben\", \"2023-05-03\", \"1\", False, \"200\", \"300\", \"recordId_ben\", \"6\"),\r\n",
					"            (\"id_amy\", \"2023-05-03\", \"1\", False, \"200\", \"300\", \"recordId_amy\", \"7\"),\r\n",
					"            (\"id_no_operations_to_be_performed\", \"2001-01-01\", \"1\", False, \"1\", \"1\", \"record_id_no_operations_performed\", \"8\"),\r\n",
					"            (\"id_no_operations_to_be_performed\", \"2001-01-01\", \"2\", True, \"1\", \"1\", \"record_id_no_operations_performed\", \"9\"),                        \r\n",
					"        ]\r\n",
					"        return self.spark_session.createDataFrame(existing_data, [\"externalMatchId\", \"workDate\", \"value\", \"isDeleted\", \"value_2\", \"value_3\", \"recordId\", \"gold_index\"]) \r\n",
					"\r\n",
					"    def get_mock_wfs_df(self):\r\n",
					"        \"\"\"\r\n",
					"        Helper method to produce mock df\r\n",
					"        \"\"\"        \r\n",
					"        existing_data = [\r\n",
					"            (\"id0\", \"2023-05-00\", 1, \"2023-04-00\", \"DELETE\", None),\r\n",
					"            (\"id0\", \"2023-05-00\", 2, \"2023-04-00\", \"DELETE\", None),\r\n",
					"            (\"id0\", \"2023-05-00\", 3, \"2023-04-00\", \"MERGE\", \"3\"),\r\n",
					"            (\"id0\", \"2023-05-00\", 4, \"2023-04-00\", \"MERGE\", \"4\"),\r\n",
					"            (\"id1\", \"2023-05-01\", 5, \"2023-04-01\", \"MERGE\", \"1\"),\r\n",
					"            (\"id1\", \"2023-05-01\", 6, \"2023-04-01\", \"DELETE\", None),\r\n",
					"            (\"id2\", \"2023-05-02\", 7, \"2023-04-02\", \"MERGE\", \"1\"),\r\n",
					"            (\"id2\", \"2023-05-02\", 8, \"2023-04-02\", \"MERGE\", \"2\"),\r\n",
					"            (\"id2\", \"2023-05-02\", 9, \"2023-04-02\", \"MERGE\", \"3\"),\r\n",
					"            (\"id2\", \"2023-05-02\", 10, \"2023-04-02\", \"DELETE\", None),\r\n",
					"            (\"id3\", \"2023-05-03\", 11, \"2023-04-03\", \"MERGE\", \"1\"),\r\n",
					"            (\"id3\", \"2023-05-13\", 12, \"2023-04-13\", \"MERGE\", \"10\"),\r\n",
					"        ]\r\n",
					"        \r\n",
					"        return self.spark_session.createDataFrame(existing_data, [\"externalMatchId\", \"workDate\", \"bronze_index\", \"startDate\", \"dataChange\", \"value\"])        \r\n",
					"    \r\n",
					"    def _get_mock_wfs_df_calc_time(self, calc_type):\r\n",
					"        calc_time_schema = StructType([\r\n",
					"            StructField(\"externalMatchId\", StringType(), False),\r\n",
					"            StructField(\"workDate\", StringType(), False),\r\n",
					"            StructField(\"timeRecords_payCode\", StringType(), True),\r\n",
					"            StructField(\"timeRecords_additionalFields_DAYS_OFF\", StringType(), True),\r\n",
					"            StructField(\"timeRecords_hours\", StringType(), True),\r\n",
					"            StructField(\"timeRecords_grossPay\", StringType(), True),\r\n",
					"            StructField(\"timeRecords_additionalFields_SP_EARNINGS_CODE\", StringType(), True),\r\n",
					"        ])\r\n",
					"\r\n",
					"        calc_time_data = [\r\n",
					"            (\"1\", \"2001-01-01\", \"VACATION_DAYS\", \"1\", None, None, calc_type),\r\n",
					"            (\"2\", \"2001-01-01\", \"TIME_OFF_UNPAID_DAYS\", \"2\", \"3\", None, calc_type),\r\n",
					"            (\"3\", \"2001-01-01\", \"WEDDING\", None, \"4\", None, calc_type),\r\n",
					"            (\"4\", \"2001-01-01\", \"WEDDING\", None, None, None, calc_type),\r\n",
					"            (\"5\", \"2001-01-01\", \"BONUS_DISC\", None, None, \"5\", calc_type),\r\n",
					"            (\"6\", \"2001-01-01\", \"BREAK_OVERRIDE_PAID\", None, \"7\", None, calc_type),\r\n",
					"            (\"7\", \"2001-01-01\", \"BREAK_OVERRIDE_PAID\", None, None, None, calc_type),\r\n",
					"            (\"8\", \"2001-01-01\", \"NO_CODE_IN_LOOK_UP\", None, None, None, calc_type),\r\n",
					"            (\"9\", \"2001-01-01\", None, None, None, None, calc_type),\r\n",
					"            (\"hash-id-example\", \"2001-01-01\", None, None, None, None, calc_type),\r\n",
					"            (\"hash-id-example\", \"2001-01-01\", None, None, None, None, calc_type),\r\n",
					"            (\"10\", \"2001-01-01\", \"NO_PAY_CODE_IN_MAPPING\", \"1\", None, None, calc_type),\r\n",
					"            (\"11\", \"2001-01-01\", \"VACATION_DAYS\", None, None, None, calc_type),    \r\n",
					"            (\"12\", \"2001-01-01\", \"MATERNITY_LEAVE\", None, \"4.0E-5\", None, calc_type),    \r\n",
					"        ]\r\n",
					"\r\n",
					"        return self.spark_session.createDataFrame(calc_time_data, calc_time_schema)"
				],
				"execution_count": null
			}
		]
	}
}