{
	"name": "publishing_logic",
	"properties": {
		"folder": {
			"name": "publishing/logics"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "50975caf-7c31-46b6-826d-0f4c7edb1152"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpoolv34",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpoolv34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col, expr, count, lit\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\r\n",
					"from datetime import datetime, date\r\n",
					"\r\n",
					"class PublishingLogicEngine:\r\n",
					"    \"\"\"Class supporting performing Publishing layer operations\"\"\"\r\n",
					"    FULL_FORMAT = 'Full'\r\n",
					"    DELTA_FORMAT = 'Delta'\r\n",
					"    CALCULATED_TIME = 'calculated-time'\r\n",
					"    ABS_SECURED_CODE = 'ABS-secured'\r\n",
					"\r\n",
					"    def __init__(\r\n",
					"        self, \r\n",
					"        spark_session, \r\n",
					"        data_configs_helper, \r\n",
					"        datalake_client, \r\n",
					"        validation_engine, \r\n",
					"        table_client, \r\n",
					"        api_data_location_table, \r\n",
					"        telemetry_client, \r\n",
					"        ms_spark_utils,\r\n",
					"        params):\r\n",
					"        \"\"\"Constructor setting the class dependencies\"\"\"        \r\n",
					"        self.spark_session = spark_session; \r\n",
					"        self.data_configs_helper = data_configs_helper\r\n",
					"        self.datalake_client = datalake_client\r\n",
					"        self.validation_engine = validation_engine\r\n",
					"        self.table_client = table_client\r\n",
					"        self.api_data_location_table = api_data_location_table \r\n",
					"        self.telemetry_client = telemetry_client\r\n",
					"        self.ms_spark_utils = ms_spark_utils\r\n",
					"        self.data_type = params.get(\"data_type\")\r\n",
					"        self.run_id = params.get(\"run_id\")\r\n",
					"        self.run_execution_time = \"{:%Y_%m_%d_%H_%M_%S}\".format(datetime.strptime(params.get(\"run_execution_time\"),\"%Y-%m-%dT%H:%M:%S.%fZ\"))\r\n",
					"\r\n",
					"    def execute(self):\r\n",
					"        \"\"\"\r\n",
					"        Executes the process_publishing_file_format which is responsible for publishing's logic; gets transformed data, validates, finalizes schema & pushes Delta & Full parquets.\r\n",
					"        \"\"\"\r\n",
					"        try:\r\n",
					"            codes = self.get_codes(self.data_type)\r\n",
					"\r\n",
					"            self.telemetry_client.track_event(f\"Starting publishing logic for codes, {codes}, run id {self.run_id} on {self.run_execution_time}.\")\r\n",
					"\r\n",
					"            for date_type_code in codes: \r\n",
					"                self.telemetry_client.track_event(f\"Starting {self.data_type} - {date_type_code} publishing logic for run id {self.run_id} on {self.run_execution_time}.\")\r\n",
					"\r\n",
					"                hrdl_entity_name = self.get_hrdl_entity_naming(self.data_type, date_type_code)   \r\n",
					"\r\n",
					"                full_already_published = self.check_if_already_published(PublishingLogicEngine.FULL_FORMAT, date_type_code)\r\n",
					"                if (full_already_published == False):\r\n",
					"                    full_data_path, full_df_count = self.process_publishing_file_format(PublishingLogicEngine.FULL_FORMAT, date_type_code, hrdl_entity_name)\r\n",
					"                    full_handshake_path = self.process_publishing_handshake(PublishingLogicEngine.FULL_FORMAT, date_type_code)\r\n",
					"                    self.update_hrdl_tables(date_type_code, full_data_path, full_df_count, PublishingLogicEngine.FULL_FORMAT, full_handshake_path, hrdl_entity_name)\r\n",
					"                else:\r\n",
					"                    self.telemetry_client.track_event(f\"Skipped publishing Full files given files already exists for {self.data_type} - {date_type_code} on {self.run_id} on {self.run_execution_time}.\")\r\n",
					"\r\n",
					"                delta_already_published = self.check_if_already_published(PublishingLogicEngine.DELTA_FORMAT, date_type_code)\r\n",
					"                if (delta_already_published  == False):\r\n",
					"                    delta_data_path, delta_df_count = self.process_publishing_file_format(PublishingLogicEngine.DELTA_FORMAT, date_type_code, hrdl_entity_name)\r\n",
					"                    delta_handshake_path = self.process_publishing_handshake(PublishingLogicEngine.DELTA_FORMAT, date_type_code)\r\n",
					"                    self.update_hrdl_tables(date_type_code, delta_data_path, delta_df_count, PublishingLogicEngine.DELTA_FORMAT, delta_handshake_path, hrdl_entity_name)\r\n",
					"                else:\r\n",
					"                    self.telemetry_client.track_event(f\"Skipped publishing Delta files given files already exists for {self.data_type} - {date_type_code} on {self.run_id} on {self.run_execution_time}.\")\r\n",
					"                \r\n",
					"                self.telemetry_client.track_event(f\"Completed {self.data_type} - {date_type_code} publishing logic for run id {self.run_id} on {self.run_execution_time}.\")\r\n",
					"\r\n",
					"            self.update_next_ingestion_cursor()\r\n",
					"            self.telemetry_client.track_event(f\"Completed publishing logic for codes, {codes}, run id {self.run_id} on {self.run_execution_time}.\")\r\n",
					"\r\n",
					"        except Exception as e:\r\n",
					"            self.telemetry_client.track_exception(e)\r\n",
					"            raise e  \r\n",
					"\r\n",
					"    def update_hrdl_tables(self, code, data_path, df_count, file_format, handshake_path, hrdl_entity_name):\r\n",
					"        \"\"\"\r\n",
					"        Updates HRDL publishing tables 'HRDLFilePublishDetails' & 'HRDLDeltaFileQueue' with latest destination paths post latest execution.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        code (string): type of data code: ABS, ABS-secured, ATT or Default\r\n",
					"        data_path (string): final data path\r\n",
					"        df_count (int): df row count to be set on column last_processed_record_count\r\n",
					"        file_format (string): type of data format: Full or Delta \r\n",
					"        handshake_path (string): final handhshake path\r\n",
					"        hrdl_entity_name (string): name of entity\r\n",
					"        \"\"\"  \r\n",
					"\r\n",
					"        published_status = False\r\n",
					"\r\n",
					"        hrdl_data_path = self.update_data_partition_name(data_path)     \r\n",
					"        hrdl_handshake_path = self.update_handshake_partition_name(handshake_path)\r\n",
					"\r\n",
					"        hrdl_file_pub_entity = HrdlPublishJobEntity.from_dict(self.table_client.get_table_entity_row(\"HRDLFilePublishDetails\", hrdl_entity_name, file_format))                \r\n",
					"        hrdl_file_pub_entity.latest_source_data_path = hrdl_data_path     \r\n",
					"        hrdl_file_pub_entity.latest_handshake_path = hrdl_handshake_path.rsplit(\"/\", 1)[0]\r\n",
					"        hrdl_file_pub_entity.is_published = published_status        \r\n",
					"        hrdl_file_pub_entity.last_folder_name = self.get_date_time_no_underscores(self.run_execution_time)\r\n",
					"        hrdl_file_pub_entity.last_processed_record_count = df_count\r\n",
					"\r\n",
					"        self.table_client.update_table_entity_row(\"HRDLFilePublishDetails\", entity=hrdl_file_pub_entity.to_dict())\r\n",
					"\r\n",
					"        if file_format == PublishingLogicEngine.DELTA_FORMAT:\r\n",
					"            hrdl_delta_queue_entity = HRDLDeltaFileQueueEntity(\r\n",
					"                partition_key = hrdl_entity_name, \r\n",
					"                row_key = self.run_id,\r\n",
					"                is_published = published_status,\r\n",
					"                data_path = hrdl_file_pub_entity.latest_source_data_path,\r\n",
					"                handshake_path = hrdl_file_pub_entity.latest_handshake_path,\r\n",
					"                file_type = PublishingLogicEngine.DELTA_FORMAT,\r\n",
					"                destination_data_path = None,\r\n",
					"                destination_handshake_path = None                 \r\n",
					"            )             \r\n",
					"\r\n",
					"            self.table_client.create_entity(\"HRDLDeltaFileQueue\", entity=hrdl_delta_queue_entity.to_dict())\r\n",
					"\r\n",
					"    def process_publishing_handshake(self, file_format, code):\r\n",
					"        \"\"\"\r\n",
					"        Executes publishing's logic; gets transformed data, validates, finalizes schema & pushes to delta & full coalesced parquet files.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        file_format (string): full or delta value \r\n",
					"        \"\"\"  \r\n",
					"        self.telemetry_client.track_event(f\"Starting publishing handshake for {self.data_type} - {code}.\")\r\n",
					"        hrdl_entity_name = self.get_hrdl_entity_naming(self.data_type, code)\r\n",
					"\r\n",
					"        handhsake_date_time = self.get_date_time_no_underscores(self.run_execution_time)\r\n",
					"        filepath = self.get_hrdl_handshake_path(file_format, code)\r\n",
					"        self.telemetry_client.track_event(f\"Handshake value is {handhsake_date_time} for {self.data_type} - {code}.\")\r\n",
					"        self.telemetry_client.track_event(f\"Handshake data path is {filepath} for {self.data_type} - {code}.\")\r\n",
					"\r\n",
					"        df = self.spark_session.createDataFrame([{\"LatestFolderName\": handhsake_date_time}])\r\n",
					"        self.datalake_client.write_parquet(df.coalesce(1), filepath, mode=\"overwrite\")\r\n",
					"        self.telemetry_client.track_event(f\"Completed publishing handshake for {self.data_type} - {code}.\")        \r\n",
					"        \r\n",
					"        return filepath\r\n",
					"            \r\n",
					"    def process_publishing_file_format(self, file_format, code, hrdl_entity_name):\r\n",
					"        \"\"\"\r\n",
					"        Executes publishing's logic; gets transformed data, validates, finalizes schema & pushes to delta & full coalesced parquet files.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        file_format (string): full or delta value \r\n",
					"        \"\"\"     \r\n",
					"                \r\n",
					"        try:\r\n",
					"            if file_format not in [PublishingLogicEngine.FULL_FORMAT, PublishingLogicEngine.DELTA_FORMAT]:\r\n",
					"                raise ValueError(f\"{self.run_id} - Publishing file format {file_format} is not supported!!\")                        \r\n",
					"\r\n",
					"            file_format_cap = file_format.capitalize()\r\n",
					"\r\n",
					"            if code == PublishingLogicEngine.ABS_SECURED_CODE:\r\n",
					"                silver_row_key = f\"Silver_{file_format_cap}_ABS\"\r\n",
					"            else:\r\n",
					"                silver_row_key = f\"Silver_{file_format_cap}_{code}\"\r\n",
					"\r\n",
					"            self.telemetry_client.track_event(f\"Starting publishing data for {self.data_type} - {silver_row_key}.\")\r\n",
					"\r\n",
					"            api_data_location_entity_silver = ApiDataLocationEntity.from_dict(self.table_client.get_table_entity_row(self.api_data_location_table, self.data_type, silver_row_key))   \r\n",
					"            transformation_df = self.datalake_client.read_parquet(api_data_location_entity_silver.dataset_location)\r\n",
					"            self.telemetry_client.track_event(f\"Pulled transformed data from {api_data_location_entity_silver.dataset_location} for {self.data_type} - {silver_row_key}.\")\r\n",
					"            \r\n",
					"            self.validation_engine.validate_df_schema(transformation_df, self.data_configs_helper.get_gold_validation_df_rules_init_schema())\r\n",
					"            if file_format == PublishingLogicEngine.DELTA_FORMAT:\r\n",
					"                validated_df = transformation_df\r\n",
					"            else:\r\n",
					"                validated_df = self.validation_engine.validate_df_columns(transformation_df, self.data_configs_helper.get_gold_validation_column_rules())\r\n",
					"                \r\n",
					"                hrdl_file_pub_entity = HrdlPublishJobEntity.from_dict(self.table_client.get_table_entity_row(\"HRDLFilePublishDetails\", hrdl_entity_name, file_format))                \r\n",
					"                self.validation_engine.existing_full_count_has_not_dropped(\r\n",
					"                    hrdl_file_pub_entity.deviation_percentage,\r\n",
					"                    hrdl_file_pub_entity.last_processed_record_count,\r\n",
					"                    validated_df.count(),\r\n",
					"                    code)\r\n",
					"\r\n",
					"            self.telemetry_client.track_event(f\"Validated transformed data for {self.data_type} - {silver_row_key}.\")            \r\n",
					"            \r\n",
					"            finalized_df = self.data_configs_helper.update_df_column_names(validated_df, self.data_configs_helper.get_gold_finalized_schema())\r\n",
					"            self.telemetry_client.track_event(f\"Completed final schema updates for {self.data_type} - {silver_row_key}.\")\r\n",
					"\r\n",
					"            publish_complete_path = self.get_hrdl_data_path(file_format, code, self.run_execution_time)\r\n",
					"            api_data_location_entity_gold = ApiDataLocationEntity.from_dict(self.table_client.get_table_entity_row(self.api_data_location_table, self.data_type, f\"Gold_{file_format_cap}_{code}\"))\r\n",
					"            api_data_location_entity_gold.dataset_location = publish_complete_path\r\n",
					"            self.telemetry_client.track_event(f\"Published data path is {publish_complete_path} for {self.data_type} - {silver_row_key}.\")\r\n",
					"            \r\n",
					"            if self.data_type == PublishingLogicEngine.CALCULATED_TIME:\r\n",
					"                if code == PublishingLogicEngine.ABS_SECURED_CODE:\r\n",
					"                    abs_df = finalized_df.drop(\"cartierId\").coalesce(1)\r\n",
					"                    self.datalake_client.write_parquet(abs_df, publish_complete_path, mode=\"overwrite\")\r\n",
					"                elif code == 'ABS':\r\n",
					"                    abs_df_drop_payCode = finalized_df.drop(\"cartierId\",\"PayCode\").coalesce(1)\r\n",
					"                    self.datalake_client.write_parquet(abs_df_drop_payCode, publish_complete_path, mode=\"overwrite\") \r\n",
					"                elif code == 'ATT':\r\n",
					"                    att_df = finalized_df.drop(\"cartierId\", \"status\").coalesce(1)\r\n",
					"                    self.datalake_client.write_parquet(att_df, publish_complete_path, mode=\"overwrite\")\r\n",
					"            else:\r\n",
					"                df = finalized_df.drop(\"cartierId\").coalesce(1)\r\n",
					"                self.datalake_client.write_parquet(df, publish_complete_path, mode=\"overwrite\")     \r\n",
					"            \r\n",
					"            self.table_client.update_table_entity_row(self.api_data_location_table, entity=api_data_location_entity_gold.to_dict())\r\n",
					"            self.telemetry_client.track_event(f\"Completed publishing data for {self.data_type} - {silver_row_key}.\")\r\n",
					"            self.telemetry_client.track_metric(f\"Published_{self.data_type}_{silver_row_key}\", finalized_df.count())\r\n",
					"        \r\n",
					"            return (publish_complete_path, finalized_df.count())\r\n",
					"        except Exception as e:\r\n",
					"            raise e\r\n",
					"                      \r\n",
					"    def get_hrdl_entity_naming(self, data_type, code):\r\n",
					"        \"\"\"\r\n",
					"        Maps api delta types into HRDL entities.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        data_type (string): 'bank' or 'calculated-time' or 'pay' or 'shift' or 'time' or 'time-off' or 'time-off-request'\r\n",
					"        \"\"\"  \r\n",
					"        match data_type:\r\n",
					"            case \"bank\":\r\n",
					"                raise Exception(\"'bank' data_type is not mapped into HRDL entity.\")\r\n",
					"\r\n",
					"            case PublishingLogicEngine.CALCULATED_TIME:\r\n",
					"                if(code == \"ATT\"):\r\n",
					"                    return \"TR01TimeRecords\"\r\n",
					"                elif(code == \"ABS\"):\r\n",
					"                    return \"TR01TimeOffRecords\"\r\n",
					"                elif(code == PublishingLogicEngine.ABS_SECURED_CODE):\r\n",
					"                    return \"TR01TimeOffRecordsS\"    \r\n",
					"                else:\r\n",
					"                    raise Exception(f\"'calculated-time' data_type with code '{code}' is not mapped into HRDL entity.\")\r\n",
					"            case \"pay\":\r\n",
					"                raise Exception(\"'pay' data_type is not mapped into HRDL entity.\")\r\n",
					"\r\n",
					"            case \"shift\":\r\n",
					"                return \"TR01ShiftRecords\"\r\n",
					"\r\n",
					"            case \"time\":\r\n",
					"                raise Exception(\"'pay' data_type is not mapped into HRDL entity.\")\r\n",
					"\r\n",
					"            case \"time-off\":\r\n",
					"                raise Exception(\"'time-off' is not being supported anymore.\")\r\n",
					"\r\n",
					"            case \"time-off-request\":\r\n",
					"                raise Exception(\"'time-off-request' is not being supported anymore.\")\r\n",
					"            \r\n",
					"            case _: \r\n",
					"                raise Exception(\"'{}' data_type is not mapped into HRDL entity.\".format(data_type))\r\n",
					"    \r\n",
					"    def get_hrdl_data_path(self, file_format, code, date_time):        \r\n",
					"        \r\n",
					"        hrdl_entity_name = self.get_hrdl_entity_naming(self.data_type, code)\r\n",
					"        date_time_no_underscores = self.get_date_time_no_underscores(date_time)\r\n",
					"\r\n",
					"        destination_path = \"\"\r\n",
					"        if file_format == PublishingLogicEngine.FULL_FORMAT:\r\n",
					"            destination_path = f\"gold/TR01/{hrdl_entity_name}/Full/{date_time_no_underscores}/D_{hrdl_entity_name}_{date_time}\"\r\n",
					"        elif file_format == PublishingLogicEngine.DELTA_FORMAT:\r\n",
					"            destination_path = f\"gold/TR01/{hrdl_entity_name}/Delta/{date_time_no_underscores}/D_{hrdl_entity_name}_{date_time}\"\r\n",
					"        else:\r\n",
					"            raise TypeError(f\"{file_format} is not a supported Entity Type.\")\r\n",
					"\r\n",
					"        return destination_path\r\n",
					"    \r\n",
					"    def get_hrdl_handshake_path(self, file_format, code):\r\n",
					"        hrdl_entity_name = self.get_hrdl_entity_naming(self.data_type, code)\r\n",
					"\r\n",
					"        destination_path = \"\"\r\n",
					"        if file_format == PublishingLogicEngine.FULL_FORMAT:\r\n",
					"            destination_path = f\"gold/TR01/{hrdl_entity_name}/Full/Handshake/{hrdl_entity_name}Handshake\"\r\n",
					"        elif file_format == PublishingLogicEngine.DELTA_FORMAT:\r\n",
					"            destination_path = f\"gold/TR01/{hrdl_entity_name}/Delta/Handshake/{hrdl_entity_name}Handshake\"\r\n",
					"        else:\r\n",
					"            raise TypeError(f\"{file_format} is not a supported Entity Type.\")\r\n",
					"\r\n",
					"        return destination_path\r\n",
					"    \r\n",
					"    def get_codes(self, data_type):\r\n",
					"        if data_type == PublishingLogicEngine.CALCULATED_TIME:\r\n",
					"                return [\"ATT\", \"ABS\", PublishingLogicEngine.ABS_SECURED_CODE]\r\n",
					"        return [\"Default\"]\r\n",
					"\r\n",
					"    def get_date_time_no_underscores(self, date_time_str):\r\n",
					"        return \"{:%Y%m%d%H%M}\".format(datetime.strptime(date_time_str,\"%Y_%m_%d_%H_%M_%S\"))    \r\n",
					"\r\n",
					"    def update_data_partition_name(self, folder_location):\r\n",
					"        \"\"\"\r\n",
					"        Updates the entity partitoned file name.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        folder_location (string): current location of existing handshake file\r\n",
					"        \"\"\"            \r\n",
					"        success_file = \"_SUCCESS\"\r\n",
					"        snappy_file_type = \".snappy.parquet\"\r\n",
					"\r\n",
					"        existing_files = self.ms_spark_utils.list_directory_contents(folder_location)\r\n",
					"        \r\n",
					"        if len(existing_files) != 2:\r\n",
					"           raise Exception(f\"update_data_partition_name() failed given we only expect 2 files of type {snappy_file_type} & success. Review folder path {folder_location}.\")        \r\n",
					"\r\n",
					"        new_folder_path = \"/\".join(folder_location.split(\"/\")[:-1])\r\n",
					"\r\n",
					"        for file in existing_files:\r\n",
					"            target_file_path = file.path\r\n",
					"\r\n",
					"            if file.name.startswith(\"part-\") and file.name.endswith(snappy_file_type):                            \r\n",
					"                new_data_file_path = new_folder_path + \"/\" + (folder_location.rsplit('/', 1)[-1] + snappy_file_type)\r\n",
					"                self.ms_spark_utils.move_directory(target_file_path, new_data_file_path)\r\n",
					"            elif file.name == success_file:\r\n",
					"                new_success_file_path = new_folder_path + \"/\" + success_file\r\n",
					"                self.ms_spark_utils.replace(target_file_path, new_success_file_path)\r\n",
					"            else:\r\n",
					"                raise Exception(f\"update_data_partition_name() failed given file names were not following 'part-' nor '{success_file}'formats. Review folder path {folder_location}.\")\r\n",
					"        \r\n",
					"        self.ms_spark_utils.delete_directory(folder_location) \r\n",
					"        hrdl_data_file_path = \"user/trusted-service-user/\" + new_data_file_path\r\n",
					"\r\n",
					"        return hrdl_data_file_path\r\n",
					"\r\n",
					"    def update_handshake_partition_name(self, folder_location):\r\n",
					"        \"\"\"\r\n",
					"        Updates the handshake partitoned file name.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        folder_location (string): current location of existing handshake file\r\n",
					"        \"\"\"    \r\n",
					"        success_file = \"_SUCCESS\"\r\n",
					"        snappy_file_type = \".snappy.parquet\"\r\n",
					"\r\n",
					"        existing_files = self.ms_spark_utils.list_directory_contents(folder_location)\r\n",
					"        \r\n",
					"        if len(existing_files) != 2:\r\n",
					"           raise Exception(f\"update_handshake_partition_name() failed given we only expect 2 files of type {snappy_file_type} & success. Review folder path {folder_location}.\")        \r\n",
					"\r\n",
					"        new_folder_path = \"/\".join(folder_location.split(\"/\")[:-1])\r\n",
					"\r\n",
					"        for file in existing_files:\r\n",
					"            target_file_path = file.path\r\n",
					"\r\n",
					"            if file.name.startswith(\"part-\") and file.name.endswith(snappy_file_type):                            \r\n",
					"                new_handshake_file_path = new_folder_path + \"/\" + (folder_location.rsplit('/', 1)[-1] + snappy_file_type)\r\n",
					"                \r\n",
					"                if len(self.ms_spark_utils.list_directory_contents(new_folder_path)) > 2:\r\n",
					"                    self.ms_spark_utils.delete_directory(new_handshake_file_path) \r\n",
					"\r\n",
					"                self.ms_spark_utils.move_directory(target_file_path, new_handshake_file_path)\r\n",
					"            elif file.name == success_file:\r\n",
					"                new_success_file_path = new_folder_path + \"/\" + success_file                \r\n",
					"                \r\n",
					"                if len(self.ms_spark_utils.list_directory_contents(new_folder_path)) > 2:\r\n",
					"                    self.ms_spark_utils.delete_directory(new_success_file_path) \r\n",
					"                    \r\n",
					"                self.ms_spark_utils.move_directory(target_file_path, new_success_file_path)\r\n",
					"            else:\r\n",
					"                raise Exception(f\"update_handshake_partition_name() failed given file names were not following 'part-' nor '{success_file}'formats. Review folder path {folder_location}.\")\r\n",
					"        \r\n",
					"        self.ms_spark_utils.delete_directory(folder_location) \r\n",
					"        hrdl_handshake_file_path = \"user/trusted-service-user/\" + new_handshake_file_path\r\n",
					"\r\n",
					"        return hrdl_handshake_file_path\r\n",
					"\r\n",
					"    def update_next_ingestion_cursor(self):\r\n",
					"        \"\"\"\r\n",
					"        Updates ApiMetaData table NextIngestionCursor with NextCursor value.\r\n",
					"        This action establishes the initial cursor position for the newer ingestion run. \r\n",
					"        Starting from the point of the last successful execution.\r\n",
					"        \"\"\"         \r\n",
					"        api_meta_data = \"ApiMetaData\"\r\n",
					"\r\n",
					"        partition_key = self.data_type\r\n",
					"        row_key = self.data_type\r\n",
					"        nextIngestionCursor = self.table_client.retrieve_cursor(partition_key, row_key, \"NextCursor\")\r\n",
					"        apiMetaDataProperties = {\r\n",
					"            \"NextIngestionCursor\": nextIngestionCursor,\r\n",
					"        }\r\n",
					"\r\n",
					"        self.table_client.update_api_tables(api_meta_data, partition_key, row_key, apiMetaDataProperties)\r\n",
					"        self.telemetry_client.track_event(f\"Updated NextIngestionCursor column value with NextCursor column value in {api_meta_data} table.\")\r\n",
					"    \r\n",
					"    def check_if_already_published(self, file_format: str, code: str):\r\n",
					"        \"\"\"\r\n",
					"        Verifies if published data file for given code & run time have already been published.\r\n",
					"        \r\n",
					"        Arguements:\r\n",
					"            file_format (string): File format, Delta or Full\r\n",
					"            code (string): Data type code, Default or Subtype (ABS, ATS, etc..)\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            Boolean, if publishes true, else false\r\n",
					"        \"\"\"  \r\n",
					"\r\n",
					"        published_file_path = self.get_hrdl_data_path(file_format, code, self.run_execution_time) + \".snappy.parquet\"\r\n",
					"        \r\n",
					"        does_file_exist = self.ms_spark_utils.does_path_exist(published_file_path)\r\n",
					"        \r\n",
					"        if (does_file_exist == False):\r\n",
					"            return False\r\n",
					"        else:\r\n",
					"            execution_time_path_folder = \"/\".join(published_file_path.split(\"/\")[:-1])\r\n",
					"            \r\n",
					"            self.telemetry_client.track_event(f\"Detected already existing published files in folder {execution_time_path_folder} \"\r\n",
					"            f\"for {self.data_type} - {code} on {self.run_execution_time}. \"\r\n",
					"            f\"This can stem from a retry or unintended notebook behavior. Please investigate.\")\r\n",
					"\r\n",
					"            existing_files_list = self.ms_spark_utils.list_directory_contents(execution_time_path_folder)\r\n",
					"            for file in existing_files_list:\r\n",
					"                self.telemetry_client.track_event(f\"Publish file {file.path} already exists for {self.data_type} - {code} on {self.run_execution_time}.\")\r\n",
					"            \r\n",
					"            return True\r\n",
					"        "
				],
				"execution_count": 1
			}
		]
	}
}