{
	"name": "transformation_helper",
	"properties": {
		"folder": {
			"name": "transformation/utilities"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "49c9bc91-f02b-435a-ba82-99e072282e30"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpoolv34",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpoolv34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, DataFrame\r\n",
					"from pyspark.sql.functions import lit, when, expr, coalesce, col\r\n",
					"from typing import List, Tuple\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, NullType\r\n",
					"\r\n",
					"class TransformationHelper:\r\n",
					"    API_DATA_LOCATION = \"ApiDataLocation\"\r\n",
					"    DATA_CHANGE_COL = \"dataChange\"\r\n",
					"    DELETE = \"DELETE\"\r\n",
					"    DELTA = \"Delta\"\r\n",
					"    GOLD_INDEX = \"gold_index\"\r\n",
					"    EXTERNAL_MATCH_ID = \"externalMatchId\"\r\n",
					"    FULL = \"Full\"\r\n",
					"    IS_DELETED_COL = \"isDeleted\"\r\n",
					"    MERGE = \"MERGE\"\r\n",
					"    PAYCODE_TABLE_NAME = \"PayCodeMetadata\"\r\n",
					"    RECORD_ID = \"recordId\"\r\n",
					"    WORK_DATE = \"workDate\"\r\n",
					"\r\n",
					"    def __init__(self, \r\n",
					"        schema_transformer,\r\n",
					"        datalake_client,\r\n",
					"        table_client,\r\n",
					"        telemetry_client,\r\n",
					"        validation_engine,\r\n",
					"        data_configs_helper,\r\n",
					"        spark_session,\r\n",
					"        params):\r\n",
					"\r\n",
					"        \"\"\"Constructor setting the class dependencies\"\"\"        \r\n",
					"        self.schema_transformer = schema_transformer  \r\n",
					"        self.datalake_client = datalake_client\r\n",
					"        self.table_client = table_client\r\n",
					"        self.telemetry_client = telemetry_client\r\n",
					"        self.validation_engine = validation_engine\r\n",
					"        self.data_configs_helper = data_configs_helper\r\n",
					"        self.spark_session = spark_session\r\n",
					"        self.data_type = params.get(\"data_type\")\r\n",
					"        self.run_execution_time = \"{:%Y_%m_%d_%H_%M_%S}\".format(datetime.strptime(params.get(\"run_execution_time\"),\"%Y-%m-%dT%H:%M:%S.%fZ\"))\r\n",
					"\r\n",
					"    def filter_time_off(\r\n",
					"            self, \r\n",
					"            existing_df: DataFrame,  \r\n",
					"            timeoff_filter: expr\r\n",
					"        ) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Adds status column based of time-off filter for calculated time records\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            existing_df (Dataframe): DataFrame to perform operations on\r\n",
					"            timeoff_filter (SQL Expr): sql expression to perform \r\n",
					"        Returns:\r\n",
					"            Updated Dataframe\r\n",
					"        \"\"\"\r\n",
					"        if timeoff_filter is not None:\r\n",
					"            timeoff_filter_exp = expr(timeoff_filter)\r\n",
					"            existing_df = existing_df.withColumn(\"status\", when(timeoff_filter_exp.eqNullSafe(lit(1)), lit(\"APPROVED\")).otherwise(lit(\"\")))\r\n",
					"\r\n",
					"        return existing_df;\r\n",
					"\r\n",
					"    def process_full_merge_and_delete_look_up(\r\n",
					"        self, \r\n",
					"        existing_full_df: DataFrame, \r\n",
					"        deletes_to_process_df: DataFrame, \r\n",
					"        merges_to_process_soft_deletes_df: DataFrame, \r\n",
					"        merges_to_process_inserts_df: DataFrame) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Handles Delete & Merge logic on existing dataframe\r\n",
					"        Deletes will be handled by setting isDeleted to True\r\n",
					"        Merges will be handled by updating existing records to isDeleted is True & insert new record with isDeleted is False\r\n",
					"        \r\n",
					"        Arguments:\r\n",
					"            existing_full_df (Dataframe): existing dataframe which will updated with soft deletes & merges\r\n",
					"            deletes_to_process_df (Dataframe): new dataframe containing records to soft delete\r\n",
					"            merges_to_process_soft_deletes_df (Dataframe): new dataframe containing records to soft delete from merge set\r\n",
					"            merges_to_process_inserts_df (Dataframe): new dataframe containing records to insert\r\n",
					"        Returns:\r\n",
					"            Updated dataframe which contains old records with isDeleted to True and new records with isDeleted to False\r\n",
					"        \"\"\"      \r\n",
					"        delete_condition = [TransformationHelper.RECORD_ID]\r\n",
					"        soft_delete_condition = [TransformationHelper.EXTERNAL_MATCH_ID, TransformationHelper.WORK_DATE]\r\n",
					"\r\n",
					"        # Process DELETE with soft-delete on recordId\r\n",
					"        # Set existing MERGE records to isDeleted is True & skip DELETE record with NO existing MERGE\r\n",
					"        processed_deletes_df = existing_full_df.alias(\"existing\") \\\r\n",
					"            .join(deletes_to_process_df.alias(\"deletes\"), on=delete_condition, how=\"left_outer\") \\\r\n",
					"            .withColumn(TransformationHelper.IS_DELETED_COL, when((col(\"deletes.recordId\").isNotNull()), lit(True)).otherwise(col(\"existing.isDeleted\"))) \\\r\n",
					"            .select(\"existing.*\", TransformationHelper.IS_DELETED_COL) \\\r\n",
					"            .dropDuplicates()\r\n",
					"\r\n",
					"        # Process MERGE-DELETE with soft-delete on externalMatchId & workDate; setting existing records to isDeleted is True\r\n",
					"        processed_merge_soft_deletes_df = processed_deletes_df.alias(\"processed_deletes\") \\\r\n",
					"            .join(merges_to_process_soft_deletes_df.alias(\"merge\"), on=soft_delete_condition, how=\"left_outer\") \\\r\n",
					"            .withColumn(TransformationHelper.IS_DELETED_COL, when((col(\"merge.externalMatchId\").isNotNull()) & (col(\"merge.workDate\").isNotNull()), lit(True)).otherwise(col(\"processed_deletes.isDeleted\"))) \\\r\n",
					"            .select(\"processed_deletes.*\", TransformationHelper.IS_DELETED_COL) \\\r\n",
					"            .dropDuplicates()\r\n",
					"\r\n",
					"        # Process MERGE-INSERTS; setting existing records to isDeleted is False & Inserting new record\r\n",
					"        merges_to_process_inserts_df = merges_to_process_inserts_df.select(sorted(merges_to_process_inserts_df.columns))\r\n",
					"        processed_merge_soft_deletes_df = processed_merge_soft_deletes_df.select(sorted(processed_merge_soft_deletes_df.columns))\r\n",
					"        processed_all_df = processed_merge_soft_deletes_df \\\r\n",
					"            .unionByName(merges_to_process_inserts_df, allowMissingColumns=True) \\\r\n",
					"            .dropDuplicates()\r\n",
					"        return processed_all_df\r\n",
					"\r\n",
					"    def add_column_expression(self, existing_df: DataFrame, col_name: str, expression: expr) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Adds a column with the value from an expression\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            existing_df (Dataframe): dataframe to perform operations on\r\n",
					"            col_name (string): name of new column\r\n",
					"            expression (SQL Expr): sql expression to set column value \r\n",
					"        Returns:\r\n",
					"            Updated Dataframe\r\n",
					"        \"\"\"             \r\n",
					"        return existing_df.withColumn(col_name, expression)\r\n",
					"\r\n",
					"    def get_max_index_by_id(self, existing_df: DataFrame) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Groups a dataframe by an id column, returns the max record in the group & renames columns\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            existing_df (Dataframe): dataframe to perform operations on\r\n",
					"        Returns:\r\n",
					"            Filtered Dataframe\r\n",
					"        \"\"\"\r\n",
					"        group_by_id = \"group_by_id\"\r\n",
					"        max_column = \"bronze_index\"\r\n",
					"\r\n",
					"        existing_df = self.add_column_expression(existing_df, group_by_id, expr(\"CONCAT_WS('_', externalMatchId, workDate)\"))\r\n",
					"\r\n",
					"        new_max_id = f\"max_{group_by_id}\"\r\n",
					"        new_max_col = f\"max_{max_column}\"\r\n",
					"\r\n",
					"        max_df = existing_df \\\r\n",
					"            .groupBy(group_by_id) \\\r\n",
					"            .max(max_column) \\\r\n",
					"            .withColumnRenamed(group_by_id, new_max_id) \\\r\n",
					"            .withColumnRenamed(f\"max({max_column})\", new_max_col)\r\n",
					"\r\n",
					"        # max_lookUpPk  |max_index|\r\n",
					"        joined_df = self.join_on_pks(existing_df, [group_by_id, max_column], max_df, [new_max_id, new_max_col])\r\n",
					"\r\n",
					"        return joined_df \\\r\n",
					"            .drop(new_max_id) \\\r\n",
					"            .drop(new_max_col) \\\r\n",
					"            .drop(group_by_id)\r\n",
					"\r\n",
					"    def join_on_pks(self, primary_df: DataFrame, primary_keys: List[str], secondary_df: DataFrame, secondary_keys: List[str]) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Joins two Dataframes based on two sets of primary keys\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            primary_df (Dataframe): dataframe to join on\r\n",
					"            primary_keys (List): Primary Dataframe key column names\r\n",
					"            secondary_df (Dataframe): dataframe to join against\r\n",
					"            secondary_keys (List): Secondary Dataframe key column names\r\n",
					"        Returns:\r\n",
					"            Joined Dataframe\r\n",
					"        \"\"\"   \r\n",
					"        return primary_df.join(secondary_df, (primary_df[primary_keys[0]] == secondary_df[secondary_keys[0]]) & (primary_df[primary_keys[1]] ==secondary_df[secondary_keys[1]]))\r\n",
					"\r\n",
					"    def add_column(self, existing_df: DataFrame, column_name: str, column_value: object) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Adds a column with set value to an existing Dataframe\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            existing_df (Dataframe): dataframe to perform operations on\r\n",
					"            column_name (string): name of new column\r\n",
					"            column_value (string): column value\r\n",
					"        Returns:\r\n",
					"            Updated Dataframe\r\n",
					"        \"\"\"                     \r\n",
					"        return existing_df.withColumn(column_name, lit(column_value))\r\n",
					"\r\n",
					"    def drop_column(self, existing_df: DataFrame, column_name: str) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Drops a column from an existing Dataframe\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            existing_df (Dataframe): dataframe to perform operations on\r\n",
					"            column_name (string): column to drop\r\n",
					"        Returns:\r\n",
					"            Updated Dataframe\r\n",
					"        \"\"\"              \r\n",
					"        return existing_df.drop(column_name)\r\n",
					"\r\n",
					"    def get_transformation_row_key(self, data_type: str, latest_location_path: str, output_type:str):\r\n",
					"        substring_ABS = \"ABS\"\r\n",
					"        substring_ATT = \"ATT\"\r\n",
					"\r\n",
					"        row_key = \"\"\r\n",
					"\r\n",
					"        if data_type == 'calculated-time':\r\n",
					"            if substring_ABS in latest_location_path:\r\n",
					"                row_key = f\"Silver_{output_type}_{substring_ABS}\"\r\n",
					"            elif substring_ATT in latest_location_path:\r\n",
					"                row_key = f\"Silver_{output_type}_{substring_ATT}\"\r\n",
					"        else:\r\n",
					"            row_key = f\"Silver_{output_type}_Default\"\r\n",
					"\r\n",
					"        return row_key\r\n",
					"\r\n",
					"    def update_dataframe_for_units_values(\r\n",
					"        self, \r\n",
					"        spark_session: SparkSession, \r\n",
					"        existing_df: DataFrame, \r\n",
					"        table_service_client: TableServiceClient) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Updates the DataFrame with \"Value\" and \"Unit\" columns based on the rowkey mapping obtained from the PayCodeMetadata table.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"            spark_session (SparkSession): The Spark session.\r\n",
					"            df (DataFrame): The input DataFrame to be updated.\r\n",
					"            table_service_client (TableServiceClient): The table service client used to fetch rowkey values.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            DataFrame: The updated DataFrame with additional \"Value\" and \"Unit\" columns.\r\n",
					"\r\n",
					"        \"\"\"  \r\n",
					"        tr_days_off = \"timeRecords_additionalFields_DAYS_OFF\"\r\n",
					"        tr_hours = \"timeRecords_hours\"\r\n",
					"        tr_gross_pay = \"timeRecords_grossPay\"\r\n",
					"        tr_pay_code = \"timeRecords_payCode\"\r\n",
					"\r\n",
					"        pay_code_mapping_dict = table_service_client.fetch_all_rowkey_values(TransformationHelper.PAYCODE_TABLE_NAME)\r\n",
					"        pay_code_mapping_df = spark_session.createDataFrame(list(pay_code_mapping_dict.items()), [\"pay_code\", \"Unit\"])\r\n",
					"\r\n",
					"        for column_name in [tr_days_off, tr_hours, tr_gross_pay, tr_pay_code]:\r\n",
					"            if column_name not in existing_df.columns:\r\n",
					"                existing_df = existing_df.withColumn(column_name,  lit(None).cast(StringType()))\r\n",
					"\r\n",
					"        # Temporary fix until WFS push change to handle NULL value in Hours - Replace NULL with 0 for hours_value \r\n",
					"        existing_df = existing_df \\\r\n",
					"            .withColumn(tr_hours, when((col(tr_hours).isNull() | (col(tr_hours) == \"null\")), 0)\r\n",
					"            .otherwise(col(tr_hours)))\r\n",
					"        \r\n",
					"        # Temporary fix until WFS push change to handle NULL value in Days - Replace NULL with 0 for hours_value \r\n",
					"        existing_df = existing_df \\\r\n",
					"            .withColumn(tr_days_off, when((col(tr_days_off).isNull() | (col(tr_hours) == \"null\")), 0)\r\n",
					"            .otherwise(col(tr_days_off)))    \r\n",
					"\r\n",
					"        #Temporary fix for converting scientific notations to regular floating-point notation\r\n",
					"        existing_df = existing_df.withColumn(\r\n",
					"            tr_hours, \r\n",
					"            expr(self.data_configs_helper.get_scientific_notation_conversion()))\r\n",
					"\r\n",
					"        joined_df = existing_df.join(pay_code_mapping_df, existing_df.timeRecords_payCode == pay_code_mapping_df.pay_code, \"left\")\r\n",
					"        mapped_pay_code_df = joined_df.select(existing_df[\"*\"], pay_code_mapping_df[\"Unit\"])\r\n",
					"\r\n",
					"        mapped_pay_code_value_df = mapped_pay_code_df.withColumn(\r\n",
					"            \"Value\",\r\n",
					"            when((col(tr_days_off).isNotNull() | (col(tr_days_off) != \"null\")) & (col(\"Unit\") == \"DAYS\"), col(tr_days_off))\r\n",
					"            .when((col(tr_hours).isNotNull() | (col(tr_hours) != \"null\")) & (col(\"Unit\") == \"HOURS\"), col(tr_hours))\r\n",
					"            .when((col(tr_gross_pay).isNotNull() | (col(tr_gross_pay) != \"null\")) & (col(\"Unit\") == \"AMOUNT\"), col(tr_gross_pay))\r\n",
					"            .when((col(tr_hours).isNotNull() | (col(tr_hours) != \"null\")) & (col(\"Unit\") == \"START_DTTM\"), col(tr_hours))\r\n",
					"            .otherwise(lit(None).cast(StringType()))\r\n",
					"        )  \r\n",
					"\r\n",
					"        return mapped_pay_code_value_df   \r\n",
					"\r\n",
					"    def add_index_to_df(self, existing_df: DataFrame, last_sequence: int) -> Tuple[DataFrame, int]:\r\n",
					"        \"\"\"\r\n",
					"        Updates the DataFrame with with new gold index values starting from the previous index value.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"            existing_df (DataFrame): The input DataFrame to be updated.\r\n",
					"            last_sequence (int): Previous index value.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            Tuple[DataFrame, int]: The updated DataFrame along with the latest index value.\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"        sequence_offset = last_sequence + 1\r\n",
					"        \r\n",
					"        indexed_df = self.schema_transformer.filter_on_gold_index_and_zip(existing_df, sequence_offset, TransformationHelper.GOLD_INDEX)                    \r\n",
					"\r\n",
					"        # Fetch the row with max gold_index value from final df\r\n",
					"        maxRow = indexed_df.agg({TransformationHelper.GOLD_INDEX: \"max\"}).collect()[0]\r\n",
					"        \r\n",
					"        # Extract max gold_index from the max row (e.g. Row(max(gold_index)=119))\r\n",
					"        updated_last_sequence = maxRow[f\"max({TransformationHelper.GOLD_INDEX})\"]\r\n",
					"        \r\n",
					"        return indexed_df, updated_last_sequence\r\n",
					"\r\n",
					"    def write_full_or_delta(\r\n",
					"        self, \r\n",
					"        data_type, \r\n",
					"        full_df, \r\n",
					"        df_latest_location_path, \r\n",
					"        previous_gold_index,\r\n",
					"        updated_gold_index, \r\n",
					"        output_type,\r\n",
					"        deletes_df) -> DataFrame:   \r\n",
					"        \"\"\"\r\n",
					"        Writes DataFrame to DataLake and updates ApiDataLocation table with file location.\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            data_type (str): WFS entity type: calculated-time, shift etc.\r\n",
					"            full_df (DataFrame): DataFrame containing latest data to write into location.\r\n",
					"            df_latest_location_path (str): Location to write DataFrame into.\r\n",
					"            previous_gold_index (str): Previous gold index position to derive Delta records with. \r\n",
					"            updated_gold_index (int): Latest gold index position to update table with.\r\n",
					"            output_type (str): File output type, Full or Delta.\r\n",
					"            deletes_df (DataFrame): DataFrame containing deletes performed on df; used to created delta.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            DataFrame: The DataFrame which has been written & updated in storage tables.\r\n",
					"        \"\"\"        \r\n",
					"        output_df, row_key = self._output_file_full_or_delta(data_type, deletes_df, full_df, df_latest_location_path, previous_gold_index, output_type)\r\n",
					"        \r\n",
					"        api_data_location_entity = ApiDataLocationEntity.from_dict(self.table_client.get_table_entity_row(TransformationHelper.API_DATA_LOCATION, data_type, row_key))\r\n",
					"        api_data_location_entity.gold_index = updated_gold_index \r\n",
					"\r\n",
					"        self.datalake_client.write_parquet(output_df, df_latest_location_path, mode=\"overwrite\")\r\n",
					"        api_data_location_entity.dataset_location = df_latest_location_path \r\n",
					"        self.table_client.update_table_entity_row(TransformationHelper.API_DATA_LOCATION, entity=api_data_location_entity.to_dict())\r\n",
					"\r\n",
					"        return output_df\r\n",
					"\r\n",
					"    def _output_file_full_or_delta(self, data_type, deletes_df, full_df, location_path, previous_gold_index, output_type):\r\n",
					"        \"\"\"\r\n",
					"        Create either Full or Delta output dataframe along with the associated row key.\r\n",
					"        Full df is the same full_df that is fed with row key\r\n",
					"        Delta df is ONLY Deletes performed on full_df & MERGES determined by selecting records over previous_gold_index\r\n",
					"        Raises exception if unsupported output_type is detected\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            data_type (DataFrame): The WFS data type: shift, calculated-time etc.\r\n",
					"            deletes_df (DataFrame): The DataFrame containing the latest Deletes performed on existing full_df.\r\n",
					"            full_df (DataFrame): The DataFrame containing the latest fulll data; deletes & merges have already been performed.\r\n",
					"            location_path (str): The path of where the data will be written into; leveraged to pull the row key.\r\n",
					"            previous_gold_index (str): The gold index from the previous run; anything greater than this will be a new merge.\r\n",
					"            output_type (str): The output file type; only supports Delta or Full.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            Tuple: (Delta or Full, RowKey)\r\n",
					"        \"\"\"        \r\n",
					"        \r\n",
					"        if output_type == TransformationHelper.FULL:\r\n",
					"            row_key = self.get_transformation_row_key(data_type, location_path, TransformationHelper.FULL)\r\n",
					"            return full_df, row_key\r\n",
					"        elif output_type == TransformationHelper.DELTA: \r\n",
					"            row_key = self.get_transformation_row_key(data_type, location_path, TransformationHelper.DELTA)\r\n",
					"            \r\n",
					"            merges_delta_df = full_df.filter(col(TransformationHelper.GOLD_INDEX) > previous_gold_index) \\\r\n",
					"                .select(sorted(full_df.columns))\r\n",
					"            \r\n",
					"            deletes_delta_df = full_df.alias(\"full\") \\\r\n",
					"                .join(deletes_df.alias(\"deletes\"), on=TransformationHelper.RECORD_ID, how=\"inner\") \\\r\n",
					"                .where(col(\"full.IsDeleted\") == True) \\\r\n",
					"                .select(\"full.*\") \\\r\n",
					"                .dropDuplicates()\r\n",
					"\r\n",
					"            delta_df = merges_delta_df.unionByName(deletes_delta_df, allowMissingColumns=True)             \r\n",
					"            return delta_df, row_key\r\n",
					"        else:\r\n",
					"            raise ValueError(f\"transformation_helper.write_full_or_delta() expects only Full or Delta values. Receieved: {output_type}.\")\r\n",
					"\r\n",
					"    def process_full_data(self,\r\n",
					"            run_execution_time: str,\r\n",
					"            existing_location_path: str,   \r\n",
					"            merges_df: DataFrame,\r\n",
					"            deletes_df: DataFrame,\r\n",
					"            root_temp_location: str\r\n",
					"        ) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Process full data using the provided DataFrames.\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            existing_location_path (str): The path to the existing data in Parquet format.\r\n",
					"            merges_df (DataFrame): The DataFrame containing the merges data.\r\n",
					"            deletes_df (DataFrame): The DataFrame containing the deletes data.\r\n",
					"            external_match_id (str): The column name representing the external match ID.\r\n",
					"            work_date (str): The column name representing the work date.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            DataFrame: DataFrame of processed Merges and Deletes.\r\n",
					"        \"\"\"\r\n",
					"        external_match_id = \"externalMatchId\"\r\n",
					"        work_date = \"workDate\" \r\n",
					"\r\n",
					"        if (existing_location_path == \"\" or existing_location_path == None):\r\n",
					"            self.telemetry_client.track_exception(f\"Did not detect existing Silver_Temp_Parquet ApiDataLocation table value for {self.data_type}. This should only happen during initial run; if not initial run this is a valid error.\")\r\n",
					"            processed_full_df = merges_df.withColumn(TransformationHelper.IS_DELETED_COL, lit(False)).dropDuplicates()\r\n",
					"\r\n",
					"            return processed_full_df\r\n",
					"        else:\r\n",
					"            existing_full_df = self.datalake_client.read_parquet(existing_location_path)\r\n",
					"            # Write and read the merges DataFrame\r\n",
					"            self.datalake_client.write_generic_format(merges_df, f\"{root_temp_location}/merges_df/{run_execution_time}\", \"parquet\", \"overwrite\")\r\n",
					"            temp_merges_df = self.datalake_client.read_parquet(f\"{root_temp_location}/merges_df/{run_execution_time}\")\r\n",
					"\r\n",
					"            # Write and read the deletes DataFrame\r\n",
					"            self.datalake_client.write_generic_format(deletes_df, f\"{root_temp_location}/deletes_df/{run_execution_time}\", \"parquet\", \"overwrite\")\r\n",
					"            temp_deletes_df = self.datalake_client.read_parquet(f\"{root_temp_location}/deletes_df/{run_execution_time}\")\r\n",
					"\r\n",
					"            # Select required columns for deletes and merges DataFrames\r\n",
					"            deletes_to_process_df = temp_deletes_df.select(\r\n",
					"                TransformationHelper.EXTERNAL_MATCH_ID, TransformationHelper.WORK_DATE, TransformationHelper.RECORD_ID)            \r\n",
					"            merges_to_process_soft_deletes_df = temp_merges_df.select(\r\n",
					"                TransformationHelper.EXTERNAL_MATCH_ID, TransformationHelper.WORK_DATE)\r\n",
					"            merges_to_process_inserts_df = temp_merges_df.select(\"*\") \\\r\n",
					"                .withColumn(TransformationHelper.IS_DELETED_COL, lit(False))\r\n",
					"\r\n",
					"            # Process full merge and delete look up\r\n",
					"            processed_full_df = self.process_full_merge_and_delete_look_up(\r\n",
					"                existing_full_df, \r\n",
					"                deletes_to_process_df,\r\n",
					"                merges_to_process_soft_deletes_df,\r\n",
					"                merges_to_process_inserts_df)\r\n",
					"            \r\n",
					"            return processed_full_df     \r\n",
					"\r\n",
					"    def update_ingestion_api_data_location_entity_is_processed(self, ingestion_api_data_location_entity):\r\n",
					"        ingestion_api_data_location_entity.is_processed = True\r\n",
					"        self.table_client.update_table_entity_row(TransformationHelper.API_DATA_LOCATION, entity=ingestion_api_data_location_entity.to_dict())\r\n",
					"\r\n",
					"    def log_invalid_unit_rows(self, calculated_time_unit_df):\r\n",
					"        invalid_unit_df = calculated_time_unit_df.filter((col(\"Unit\").isNull()) | (col(\"Unit\") == \"null\"))\r\n",
					"        \r\n",
					"        invalid_unit_count = invalid_unit_df.count()\r\n",
					"\r\n",
					"        if invalid_unit_count > 0:\r\n",
					"            invalid_code_location = f\"silver/errors/{self.data_type}_missing_payCodes/{self.run_execution_time}\"\r\n",
					"            self.datalake_client.write_parquet(invalid_unit_df, invalid_code_location, mode=\"overwrite\")\r\n",
					"            self.telemetry_client.track_exception(f\"{invalid_unit_count} invalid paycode mappings detected for {self.data_type} on {self.run_execution_time}. Review mapping codes in parquet {invalid_code_location} & verify why mappings are missing from PayCodeMetadata table.\")\r\n",
					"            self.telemetry_client.track_metric(f\"Error_Mapping_Codes_{self.data_type}\", invalid_unit_count),          \r\n",
					"\r\n",
					"        return invalid_unit_df\r\n",
					"\r\n",
					"    def log_invalid_hashed_ids_rows(self, ingestion_df):\r\n",
					"        invalid_hashed_ids_df = ingestion_df.filter(col(\"externalMatchId\").contains(\"-\"))   \r\n",
					"        \r\n",
					"        invalid_hashed_ids_count = invalid_hashed_ids_df.count()\r\n",
					"\r\n",
					"        if invalid_hashed_ids_count > 0:\r\n",
					"            invalid_hashed_ids_location = f\"silver/errors/{self.data_type}_invalid_hashed_ids/{self.run_execution_time}\"\r\n",
					"            self.datalake_client.write_parquet(invalid_hashed_ids_df, invalid_hashed_ids_location, mode=\"overwrite\")\r\n",
					"            self.telemetry_client.track_exception(f\"{invalid_hashed_ids_count} invalid hashed ids detected for {self.data_type} on {self.run_execution_time}. Review records in parquet {invalid_hashed_ids_location}.\")\r\n",
					"            self.telemetry_client.track_metric(f\"Error_Hashed_Ids_{self.data_type}\", invalid_hashed_ids_count),          \r\n",
					"        \r\n",
					"        return invalid_hashed_ids_df\r\n",
					"\r\n",
					"    def _filter_deletes_merges_dfs(self, df: DataFrame) -> Tuple[DataFrame, DataFrame]:\r\n",
					"        \"\"\"\r\n",
					"        Filters DataFrame based on DataChange column for merges and deletes.\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            df (DataFrame): The DataFrame containing data.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            Tupple[DataFrame, DataFrame] : A tuple with deletes & merges.\r\n",
					"        \"\"\"                    \r\n",
					"        deletes_df = df.filter(col(TransformationHelper.DATA_CHANGE_COL) == TransformationHelper.DELETE)                              \r\n",
					"        merges_df = df.filter(col(TransformationHelper.DATA_CHANGE_COL) == TransformationHelper.MERGE)\r\n",
					"\r\n",
					"        return deletes_df, merges_df\r\n",
					"\r\n",
					"    def perform_ingestion_validations(self, ingestion_df: DataFrame) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Performs data validations on initial ingestion data.\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            ingestion_df (DataFrame): The DataFrame containing the initial data.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            DataFrame : A DataFrame which has been validated.\r\n",
					"        \"\"\"            \r\n",
					"        self.validation_engine.validate_df_schema(ingestion_df, self.data_configs_helper.get_silver_validation_df_rules_init_schema())\r\n",
					"        validated_df = self.validation_engine.validate_df_columns(ingestion_df, self.data_configs_helper.get_silver_validation_column_rules())\r\n",
					"\r\n",
					"        return validated_df\r\n",
					"\r\n",
					"    def perform_ingestion_transformations(self, ingestion_df: DataFrame) -> Tuple[DataFrame, DataFrame]:\r\n",
					"        \"\"\"\r\n",
					"        Performs data transformations on initial ingestion data.\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            ingestion_df (DataFrame): The DataFrame containing the initial data.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            Tupple[DataFrame, DataFrame] : A tuple with transformed deletes & merges.\r\n",
					"        \"\"\"            \r\n",
					"        group_by_max_id_df = self.get_max_index_by_id(ingestion_df)\r\n",
					"        group_by_max_id_df = self.add_column(group_by_max_id_df, \"gold_index\", lit(-1))\r\n",
					"        self.telemetry_client.track_event(f\"Ingestion data count after group by max index for {self.data_type} is {group_by_max_id_df.count()}.\")\r\n",
					"\r\n",
					"        deletes_df, merges_df = self._filter_deletes_merges_dfs(group_by_max_id_df)\r\n",
					"\r\n",
					"        return deletes_df, merges_df\r\n",
					"    \r\n",
					"    def perform_calculated_time_transformations(self, calc_time_df: DataFrame):\r\n",
					"        \"\"\"\r\n",
					"        Performs calculated-time specific data transformations.\r\n",
					"        Adds unit / value columns & logs any invalid mappings.\r\n",
					"        Creates ATT & ABS DataFrames from original DataFrame.\r\n",
					"\r\n",
					"        Arguments:\r\n",
					"            calc_time_df (DataFrame): The DataFrame containing the merges data.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"            Object : An object, the key is the DataFrame and the value is the expression with the type.\r\n",
					"        \"\"\"        \r\n",
					"        unit_values_merges_df = self.update_dataframe_for_units_values(self.spark_session, calc_time_df, self.table_client)\r\n",
					"        self.telemetry_client.track_event(f\"Updated data with Values & Units cols for {self.data_type}.\")\r\n",
					"\r\n",
					"        self.log_invalid_unit_rows(unit_values_merges_df)\r\n",
					"\r\n",
					"        transform_expr_ABS = self.data_configs_helper.get_time_off_data()[\"transformExprABS\"]\r\n",
					"        transform_expr_ATT = self.data_configs_helper.get_time_off_data()[\"transformExprATT\"]\r\n",
					"\r\n",
					"        merges_df_ABS = unit_values_merges_df.filter(expr(transform_expr_ABS))\r\n",
					"        merges_df_ATT = unit_values_merges_df.filter(expr(transform_expr_ATT))\r\n",
					"        self.telemetry_client.track_event(f\"Created subset dataframes of type ATT and ABS for {self.data_type}.\")        \r\n",
					"\r\n",
					"        return { merges_df_ABS: (transform_expr_ABS, \"ABS\"), merges_df_ATT: (transform_expr_ATT, \"ATT\") }"
				],
				"execution_count": 2
			}
		]
	}
}