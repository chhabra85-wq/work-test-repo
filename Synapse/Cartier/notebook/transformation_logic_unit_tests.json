{
	"name": "transformation_logic_unit_tests",
	"properties": {
		"folder": {
			"name": "tests/test_notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "634c7c40-28cb-4576-a296-d3e1af1a67e2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpool",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run transformation/logics/transformation_logic"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"from unittest.mock import Mock, MagicMock, ANY\r\n",
					"from datetime import datetime, timezone\r\n",
					"from uuid import UUID\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\r\n",
					"from pyspark.sql.functions import expr, lit\r\n",
					"\r\n",
					"class TestTransformationLogicUnit(SparkSessionBaseClass):\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        super().setUp()\r\n",
					"\r\n",
					"    _TIME_OFF_DATA_FILTER = { \\\r\n",
					"        \"transformExprABS\": \"CASE WHEN timeRecords_additionalFields_SP_EARNINGS_CODE = 'ABS' THEN 1 ELSE 0 END = 1\", \\\r\n",
					"        \"transformExprATT\": \"CASE WHEN timeRecords_additionalFields_SP_EARNINGS_CODE = 'ATT' THEN 1 ELSE 0 END = 1\" \\\r\n",
					"    }\r\n",
					"    _SILVER_VALIDATION_DF_RULES_INIT_SCHEMA = { \"externalMatchId\": \"StringType()\", \"index\": \"IntegerType()\", \"workDate\": \"StringType()\" }\r\n",
					"    _DATA_PRIMARY_KEY = \"CASE WHEN dataChange = 'DELETE' THEN CONCAT_WS('_', externalMatchId, workDate) ELSE CONCAT_WS('_', externalMatchId, timeRecords_workDate) END\"\r\n",
					"\r\n",
					"    def _create_ingested_abs_df(self):\r\n",
					"        calculated_time_abs_schema = StructType([\r\n",
					"            StructField(\"dataChange\", StringType(), nullable=True),\r\n",
					"            StructField(\"externalMatchId\", StringType(), nullable=True),\r\n",
					"            StructField(\"recordId\", StringType(), nullable=True),\r\n",
					"            StructField(\"sequence\", LongType(), nullable=True),\r\n",
					"            StructField(\"workDate\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_amount\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_comments\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_effectiveRate\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_employeeJob\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_endTimestamp\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_grossPay\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_hours\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_index\", LongType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_jobId\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_payCode\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_payCurrencyCode\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_payrollId\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_recordType\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_startTimestamp\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_workDate\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_C_ABS_REASON_CODE\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_C_TIME_OFF_REQUEST_ID\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_OTHER_HOURS\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_OUTSIDE_SCHEDULE_FLAG\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_SERVER_TOTALS\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_SP_EARNINGS_CODE\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_WORK_SCHED_FIRST_ENTRY\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_WORK_SCHED_ID\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_WORK_SCHED_START\", StringType(), nullable=True),\r\n",
					"            StructField(\"index\", IntegerType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_DAYS_OFF\", StringType(), nullable=True),\r\n",
					"            StructField(\"timeRecords_additionalFields_DAY_NIGHT_INDICATOR\", StringType(), nullable=True),\r\n",
					"            StructField(\"Unit\", StringType(), nullable=True),\r\n",
					"        ])\r\n",
					"\r\n",
					"        calculated_time_abs_data = [\r\n",
					"            (\"DELETE\", \"id_1\", \"record1\", 1, \"2023-01-01\", \"10.0\", \"Comment 1\", \"5.0\", \"Job 1\", \"2023-01-01T09:00:00\", \"100.0\", \"2.5\", 1, \"job123\", \"pay1\", \"USD\", \"payroll1\", \"type1\", \"2023-01-01T08:00:00\", \"2023-01-01\", \"reason1\", \"request1\", \"2.0\", \"true\", \"totals1\", \"ABS\", \"first_entry1\", \"sched_id1\", \"sched_start1\", 1, \"2\", \"night\", \"PayCode\"),\r\n",
					"            (\"DELETE\", \"id_2\", \"record2\", 2, \"2023-01-02\", \"20.0\", \"Comment 2\", \"7.5\", \"Job 2\", \"2023-01-02T09:00:00\", \"200.0\", \"5.0\", 2, \"job456\", \"pay2\", \"USD\", \"payroll2\", \"type2\", \"2023-01-02T08:00:00\", \"2023-01-02\", \"reason2\", \"request2\", \"4.0\", \"false\", \"totals2\", \"ATT\", \"first_entry2\", \"sched_id2\", \"sched_start2\", 2, \"4\", \"day\", \"PayCode\"),\r\n",
					"            (\"MERGE\", \"id_3\", \"record3\", 3, \"2023-01-03\", \"15.0\", \"Comment 3\", \"6.0\", \"Job 3\", \"2023-01-03T09:00:00\", \"150.0\", \"4.0\", 3, \"job789\", \"pay3\", \"USD\", \"payroll3\", \"type3\", \"2023-01-03T08:00:00\", \"2023-01-03\", \"reason3\", \"request3\", \"6.0\", \"true\", \"totals3\", \"ABS\", \"first_entry3\", \"sched_id3\", \"sched_start3\", 3, \"6\", \"day\", \"PayCode\"),\r\n",
					"            (\"MERGE\", \"id_4\", \"record4\", 4, \"2023-01-04\", \"12.0\", \"Comment 4\", \"7.0\", \"Job 4\", \"2023-01-04T09:00:00\", \"120.0\", \"3.5\", 4, \"job012\", \"pay4\", \"USD\", \"payroll4\", \"type4\", \"2023-01-04T08:00:00\", \"2023-01-04\", \"reason4\", \"request4\", \"8.0\", \"false\", \"totals4\", \"ABS\", \"first_entry4\", \"sched_id4\", \"sched_start4\", 4, \"8\", \"night\", \"PayCode\"),\r\n",
					"            (\"MERGE\", \"id_5\", \"record5\", 5, \"2023-01-05\", \"8.0\", \"Comment 5\", \"8.0\", \"Job 5\", \"2023-01-05T09:00:00\", \"80.0\", \"8.0\", 5, \"job345\", \"pay5\", \"USD\", \"payroll5\", \"type5\", \"2023-01-05T08:00:00\", \"2023-01-05\", \"reason5\", \"request5\", \"10.0\", \"true\", \"totals5\", \"ATT\", \"first_entry5\", \"sched_id5\", \"sched_start5\", 5, \"10\", \"day\", \"PayCode\")\r\n",
					"        ]\r\n",
					"\r\n",
					"        return self.spark_session.createDataFrame(calculated_time_abs_data, calculated_time_abs_schema)\r\n",
					"\r\n",
					"    def _create_param_obj(self, data_type):\r\n",
					"        return { \r\n",
					"            \"data_type\": data_type, \r\n",
					"            \"run_id\": str(UUID(int=0)), \r\n",
					"            \"run_execution_time\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\r\n",
					"        }\r\n",
					"\r\n",
					"    def test_data_has_been_processed(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of transformation logic when existing ingestion data has already been processed.\r\n",
					"        Expected behavior is that no transformations take place and function exits.\r\n",
					"        Asserts the expected log calls & fetch call to get latest location.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        expected_log_count = 3\r\n",
					"        expected_table_client_count = 1\r\n",
					"        \r\n",
					"        data_type_mock = \"default\"\r\n",
					"        ingestion_location_mock = f\"bronze/location/{data_type_mock}\"\r\n",
					"        params_mock = self._create_param_obj(data_type_mock)\r\n",
					"\r\n",
					"        data_configs_helper = Mock()\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        transformation_helper_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        ms_spark_utils = Mock()\r\n",
					"\r\n",
					"        has_been_processed_entity = ApiDataLocationEntity(\r\n",
					"            partition_key = data_type_mock, \r\n",
					"            row_key = data_type_mock, \r\n",
					"            dataset_location = ingestion_location_mock,\r\n",
					"            gold_index = 0,\r\n",
					"            is_processed = True)\r\n",
					"\r\n",
					"        table_client_mock = Mock()\r\n",
					"        table_client_mock.get_table_entity_row = MagicMock(return_value = has_been_processed_entity.to_dict())\r\n",
					"\r\n",
					"        transformation_logic_engine = TransformationLogicEngine(\r\n",
					"            self.spark_session, \r\n",
					"            data_configs_helper, \r\n",
					"            datalake_client_mock, \r\n",
					"            transformation_helper_mock, \r\n",
					"            table_client_mock,\r\n",
					"            telemetry_client_mock, \r\n",
					"            ms_spark_utils,\r\n",
					"            params_mock\r\n",
					"        )\r\n",
					"\r\n",
					"        # ACT\r\n",
					"        transformation_logic_engine.execute()\r\n",
					"\r\n",
					"        log_count_result = telemetry_client_mock.track_event.call_count\r\n",
					"        table_client_count_result = table_client_mock.get_table_entity_row.call_count\r\n",
					"\r\n",
					"        # ASSERT Called\r\n",
					"        assert log_count_result == expected_log_count, f\"Expected {expected_log_count} track_event() calls. Result was {log_count_result}\"\r\n",
					"        assert table_client_count_result == expected_table_client_count, f\"Expected {table_client_count_result} get_table_entity_row() calls. Result was {expected_table_client_count}\"\r\n",
					"        table_client_mock.get_table_entity_row.assert_called_once_with(TransformationLogicEngine.API_DATA_LOCATION, data_type_mock, \"Bronze_Full_Default\")\r\n",
					"\r\n",
					"    def test_no_merges_deletes_to_process(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of transformation logic when there are 0 merges / deletes to process.\r\n",
					"        Expected behavior is transformation calls are invoked until check for 0 merges / deletes step.\r\n",
					"        Asserts the expected log calls & transformation calls.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        data_type_mock = \"default\"\r\n",
					"        default_schema = StructType([\r\n",
					"            StructField(\"dataChange\", StringType()),\r\n",
					"            StructField(\"externalMatchId\", StringType())\r\n",
					"        ])\r\n",
					"        empty_df = self.spark_session.createDataFrame([], default_schema)\r\n",
					"        latest_bronze_location_mock = f\"bronze/location/{data_type_mock}\"\r\n",
					"        \r\n",
					"        expected_log_count = 11\r\n",
					"\r\n",
					"        data_configs_helper_mock = Mock()\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        ms_spark_utils = Mock()\r\n",
					"        params_mock = self._create_param_obj(data_type_mock)\r\n",
					"\r\n",
					"        initial_run_entity_data = ApiDataLocationEntity(\r\n",
					"            partition_key = data_type_mock, \r\n",
					"            row_key = data_type_mock, \r\n",
					"            dataset_location = latest_bronze_location_mock,\r\n",
					"            gold_index = 0,\r\n",
					"            is_processed = False)\r\n",
					"\r\n",
					"        table_client_mock = Mock()\r\n",
					"        table_client_mock.get_table_entity_row = MagicMock(return_value = initial_run_entity_data.to_dict())\r\n",
					"\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        datalake_client_mock.read_parquet = MagicMock(return_value = empty_df)\r\n",
					"\r\n",
					"        transformation_helper_mock = Mock()\r\n",
					"        transformation_helper_mock.perform_ingestion_validations = MagicMock(return_value = empty_df)\r\n",
					"        transformation_helper_mock.perform_ingestion_transformations =  MagicMock(return_value = (empty_df, empty_df))\r\n",
					"        \r\n",
					"        transformation_logic_engine = TransformationLogicEngine(\r\n",
					"            self.spark_session, \r\n",
					"            data_configs_helper_mock, \r\n",
					"            datalake_client_mock, \r\n",
					"            transformation_helper_mock, \r\n",
					"            table_client_mock,\r\n",
					"            telemetry_client_mock, \r\n",
					"            ms_spark_utils,\r\n",
					"            params_mock)    \r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        transformation_logic_engine.execute()\r\n",
					"\r\n",
					"        log_count_result = telemetry_client_mock.track_event.call_count\r\n",
					"        \r\n",
					"        # ASSERT CALLED\r\n",
					"        log_count_msg = f\"Expected {expected_log_count} track_event() calls. Result was {log_count_result}\"\r\n",
					"        assert log_count_result == expected_log_count, log_count_msg\r\n",
					"\r\n",
					"        table_entity_row_msg = f\"Expected 1 get_table_entity_row() call. Result was {table_client_mock.get_table_entity_row.call_count}\"        \r\n",
					"        table_client_mock.get_table_entity_row.assert_called_once_with(TransformationLogicEngine.API_DATA_LOCATION, data_type_mock, \"Bronze_Full_Default\"), table_entity_row_msg\r\n",
					"\r\n",
					"        read_parquet_msg = f\"Expected 1 read_parquet() call. Result was {datalake_client_mock.read_parquet.call_count}\"        \r\n",
					"        datalake_client_mock.read_parquet.assert_called_once_with(latest_bronze_location_mock), read_parquet_msg\r\n",
					"\r\n",
					"        hashed_ids_rows_msg = f\"Expected 1 log_invalid_hashed_ids_rows() call. Result was {transformation_helper_mock.log_invalid_hashed_ids_rows.call_count}\"        \r\n",
					"        transformation_helper_mock.log_invalid_hashed_ids_rows.assert_called_once_with(empty_df), hashed_ids_rows_msg\r\n",
					"        \r\n",
					"        validations_msg = f\"Expected 1 perform_ingestion_validations() call. Result was {transformation_helper_mock.perform_ingestion_validations.call_count}\"        \r\n",
					"        transformation_helper_mock.perform_ingestion_validations.assert_called_once_with(ANY), validations_msg\r\n",
					"\r\n",
					"        transformations_msg = f\"Expected 1 perform_ingestion_transformations() call. Result was {transformation_helper_mock.perform_ingestion_transformations.call_count}\"        \r\n",
					"        transformation_helper_mock.perform_ingestion_transformations.assert_called_once_with(empty_df), transformations_msg\r\n",
					"        \r\n",
					"        is_processed_msg = f\"Expected 1 update_ingestion_api_data_location_entity_is_processed() call. Result was {transformation_helper_mock.update_ingestion_api_data_location_entity_is_processed.call_count}\"        \r\n",
					"        transformation_helper_mock.update_ingestion_api_data_location_entity_is_processed.assert_called_once_with(ANY), is_processed_msg\r\n",
					"\r\n",
					"    def test_merges_deletes_to_process_default_type(self):\r\n",
					"        \"\"\"\r\n",
					"        Tests the behavior of transformation logic when there are valid merges / deletes to process.\r\n",
					"        Expected behavior is transformation calls are invoked for default type steps.\r\n",
					"        Asserts the expected log calls & transformation calls.\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # ARRANGE\r\n",
					"        data_type_mock = \"default\"\r\n",
					"        default_schema = StructType([\r\n",
					"            StructField(\"dataChange\", StringType(), nullable=True),\r\n",
					"            StructField(\"externalMatchId\", StringType(), nullable=True),\r\n",
					"            StructField(\"recordId\", StringType(), nullable=True),\r\n",
					"            StructField(\"sequence\", LongType(), nullable=True),\r\n",
					"            StructField(\"workDate\", StringType(), nullable=True),\r\n",
					"        ])\r\n",
					"\r\n",
					"        default_data = [\r\n",
					"            (\"DELETE\", \"id_1\", \"record1\", 1, \"2023-01-01\"),\r\n",
					"            (\"DELETE\", \"id_2\", \"record2\", 2, \"2023-01-02\"),\r\n",
					"            (\"MERGE\", \"id_3\", \"record3\", 3, \"2023-01-03\"),\r\n",
					"            (\"MERGE\", \"id_4\", \"record4\", 4, \"2023-01-04\"),\r\n",
					"            (\"MERGE\", \"id_5\", \"record5\", 5, \"2023-01-05\")\r\n",
					"        ]\r\n",
					"\r\n",
					"        default_df = self.spark_session.createDataFrame(default_data, default_schema)\r\n",
					"        default_deletes_df = default_df\r\n",
					"        default_merges_df = default_df\r\n",
					"        existing_silver_location_mock = f\"silver/location/{data_type_mock}\"\r\n",
					"        latest_bronze_location_mock = f\"bronze/location/{data_type_mock}\"\r\n",
					"\r\n",
					"        telemetry_client_mock = Mock()\r\n",
					"        ms_spark_utils_mock = Mock()\r\n",
					"        params_mock = self._create_param_obj(data_type_mock)\r\n",
					"\r\n",
					"        ingestion_entity_data = ApiDataLocationEntity(\r\n",
					"            partition_key = data_type_mock, \r\n",
					"            row_key = data_type_mock, \r\n",
					"            dataset_location = latest_bronze_location_mock,\r\n",
					"            gold_index = 0,\r\n",
					"            is_processed = False)\r\n",
					"\r\n",
					"        existing_trasnform_entity_data = ApiDataLocationEntity(\r\n",
					"            partition_key = data_type_mock, \r\n",
					"            row_key = data_type_mock, \r\n",
					"            dataset_location = existing_silver_location_mock,\r\n",
					"            gold_index = 0,\r\n",
					"            is_processed = False)\r\n",
					"\r\n",
					"        table_client_mock = Mock()\r\n",
					"        table_client_mock.get_table_entity_row.side_effect = [ingestion_entity_data.to_dict(), existing_trasnform_entity_data.to_dict()]\r\n",
					"\r\n",
					"        datalake_client_mock = Mock()\r\n",
					"        datalake_client_mock.read_parquet = MagicMock(return_value = default_df)\r\n",
					"\r\n",
					"        transformation_helper_mock = Mock()\r\n",
					"        transformation_helper_mock.perform_ingestion_validations = MagicMock(return_value = default_df)\r\n",
					"        transformation_helper_mock.perform_ingestion_transformations =  MagicMock(return_value = (default_df, default_df))\r\n",
					"        transformation_helper_mock.process_full_data = MagicMock(return_value = default_df)\r\n",
					"        transformation_helper_mock.filter_time_off = MagicMock(return_value = default_df)\r\n",
					"        transformation_helper_mock.get_transformation_row_key = MagicMock(return_value = \"default_row_key\")\r\n",
					"        transformation_helper_mock.retrieve_index_value_from_table = MagicMock(return_value = 10)\r\n",
					"        transformation_helper_mock.add_index_to_df = MagicMock(return_value = (default_df, 15))\r\n",
					"        transformation_helper_mock.write_full_or_delta.side_effect = [default_df, default_df]\r\n",
					"\r\n",
					"        data_configs_helper_mock = Mock()\r\n",
					"        data_configs_helper_mock.update_df_column_names = MagicMock(return_value = default_df)\r\n",
					"\r\n",
					"        transformation_logic_engine = TransformationLogicEngine(\r\n",
					"            self.spark_session, \r\n",
					"            data_configs_helper_mock, \r\n",
					"            datalake_client_mock, \r\n",
					"            transformation_helper_mock, \r\n",
					"            table_client_mock,\r\n",
					"            telemetry_client_mock, \r\n",
					"            ms_spark_utils_mock,\r\n",
					"            params_mock)    \r\n",
					"        \r\n",
					"        # ACT\r\n",
					"        transformation_logic_engine.execute()\r\n",
					"\r\n",
					"        log_count_result = telemetry_client_mock.track_event.call_count\r\n",
					"        read_parquet_count_result = datalake_client_mock.read_parquet.call_count\r\n",
					"        write_generic_format_result = datalake_client_mock.write_generic_format.call_count\r\n",
					"        get_table_entity_row_count_result = table_client_mock.get_table_entity_row.call_count   \r\n",
					"        write_full_or_delta_count_result = transformation_helper_mock.write_full_or_delta.call_count \r\n",
					"        \r\n",
					"        # ASSERT CALLED\r\n",
					"        log_count_msg = f\"Expected 21 track_event() calls. Result was {log_count_result}\"\r\n",
					"        assert log_count_result == 21, log_count_msg\r\n",
					"\r\n",
					"        get_table_entity_row_msg = f\"Expected 2 get_table_entity_row() calls. Result was {get_table_entity_row_count_result}\"\r\n",
					"        assert get_table_entity_row_count_result == 2, get_table_entity_row_msg\r\n",
					"\r\n",
					"        read_parquet_msg = f\"Expected 2 read_parquet() call. Result was {read_parquet_count_result}\"        \r\n",
					"        assert read_parquet_count_result == 2, read_parquet_msg\r\n",
					"\r\n",
					"        write_generic_format_msg = f\"Expected 2 write_generic_format() call. Result was {write_generic_format_result}\"        \r\n",
					"        assert write_generic_format_result == 2, write_generic_format_msg\r\n",
					"\r\n",
					"        write_full_or_delta_msg = f\"Expected 2 write_full_or_delta() call. Result was {transformation_helper_mock.write_full_or_delta.call_count}\"\r\n",
					"        assert write_full_or_delta_count_result == 2, write_full_or_delta_msg\r\n",
					"        \r\n",
					"        transformation_helper_mock.log_invalid_hashed_ids_rows.assert_called_once_with(default_df)        \r\n",
					"        transformation_helper_mock.perform_ingestion_validations.assert_called_once_with(ANY)\r\n",
					"        transformation_helper_mock.perform_ingestion_transformations.assert_called_once_with(default_df)\r\n",
					"        transformation_helper_mock.update_ingestion_api_data_location_entity_is_processed.assert_called_once_with(ANY)\r\n",
					"        transformation_helper_mock.process_full_data.assert_called_once_with(ANY, ANY, ANY, ANY, ANY)\r\n",
					"        transformation_helper_mock.filter_time_off.assert_called_once_with(ANY, ANY)\r\n",
					"        transformation_helper_mock.get_transformation_row_key.assert_called_once_with(ANY, ANY, ANY)        \r\n",
					"        table_client_mock.retrieve_index_value_from_table.assert_called_once_with(ANY, ANY, ANY, ANY)        \r\n",
					"        transformation_helper_mock.add_index_to_df.assert_called_once_with(ANY, ANY)\r\n",
					"        table_client_mock.update_table_entity_row.assert_called_once_with(ANY, entity=ANY)\r\n",
					"        ms_spark_utils_mock.delete_directory.assert_called_once_with(ANY, recurse=ANY)"
				],
				"execution_count": null
			}
		]
	}
}