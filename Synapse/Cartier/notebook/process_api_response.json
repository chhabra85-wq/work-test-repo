{
	"name": "process_api_response",
	"properties": {
		"folder": {
			"name": "ingestion/utilities"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpoolv34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f1d5bde6-7c80-4d3c-91b9-2e20e4856c1f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f8a339ea-4445-4d69-a4bb-b7c77c9db241/resourceGroups/RG-Cartier-Dev/providers/Microsoft.Synapse/workspaces/synw-time-cartier-dev/bigDataPools/devpool",
				"name": "devpoolv34",
				"type": "Spark",
				"endpoint": "https://synw-time-cartier-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import json\r\n",
					"from datetime import datetime\r\n",
					"from pyspark.sql import SparkSession, DataFrame\r\n",
					"from pyspark.sql.functions import lit\r\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType\r\n",
					"from pyspark.rdd import RDD\r\n",
					"\r\n",
					"class ApiProcessor:\r\n",
					"    BRONZE_INDEX = \"bronze_index\"\r\n",
					"    BRONZE_INDEX_TABLE_COLUMN_NAME = \"BronzeIndex\"\r\n",
					"    DATA_LOCATION_TABLE_NAME = \"ApiDataLocation\"\r\n",
					"    META_DATA_TABLE_NAME = \"ApiMetaData\"\r\n",
					"    BRONZE_DEFAULT_ROW_KEY = \"Bronze_Full_Default\"\r\n",
					"    \r\n",
					"    def __init__(\r\n",
					"        self, \r\n",
					"        spark: SparkSession, \r\n",
					"        params: str, \r\n",
					"        telemetry_client: TelemetryClient, \r\n",
					"        datalake_client: DataLakeClient, \r\n",
					"        table_service_client: AzureTableClient, \r\n",
					"        schema_transformer, \r\n",
					"        flatten_nested_json: FlattenNestedJson,\r\n",
					"        validation_engine,\r\n",
					"        data_configs_helper):\r\n",
					"        \"\"\"Constructor setting the class dependencies\"\"\"        \r\n",
					"        self.spark = spark\r\n",
					"        self.api_name = params.get(\"api_name\")\r\n",
					"        self.run_execution_time = params.get(\"run_execution_time\")\r\n",
					"        self.telemetry_client = telemetry_client\r\n",
					"        self.datalake_client = datalake_client\r\n",
					"        self.table_service_client = table_service_client\r\n",
					"        self.schema_transformer = schema_transformer\r\n",
					"        self.flatten_nested_json = flatten_nested_json\r\n",
					"        self.validation_engine = validation_engine\r\n",
					"        self.data_configs_helper = data_configs_helper\r\n",
					"\r\n",
					"    def get_file_location(self):\r\n",
					"        \"\"\"Returns the name of the Delta table for a given API\"\"\"\r\n",
					"        return f\"bronze/{self.api_name}{self.run_execution_time}\"\r\n",
					"\r\n",
					"    def create_update_sequence_rdd(self, api_response: dict) -> RDD:\r\n",
					"        sc = self.spark.sparkContext\r\n",
					"        # Convert the 'updateSequence' value to a JSON string\r\n",
					"        update_sequence_json = json.dumps(api_response.get(\"updateSequence\"))\r\n",
					"        return sc.parallelize([update_sequence_json])    \r\n",
					"\r\n",
					"    def convert_rdd_to_dataframe(self, rdd: RDD) -> DataFrame:\r\n",
					"        return self.spark.read.option(\"mergeSchema\", \"true\").json(rdd)    \r\n",
					"\r\n",
					"\r\n",
					"    def load_data_into_delta_lake(self, api_response: dict, last_sequence: int, next_cursor: str) -> DataFrame:\r\n",
					"        \"\"\"\r\n",
					"        Converts the API response to a flattened DataFrame and loads it into Delta Lake.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        api_response (dict): The response from the API as a dictionary.\r\n",
					"        last_sequence (int): The last sequence number that was processed.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"        DataFrame: The flattened DataFrame that was loaded into Delta Lake.\r\n",
					"        \"\"\"\r\n",
					"        # Create an RDD from the JSON data\r\n",
					"        update_sequence_rdd = self.create_update_sequence_rdd(api_response)\r\n",
					"\r\n",
					"        # Read the JSON data into a DataFrame with the 'mergeSchema' option\r\n",
					"        result_df = self.convert_rdd_to_dataframe(update_sequence_rdd)\r\n",
					"\r\n",
					"        self.validation_engine.validate_df_schema(result_df, self.data_configs_helper.get_bronze_validation_df_rules_init_schema())\r\n",
					"        self.telemetry_client.track_event(f\"Validated ingested initial schema for {self.api_name}.\")            \r\n",
					"\r\n",
					"        # calculate the start and end sequence numbers\r\n",
					"        sequence_offset = last_sequence + 1\r\n",
					"        self.telemetry_client.track_event({\"BronzeObjectIndex Sequence\": [str(sequence_offset)]})\r\n",
					"\r\n",
					"        result_df = result_df.withColumn(\"cursor\", lit(next_cursor)).withColumn(\"ingestionTime\", lit(self.run_execution_time))\r\n",
					"\r\n",
					"        # add the index on object level to Un-Flattened DataFrame\r\n",
					"        result_df_with_index = self.schema_transformer.add_index_column(result_df, sequence_offset, ApiProcessor.BRONZE_INDEX)\r\n",
					"       \r\n",
					"        self.validation_engine.validate_df_columns(result_df_with_index, self.data_configs_helper.get_bronze_validation_column_rules())\r\n",
					"        self.telemetry_client.track_event(f\"Validated ingested data columns for {self.api_name}.\")            \r\n",
					"\r\n",
					"        # flatten the DataFrame to remove nested structures\r\n",
					"        flattened_df = self.flatten_nested_json.flatten_df(result_df_with_index)\r\n",
					"\r\n",
					"        # define the Delta Lake File name\r\n",
					"        raw_file_location = self.get_file_location()\r\n",
					"\r\n",
					"        # write the flattened DataFrame to Delta Lake\r\n",
					"        self.datalake_client.write_parquet(flattened_df, raw_file_location)\r\n",
					"\r\n",
					"        return result_df_with_index, raw_file_location\r\n",
					"\r\n",
					"    def process_api_response(self, api_response_text: str) -> None:\r\n",
					"        \"\"\"\r\n",
					"        Processes the API response.\r\n",
					"\r\n",
					"        Parameters:\r\n",
					"        api_response_text (str): The response from the API as a string.\r\n",
					"\r\n",
					"        Returns:\r\n",
					"        None\r\n",
					"        \"\"\"\r\n",
					"        # parse the JSON response text\r\n",
					"        api_response = json.loads(api_response_text.text)\r\n",
					"        # set the partition and row keys for the metadata table\r\n",
					"        partition_key = self.api_name\r\n",
					"        row_key = self.api_name\r\n",
					"        # get the last sequence number from the metadata table\r\n",
					"        last_sequence = self.table_service_client.retrieve_index_value_from_table(partition_key, row_key, ApiProcessor.BRONZE_INDEX_TABLE_COLUMN_NAME, ApiProcessor.META_DATA_TABLE_NAME)\r\n",
					"\r\n",
					"        next_cursor = api_response.get(\"nextCursor\")\r\n",
					"\r\n",
					"        # load the data into Delta Lake\r\n",
					"        result_df_with_index, raw_file_location = self.load_data_into_delta_lake(api_response, last_sequence, next_cursor)\r\n",
					"\r\n",
					"        row_key_data_location = ApiProcessor.BRONZE_DEFAULT_ROW_KEY\r\n",
					"        api_data_location_entity = ApiDataLocationEntity.from_dict(self.table_service_client.get_table_entity_row(ApiProcessor.DATA_LOCATION_TABLE_NAME, partition_key, row_key_data_location))\r\n",
					"        api_data_location_entity.dataset_location = raw_file_location\r\n",
					"        api_data_location_entity.is_processed = False\r\n",
					"        self.table_service_client.update_table_entity_row(ApiProcessor.DATA_LOCATION_TABLE_NAME, entity=api_data_location_entity.to_dict())\r\n",
					"\r\n",
					"        updated_last_sequence = last_sequence + result_df_with_index.count()\r\n",
					"        self.telemetry_client.track_event({\"End Sequence\": [str(updated_last_sequence)]})\r\n",
					"\r\n",
					"        metaDataProperties = {\r\n",
					"            \"NextCursor\": next_cursor,\r\n",
					"            ApiProcessor.BRONZE_INDEX_TABLE_COLUMN_NAME: updated_last_sequence\r\n",
					"        }\r\n",
					"\r\n",
					"        self.table_service_client.update_api_tables(ApiProcessor.META_DATA_TABLE_NAME, partition_key, row_key, metaDataProperties)\r\n",
					"\r\n",
					"        dataLocationProperties = {\r\n",
					"            \"Path\": raw_file_location\r\n",
					"        }\r\n",
					"\r\n",
					"        self.table_service_client.update_api_tables(ApiProcessor.DATA_LOCATION_TABLE_NAME, partition_key, row_key_data_location, dataLocationProperties)\r\n",
					"        self.telemetry_client.track_event(f\"{ApiProcessor.DATA_LOCATION_TABLE_NAME} table for {self.api_name} has been updated with the path as {raw_file_location}\")\r\n",
					"\r\n",
					"        \r\n",
					""
				],
				"execution_count": 7
			}
		]
	}
}